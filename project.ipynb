{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import pandas as pd\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexual Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download dataset\n",
    "# kaggle.api.dataset_download_files('pashupatigupta/emotion-detection-from-text', path='.', unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# dataset = pd.read_csv('Data/tweet_emotions.csv')\n",
    "# print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download dataset\n",
    "# kaggle.api.dataset_download_files('ritika0111/emotion-detection-dataset', path='.', unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'disgusting', 'fear', 'happy', 'neutral', 'sad']\n"
     ]
    }
   ],
   "source": [
    "# List files in the extracted directory\n",
    "print(os.listdir('Data/emotion-detection-dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess Data\n",
    "\n",
    "Text Data: Loaded from Data/tweet_emotions.csv, tokenized, and normalized.\n",
    "\n",
    "Audio Data: Loaded from Data/emotion-detection-dataset, features extracted (MFCCs), and labeled.\n",
    "\n",
    "Optional: Saved the preprocessed data for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id   sentiment                                            content\n",
      "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
      "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
      "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
      "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
      "4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "text_dataset = pd.read_csv('Data/tweet_emotions.csv')\n",
    "print(text_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
      " 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"
     ]
    }
   ],
   "source": [
    "labels = text_dataset['sentiment']\n",
    "\n",
    "print(labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "label_mapping = {\n",
    "    'anger': 'angry',\n",
    "    'disgust': 'disgusting',\n",
    "    'fear': 'fear',\n",
    "    'happiness': 'happy',\n",
    "    'neutral': 'neutral',\n",
    "    'sadness': 'sad'\n",
    "}\n",
    "\n",
    "# Map text dataset labels to audio dataset labels\n",
    "text_dataset['mapped_sentiment'] = text_dataset['sentiment'].map(label_mapping)\n",
    "\n",
    "# Filter out rows with labels that do not have a corresponding audio label\n",
    "filtered_text_dataset = text_dataset.dropna(subset=['mapped_sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\navsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tweet_id sentiment                                            content  \\\n",
      "1  1956967666   sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
      "2  1956967696   sadness                Funeral ceremony...gloomy friday...   \n",
      "4  1956968416   neutral  @dannycastillo We want to trade with someone w...   \n",
      "6  1956968487   sadness  I should be sleep, but im not! thinking about ...   \n",
      "8  1956969035   sadness            @charviray Charlene my love. I miss you   \n",
      "\n",
      "  mapped_sentiment                                             tokens  \n",
      "1              sad  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
      "2              sad                [funeral, ceremony, gloomy, friday]  \n",
      "4          neutral  [dannycastillo, we, want, to, trade, with, som...  \n",
      "6              sad  [i, should, be, sleep, but, im, not, thinking,...  \n",
      "8              sad      [charviray, charlene, my, love, i, miss, you]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navsa\\AppData\\Local\\Temp\\ipykernel_25864\\2135114820.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_text_dataset['tokens'] = filtered_text_dataset['content'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Normalization: Convert to lowercase and remove punctuation\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "filtered_text_dataset['tokens'] = filtered_text_dataset['content'].apply(preprocess_text)\n",
    "print(filtered_text_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed text data\n",
    "filtered_text_dataset.to_csv('Data/preprocessed_text_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing audio files\n",
    "audio_dir = 'Data/emotion-detection-dataset'\n",
    "emotions = list(label_mapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = []\n",
    "audio_labels = []\n",
    "\n",
    "for emotion in emotions:\n",
    "    emotion_dir = os.path.join(audio_dir, emotion)\n",
    "    for file in os.listdir(emotion_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(emotion_dir, file)\n",
    "            features = extract_features(file_path)\n",
    "            audio_features.append(features)\n",
    "            audio_labels.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6522, 13) (6522,)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy arrays\n",
    "audio_features = np.array(audio_features)\n",
    "audio_labels = np.array(audio_labels)\n",
    "print(audio_features.shape, audio_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed audio data\n",
    "np.save('Data/preprocessed_audio_features.npy', audio_features)\n",
    "np.save('Data/audio_labels.npy', audio_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Emotion Detection Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed text data\n",
    "text_dataset = pd.read_csv('Data/preprocessed_text_data.csv')\n",
    "\n",
    "# Prepare data for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "texts = list(text_dataset['content'])\n",
    "labels = list(text_dataset['mapped_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "inputs['labels'] = torch.tensor(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_size = 0.8\n",
    "train_dataset = torch.utils.data.TensorDataset(inputs['input_ids'][:int(train_size*len(texts))], inputs['attention_mask'][:int(train_size*len(texts))], inputs['labels'][:int(train_size*len(texts))])\n",
    "test_dataset = torch.utils.data.TensorDataset(inputs['input_ids'][int(train_size*len(texts)):], inputs['attention_mask'][int(train_size*len(texts)):], inputs['labels'][int(train_size*len(texts)):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:17.283547Z","iopub.status.busy":"2024-08-28T01:13:17.282999Z","iopub.status.idle":"2024-08-28T01:13:17.288945Z","shell.execute_reply":"2024-08-28T01:13:17.287864Z","shell.execute_reply.started":"2024-08-28T01:13:17.283465Z"},"executionInfo":{"elapsed":21365,"status":"ok","timestamp":1724628442350,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"4AW4mrOxl-hX","outputId":"2a5ac3da-b816-4c19-880b-42a3cbaebf08","trusted":true},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:17.292164Z","iopub.status.busy":"2024-08-28T01:13:17.291690Z","iopub.status.idle":"2024-08-28T01:13:17.298548Z","shell.execute_reply":"2024-08-28T01:13:17.297497Z","shell.execute_reply.started":"2024-08-28T01:13:17.292105Z"},"executionInfo":{"elapsed":2070,"status":"ok","timestamp":1724628444411,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"OkKHVdpWkoKU","outputId":"c076dbbf-6c18-4a75-cb68-96e34aa427f9","trusted":true},"outputs":[],"source":["# !pip install pandas nltk librosa numpy transformers scikit-learn torch tensorflow accelerate"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:17.300736Z","iopub.status.busy":"2024-08-28T01:13:17.299993Z","iopub.status.idle":"2024-08-28T01:13:17.308451Z","shell.execute_reply":"2024-08-28T01:13:17.307393Z","shell.execute_reply.started":"2024-08-28T01:13:17.300685Z"},"id":"iSGTV7kThviA","trusted":true},"outputs":[],"source":["# os.environ['KAGGLE_USERNAME'] = 'your_username'\n","# os.environ['KAGGLE_KEY'] = 'your_key'"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:17.310752Z","iopub.status.busy":"2024-08-28T01:13:17.310293Z","iopub.status.idle":"2024-08-28T01:13:27.864722Z","shell.execute_reply":"2024-08-28T01:13:27.863767Z","shell.execute_reply.started":"2024-08-28T01:13:17.310703Z"},"executionInfo":{"elapsed":18327,"status":"ok","timestamp":1724628462705,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"fsItAvxZkoKd","outputId":"1ffc68e2-3c7c-4b6a-c1ea-1cdf3b88ec8a","trusted":true},"outputs":[],"source":["# import kaggle\n","import pandas as pd\n","\n","import shutil\n","import os\n","\n","import nltk\n","import string\n","\n","import librosa\n","import numpy as np\n","\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","# Use a smaller model\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","from tqdm import tqdm\n","import logging\n","from torch.cuda.amp import GradScaler\n","\n","from collections import Counter\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","\n","import spacy\n","from sklearn.utils import resample\n","\n","\n","# import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:27.867787Z","iopub.status.busy":"2024-08-28T01:13:27.867205Z","iopub.status.idle":"2024-08-28T01:13:27.872278Z","shell.execute_reply":"2024-08-28T01:13:27.871211Z","shell.execute_reply.started":"2024-08-28T01:13:27.867747Z"},"id":"9Q9a-pyousX6","trusted":true},"outputs":[],"source":["# # Change to the desired directory\n","# os.chdir('/content/drive/My Drive/Test Code')"]},{"cell_type":"markdown","metadata":{"id":"KYDe0YjFkoKh"},"source":["# 3. Set Up Logging\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:27.874068Z","iopub.status.busy":"2024-08-28T01:13:27.873748Z","iopub.status.idle":"2024-08-28T01:13:27.893263Z","shell.execute_reply":"2024-08-28T01:13:27.892248Z","shell.execute_reply.started":"2024-08-28T01:13:27.874032Z"},"executionInfo":{"elapsed":63,"status":"ok","timestamp":1724628462706,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"TWc9TVbXkoKl","outputId":"c05ff0e6-7fe6-4e3b-8048-f1f78e4c632e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'training.log' has been deleted.\n","Logging file created\n"]}],"source":["# # Ensure old log file is removed before creating a new one\n","# file_path = 'training.log'\n","# if os.path.exists(file_path):\n","#     os.remove(file_path)\n","#     print(f\"File '{file_path}' has been deleted.\")\n","# else:\n","#     print(f\"File '{file_path}' does not exist.\")\n","\n","# # Set up logging to record training progress\n","# logging.basicConfig(filename='training.log', level=logging.INFO)\n","# print(\"Logging file created\")\n"]},{"cell_type":"markdown","metadata":{"id":"fyjB9vubkoKm"},"source":["# Contexual Emotion Detection"]},{"cell_type":"markdown","metadata":{"id":"Q_nOkZ0ckoKn"},"source":["## 1. Download Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:27.896000Z","iopub.status.busy":"2024-08-28T01:13:27.894772Z","iopub.status.idle":"2024-08-28T01:13:27.902065Z","shell.execute_reply":"2024-08-28T01:13:27.900943Z","shell.execute_reply.started":"2024-08-28T01:13:27.895960Z"},"id":"gqU2WQjLkoKo","trusted":true},"outputs":[],"source":["# # Download dataset\n","# kaggle.api.dataset_download_files('pashupatigupta/emotion-detection-from-text', path='Data/.', unzip=True)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:27.903677Z","iopub.status.busy":"2024-08-28T01:13:27.903313Z","iopub.status.idle":"2024-08-28T01:13:28.054083Z","shell.execute_reply":"2024-08-28T01:13:28.053078Z","shell.execute_reply.started":"2024-08-28T01:13:27.903636Z"},"id":"CDq6jM9zkoKq","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     tweet_id   sentiment                                            content\n","0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n","1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696     sadness                Funeral ceremony...gloomy friday...\n","3  1956967789  enthusiasm               wants to hang out with friends SOON!\n","4  1956968416     neutral  @dannycastillo We want to trade with someone w...\n","['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n"," 'hate' 'happiness' 'boredom' 'relief' 'anger']\n"]}],"source":["# # Load dataset\n","# text_dataset  = pd.read_csv('/kaggle/input/d/pashupatigupta/emotion-detection-from-text/tweet_emotions.csv')\n","# print(text_dataset.head())\n","# print(text_dataset['sentiment'].unique())"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:28.055615Z","iopub.status.busy":"2024-08-28T01:13:28.055284Z","iopub.status.idle":"2024-08-28T01:13:28.059992Z","shell.execute_reply":"2024-08-28T01:13:28.059026Z","shell.execute_reply.started":"2024-08-28T01:13:28.055580Z"},"id":"GaKzFhiWkoKs","trusted":true},"outputs":[],"source":["# # Download dataset\n","# kaggle.api.dataset_download_files('ritika0111/emotion-detection-dataset', path='.', unzip=True)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:28.061536Z","iopub.status.busy":"2024-08-28T01:13:28.061208Z","iopub.status.idle":"2024-08-28T01:13:28.078242Z","shell.execute_reply":"2024-08-28T01:13:28.077184Z","shell.execute_reply.started":"2024-08-28T01:13:28.061486Z"},"id":"idlGt6UdkoKs","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['fear', 'disgusting', 'angry', 'neutral', 'sad', 'happy']\n"]}],"source":["# # List files in the extracted directory\n","# print(os.listdir('/kaggle/input/emotion-detection-dataset'))"]},{"cell_type":"markdown","metadata":{"id":"6JJxFCyckoKt"},"source":["## 2. Preprocess Data\n","\n","Text Data: Loaded from Data/tweet_emotions.csv, tokenized, and normalized.\n","\n","Audio Data: Loaded from Data/emotion-detection-dataset, features extracted (MFCCs), and labeled.\n","\n","Optional: Saved the preprocessed data for future use."]},{"cell_type":"markdown","metadata":{"id":"TD9dKu5BkoKu"},"source":["#### Text Data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:28.080096Z","iopub.status.busy":"2024-08-28T01:13:28.079642Z","iopub.status.idle":"2024-08-28T01:13:29.439089Z","shell.execute_reply":"2024-08-28T01:13:29.438080Z","shell.execute_reply.started":"2024-08-28T01:13:28.080040Z"},"trusted":true},"outputs":[],"source":["# # Load the English model from SpaCy\n","# nlp = spacy.load('en_core_web_sm')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:29.440824Z","iopub.status.busy":"2024-08-28T01:13:29.440404Z","iopub.status.idle":"2024-08-28T01:13:29.450092Z","shell.execute_reply":"2024-08-28T01:13:29.449163Z","shell.execute_reply.started":"2024-08-28T01:13:29.440780Z"},"id":"tS-mZr4bgYbU","trusted":true},"outputs":[],"source":["# # Additional text preprocessing\n","# stop_words = set(stopwords.words('english'))\n","# lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:29.455313Z","iopub.status.busy":"2024-08-28T01:13:29.454987Z","iopub.status.idle":"2024-08-28T01:13:29.481741Z","shell.execute_reply":"2024-08-28T01:13:29.480975Z","shell.execute_reply.started":"2024-08-28T01:13:29.455276Z"},"id":"8_f94iL7koKu","trusted":true},"outputs":[],"source":["# # Label mapping\n","# label_mapping = {\n","#     'anger': 'angry',\n","#     'hate': 'disgusting',\n","#     'worry': 'fear',\n","#     'happiness': 'happy',\n","#     'fun': 'happy',\n","#     'enthusiasm': 'happy',\n","#     'neutral': 'neutral',\n","#     'sadness': 'sad',\n","#     'empty': 'sad'\n","# }\n","\n","# # Map text dataset labels to audio dataset labels\n","# text_dataset['mapped_sentiment'] = text_dataset['sentiment'].map(label_mapping)\n","\n","# # Filter out rows with labels that do not have a corresponding audio label\n","# filtered_text_dataset = text_dataset.dropna(subset=['mapped_sentiment'])\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:29.483088Z","iopub.status.busy":"2024-08-28T01:13:29.482796Z","iopub.status.idle":"2024-08-28T01:13:29.490888Z","shell.execute_reply":"2024-08-28T01:13:29.489855Z","shell.execute_reply.started":"2024-08-28T01:13:29.483057Z"},"id":"kNAO9XsvkoKv","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['sad' 'happy' 'neutral' 'fear' 'disgusting' 'angry']\n"]}],"source":["# print(filtered_text_dataset['mapped_sentiment'].unique())"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:13:29.493156Z","iopub.status.busy":"2024-08-28T01:13:29.492470Z","iopub.status.idle":"2024-08-28T01:17:53.565416Z","shell.execute_reply":"2024-08-28T01:17:53.564346Z","shell.execute_reply.started":"2024-08-28T01:13:29.493107Z"},"id":"VVEyFgGAkoKw","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     tweet_id   sentiment                                      content  \\\n","0  1956967341       empty  know listenin bad habit early start freakin   \n","1  1956967666     sadness           layin n bed headache ughhhh waitin   \n","2  1956967696     sadness               funeral ceremony gloomy friday   \n","3  1956967789  enthusiasm                        want hang friend soon   \n","4  1956968416     neutral                    want trade houston ticket   \n","\n","  mapped_sentiment                                             tokens  \n","0              sad  [know, listenin, bad, habit, early, start, fre...  \n","1              sad          [layin, n, bed, headache, ughhhh, waitin]  \n","2              sad                [funeral, ceremony, gloomy, friday]  \n","3            happy                         [want, hang, friend, soon]  \n","4          neutral                     [want, trade, houston, ticket]  \n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/1230115163.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_text_dataset['tokens'] = filtered_text_dataset['content'].apply(preprocess_text)\n","/tmp/ipykernel_36/1230115163.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  filtered_text_dataset['content'] = filtered_text_dataset['tokens'].apply(lambda x: ' '.join(x))\n"]}],"source":["# def preprocess_text(text):\n","#     # Use SpaCy for text processing: tokenization, lemmatization, and stopword removal\n","#     doc = nlp(text)\n","#     tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n","#     return tokens\n","\n","# # Apply preprocessing\n","# filtered_text_dataset['tokens'] = filtered_text_dataset['content'].apply(preprocess_text)\n","# filtered_text_dataset['content'] = filtered_text_dataset['tokens'].apply(lambda x: ' '.join(x))\n","# print(filtered_text_dataset.head())"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:53.567555Z","iopub.status.busy":"2024-08-28T01:17:53.567046Z","iopub.status.idle":"2024-08-28T01:17:53.575784Z","shell.execute_reply":"2024-08-28T01:17:53.574610Z","shell.execute_reply.started":"2024-08-28T01:17:53.567478Z"},"id":"6spb2QnigYbW","trusted":true},"outputs":[],"source":["# # Balancing the dataset by oversampling minority classes\n","# def balance_dataset(df, target_column):\n","#     class_counts = df[target_column].value_counts()\n","#     max_class = class_counts.max()\n","\n","#     dfs = [df]\n","#     for class_ in class_counts.index:\n","#         df_class = df[df[target_column] == class_]\n","#         if len(df_class) < max_class:\n","#             df_class = resample(df_class, replace=True, n_samples=max_class, random_state=42)\n","#         dfs.append(df_class)\n","\n","#     return pd.concat(dfs)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:53.577910Z","iopub.status.busy":"2024-08-28T01:17:53.577411Z","iopub.status.idle":"2024-08-28T01:17:53.672159Z","shell.execute_reply":"2024-08-28T01:17:53.670980Z","shell.execute_reply.started":"2024-08-28T01:17:53.577855Z"},"id":"bOqCxeHogYbW","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mapped_sentiment\n","neutral       17276\n","fear          17097\n","happy         16382\n","sad           14630\n","disgusting     9961\n","angry          8748\n","Name: count, dtype: int64\n"]}],"source":["# balanced_text_dataset = balance_dataset(filtered_text_dataset, 'mapped_sentiment')\n","# print(balanced_text_dataset['mapped_sentiment'].value_counts())"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:53.673988Z","iopub.status.busy":"2024-08-28T01:17:53.673592Z","iopub.status.idle":"2024-08-28T01:17:54.402327Z","shell.execute_reply":"2024-08-28T01:17:54.401109Z","shell.execute_reply.started":"2024-08-28T01:17:53.673933Z"},"id":"WLYR1DODkoKw","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'processed_text_data.csv' does not exist.\n"]}],"source":["# # Ensure old log file is removed before creating a new one\n","# file_path = 'processed_text_data.csv'\n","# if os.path.exists(file_path):\n","#     os.remove(file_path)\n","#     print(f\"File '{file_path}' has been deleted.\")\n","# else:\n","#     print(f\"File '{file_path}' does not exist.\")\n","\n","# # Save preprocessed text data\n","# balanced_text_dataset.to_csv('processed_text_data.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"Cr7_7SyRkoKx"},"source":["#### Audio Data"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:54.404272Z","iopub.status.busy":"2024-08-28T01:17:54.403815Z","iopub.status.idle":"2024-08-28T01:17:54.411069Z","shell.execute_reply":"2024-08-28T01:17:54.409877Z","shell.execute_reply.started":"2024-08-28T01:17:54.404223Z"},"id":"M0IH6onjkoKx","trusted":true},"outputs":[],"source":["# # Function to extract MFCC features from audio files\n","# def extract_features(audio_path):\n","#     y, sr = librosa.load(audio_path)\n","#     mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","#     return np.mean(mfccs.T, axis=0)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:54.413210Z","iopub.status.busy":"2024-08-28T01:17:54.412757Z","iopub.status.idle":"2024-08-28T01:17:54.420401Z","shell.execute_reply":"2024-08-28T01:17:54.419258Z","shell.execute_reply.started":"2024-08-28T01:17:54.413149Z"},"id":"0dJePyOOkoKy","trusted":true},"outputs":[],"source":["# # Directory containing audio files\n","# audio_dir = '/kaggle/input/emotion-detection-dataset'\n","# emotions = list(label_mapping.values())"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:17:54.422301Z","iopub.status.busy":"2024-08-28T01:17:54.421937Z","iopub.status.idle":"2024-08-28T01:22:39.224682Z","shell.execute_reply":"2024-08-28T01:22:39.223075Z","shell.execute_reply.started":"2024-08-28T01:17:54.422264Z"},"id":"cmpsnI4RkoKy","trusted":true},"outputs":[],"source":["# audio_features = []\n","# audio_labels = []\n","\n","# for emotion in emotions:\n","#     emotion_dir = os.path.join(audio_dir, emotion)\n","#     for file in os.listdir(emotion_dir):\n","#         if file.endswith('.wav'):\n","#             file_path = os.path.join(emotion_dir, file)\n","#             features = extract_features(file_path)\n","#             audio_features.append(features)\n","#             audio_labels.append(emotion)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.228880Z","iopub.status.busy":"2024-08-28T01:22:39.227206Z","iopub.status.idle":"2024-08-28T01:22:39.264828Z","shell.execute_reply":"2024-08-28T01:22:39.263259Z","shell.execute_reply.started":"2024-08-28T01:22:39.228804Z"},"id":"chggLrAFkoKy","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(9783, 13) (9783,)\n"]}],"source":["# # Convert to numpy arrays\n","# audio_features = np.array(audio_features)\n","# audio_labels = np.array(audio_labels)\n","# print(audio_features.shape, audio_labels.shape)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.268656Z","iopub.status.busy":"2024-08-28T01:22:39.267568Z","iopub.status.idle":"2024-08-28T01:22:39.282210Z","shell.execute_reply":"2024-08-28T01:22:39.280595Z","shell.execute_reply.started":"2024-08-28T01:22:39.268583Z"},"id":"6tGumhpIkoKz","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'processed_audio_features.npy' does not exist.\n"]}],"source":["# # Ensure old log file is removed before creating a new one\n","# file_path = 'processed_audio_features.npy'\n","# if os.path.exists(file_path):\n","#     os.remove(file_path)\n","#     print(f\"File '{file_path}' has been deleted.\")\n","# else:\n","#     print(f\"File '{file_path}' does not exist.\")\n","\n","# # Save preprocessed audio data\n","# np.save('processed_audio_features.npy', audio_features)\n","# np.save('audio_labels.npy', audio_labels)"]},{"cell_type":"markdown","metadata":{"id":"tgLnrPSPkoKz"},"source":["## Train Emotion Detection Models"]},{"cell_type":"markdown","metadata":{"id":"li8_WMt9koK0"},"source":["#### Text Emotion Detection\n","Training Text Emotion Detection Model:\n","\n","You can use models like Distilbert for text emotion detection."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.286463Z","iopub.status.busy":"2024-08-28T01:22:39.285135Z","iopub.status.idle":"2024-08-28T01:22:39.488783Z","shell.execute_reply":"2024-08-28T01:22:39.487827Z","shell.execute_reply.started":"2024-08-28T01:22:39.286391Z"},"id":"h3wz5uvwkoK0","trusted":true},"outputs":[],"source":["balanced_text_dataset = pd.read_csv('processed_text_data.csv')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.490372Z","iopub.status.busy":"2024-08-28T01:22:39.490075Z","iopub.status.idle":"2024-08-28T01:22:39.499069Z","shell.execute_reply":"2024-08-28T01:22:39.497884Z","shell.execute_reply.started":"2024-08-28T01:22:39.490340Z"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1724628464045,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"QdBJNVdMkoK0","outputId":"8d06d023-81f8-448d-c361-08f27cc7ad4c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     tweet_id   sentiment                                      content  \\\n","0  1956967341       empty  know listenin bad habit early start freakin   \n","1  1956967666     sadness           layin n bed headache ughhhh waitin   \n","2  1956967696     sadness               funeral ceremony gloomy friday   \n","3  1956967789  enthusiasm                        want hang friend soon   \n","4  1956968416     neutral                    want trade houston ticket   \n","\n","  mapped_sentiment                                             tokens  \n","0              sad  ['know', 'listenin', 'bad', 'habit', 'early', ...  \n","1              sad  ['layin', 'n', 'bed', 'headache', 'ughhhh', 'w...  \n","2              sad        ['funeral', 'ceremony', 'gloomy', 'friday']  \n","3            happy                 ['want', 'hang', 'friend', 'soon']  \n","4          neutral             ['want', 'trade', 'houston', 'ticket']  \n"]}],"source":["print(balanced_text_dataset.head())"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.500741Z","iopub.status.busy":"2024-08-28T01:22:39.500386Z","iopub.status.idle":"2024-08-28T01:22:39.517014Z","shell.execute_reply":"2024-08-28T01:22:39.515975Z","shell.execute_reply.started":"2024-08-28T01:22:39.500678Z"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1724628464045,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"ldh0d3XT7TwV","outputId":"b2073a7d-946a-41de-d455-d792e30d671e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['sad' 'happy' 'neutral' 'fear' 'disgusting' 'angry']\n"]}],"source":["print(balanced_text_dataset['mapped_sentiment'].unique())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.518693Z","iopub.status.busy":"2024-08-28T01:22:39.518284Z","iopub.status.idle":"2024-08-28T01:22:39.542884Z","shell.execute_reply":"2024-08-28T01:22:39.541480Z","shell.execute_reply.started":"2024-08-28T01:22:39.518640Z"},"id":"ZFcWkrE7koK1","trusted":true},"outputs":[],"source":["# Split data\n","X = balanced_text_dataset['content']\n","y = balanced_text_dataset['mapped_sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_train = X_train.reset_index(drop=True)\n","X_test = X_test.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","y_test = y_test.reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.545373Z","iopub.status.busy":"2024-08-28T01:22:39.544595Z","iopub.status.idle":"2024-08-28T01:22:39.683582Z","shell.execute_reply":"2024-08-28T01:22:39.682563Z","shell.execute_reply.started":"2024-08-28T01:22:39.545324Z"},"executionInfo":{"elapsed":1268,"status":"ok","timestamp":1724628465300,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"1qGYC5ljkoK1","outputId":"0721af3f-97ed-463c-8209-afe17869b1f1","trusted":true},"outputs":[{"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/input/huggingface-distilbert/distilbert-base-uncased'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\utils\\_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n","\u001b[1;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/input/huggingface-distilbert/distilbert-base-uncased'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use pre-downloaded model in Kaggle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/huggingface-distilbert/distilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2210\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[1;32m-> 2210\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2223\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2227\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n","\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/input/huggingface-distilbert/distilbert-base-uncased'. Please provide either the path to a local folder or the repo_id of a model on the Hub."]}],"source":["# # Use pre-downloaded model in Kaggle\n","# tokenizer = DistilBertTokenizer.from_pretrained('/kaggle/input/huggingface-distilbert/distilbert-base-uncased')\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.685607Z","iopub.status.busy":"2024-08-28T01:22:39.685153Z","iopub.status.idle":"2024-08-28T01:22:39.694673Z","shell.execute_reply":"2024-08-28T01:22:39.693532Z","shell.execute_reply.started":"2024-08-28T01:22:39.685557Z"},"id":"4rwEmlqukoK2","trusted":true},"outputs":[],"source":["# def encode_data(texts, tokenizer, max_len):\n","#     input_ids = []\n","#     attention_masks = []\n","\n","#     for text in texts:\n","#         if isinstance(text, str):  # Ensure the text is a string\n","#             encoded_dict = tokenizer.encode_plus(\n","#                 text,\n","#                 add_special_tokens=True,\n","#                 max_length=max_len,\n","#                 padding='max_length',\n","#                 return_attention_mask=True,\n","#                 return_tensors='pt',\n","#                 truncation=True\n","#             )\n","#             input_ids.append(encoded_dict['input_ids'].squeeze(0))  # Remove batch dimension\n","#             attention_masks.append(encoded_dict['attention_mask'].squeeze(0))  # Remove batch dimension\n","#         else:\n","#             # Handle non-string text by appending empty tensors\n","#             empty_tensor = torch.zeros((max_len,), dtype=torch.long)\n","#             input_ids.append(empty_tensor)\n","#             attention_masks.append(empty_tensor)\n","\n","#     input_ids = torch.stack(input_ids)\n","#     attention_masks = torch.stack(attention_masks)\n","\n","#     return input_ids, attention_masks\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.696881Z","iopub.status.busy":"2024-08-28T01:22:39.696436Z","iopub.status.idle":"2024-08-28T01:22:39.752740Z","shell.execute_reply":"2024-08-28T01:22:39.751808Z","shell.execute_reply.started":"2024-08-28T01:22:39.696833Z"},"id":"I9lgTR57koK3","trusted":true},"outputs":[],"source":["# # Label encoding\n","# label_encoder = LabelEncoder()\n","# y_train_labels = label_encoder.fit_transform(y_train)\n","# y_test_labels = label_encoder.transform(y_test)\n","\n","# # Convert labels to tensors\n","# train_labels = torch.tensor(y_train_labels, dtype=torch.long)\n","# test_labels = torch.tensor(y_test_labels, dtype=torch.long)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:22:39.754562Z","iopub.status.busy":"2024-08-28T01:22:39.754122Z","iopub.status.idle":"2024-08-28T01:23:33.398183Z","shell.execute_reply":"2024-08-28T01:23:33.396866Z","shell.execute_reply.started":"2024-08-28T01:22:39.754497Z"},"id":"USz7uL16gYbc","trusted":true},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Encode the training and testing data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_inputs, train_masks \u001b[38;5;241m=\u001b[39m \u001b[43mencode_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m225\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_inputs, test_masks \u001b[38;5;241m=\u001b[39m encode_data(X_test, tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m225\u001b[39m)\n","Cell \u001b[1;32mIn[22], line 16\u001b[0m, in \u001b[0;36mencode_data\u001b[1;34m(texts, tokenizer, max_len)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# Ensure the text is a string\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     encoded_dict \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m      8\u001b[0m         text,\n\u001b[0;32m      9\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     )\n\u001b[1;32m---> 16\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(\u001b[43mencoded_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Handle non-string text by appending empty tensors\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# # Encode the training and testing data\n","# train_inputs, train_masks = encode_data(X_train, tokenizer, max_len=225)\n","# test_inputs, test_masks = encode_data(X_test, tokenizer, max_len=225)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:33.400122Z","iopub.status.busy":"2024-08-28T01:23:33.399762Z","iopub.status.idle":"2024-08-28T01:23:33.408898Z","shell.execute_reply":"2024-08-28T01:23:33.407653Z","shell.execute_reply.started":"2024-08-28T01:23:33.400083Z"},"id":"h7bWrwGKkoK4","trusted":true},"outputs":[],"source":["# # Create DataLoader for training and testing sets\n","# batch_size = 16\n","# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","# train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n","\n","# test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","# test_dataloader = DataLoader(test_data, sampler=SequentialSampler(test_data), batch_size=batch_size)"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:33.410661Z","iopub.status.busy":"2024-08-28T01:23:33.410284Z","iopub.status.idle":"2024-08-28T01:23:33.423137Z","shell.execute_reply":"2024-08-28T01:23:33.422034Z","shell.execute_reply.started":"2024-08-28T01:23:33.410622Z"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1724628498540,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"DW0rO1XvkoK4","outputId":"38353f6a-2eed-4f05-b026-fb7c2292816b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique labels in training set: [0 1 2 3 4 5]\n","Unique labels in testing set: [0 1 2 3 4 5]\n"]}],"source":["# print(\"Unique labels in training set:\", np.unique(y_train_labels))\n","# print(\"Unique labels in testing set:\", np.unique(y_test_labels))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:33.425398Z","iopub.status.busy":"2024-08-28T01:23:33.424984Z","iopub.status.idle":"2024-08-28T01:23:33.434879Z","shell.execute_reply":"2024-08-28T01:23:33.433832Z","shell.execute_reply.started":"2024-08-28T01:23:33.425350Z"},"id":"kgR4-_yAgYbj","trusted":true},"outputs":[],"source":["# # Adjust model architecture with dropout\n","# class DistilBertForSequenceClassificationWithDropout(DistilBertForSequenceClassification):\n","#     def __init__(self, config):\n","#         super().__init__(config)\n","#         self.dropout = torch.nn.Dropout(0.3)  # Add dropout with 30% probability\n","\n","#     def forward(self, input_ids, attention_mask=None, labels=None):\n","#         outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n","#         pooled_output = outputs[0][:, 0]\n","#         pooled_output = self.dropout(pooled_output)  # Apply dropout\n","#         logits = self.classifier(pooled_output)\n","\n","#         loss = None\n","#         if labels is not None:\n","#             loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))\n","#             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","#         return torch.nn.functional.softmax(logits, dim=-1) if loss is None else loss, logits\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:33.436531Z","iopub.status.busy":"2024-08-28T01:23:33.436203Z","iopub.status.idle":"2024-08-28T01:23:36.145540Z","shell.execute_reply":"2024-08-28T01:23:36.144564Z","shell.execute_reply.started":"2024-08-28T01:23:33.436495Z"},"executionInfo":{"elapsed":2017,"status":"ok","timestamp":1724628500534,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"CB7SjpesgYbk","outputId":"390bc0d8-b69d-4c19-a5f0-a5aebbe35ea4","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassificationWithDropout were not initialized from the model checkpoint at /kaggle/input/huggingface-distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# # Initialize the new model with dropout\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model = DistilBertForSequenceClassificationWithDropout.from_pretrained(\"/kaggle/input/huggingface-distilbert/distilbert-base-uncased\", num_labels=len(set(y_train))).to(device)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:36.147187Z","iopub.status.busy":"2024-08-28T01:23:36.146834Z","iopub.status.idle":"2024-08-28T01:23:36.774890Z","shell.execute_reply":"2024-08-28T01:23:36.773928Z","shell.execute_reply.started":"2024-08-28T01:23:36.147149Z"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1724628500704,"user":{"displayName":"Nav Sanya Anand","userId":"02554823300121068065"},"user_tz":420},"id":"ctlJ5MpVgYbl","outputId":"6aace3c7-124b-4270-d8a9-6a9fa2a07923","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# # Calculate class weights\n","# class_counts = Counter(y_train_labels)\n","# total_samples = len(y_train_labels)\n","# class_weights = [total_samples / class_counts[i] for i in range(len(set(y_train_labels)))]\n","# epochs = 25\n","\n","# # Optimizer and scheduler\n","# optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n","# total_steps = len(train_dataloader) * epochs / 2\n","# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:36.782668Z","iopub.status.busy":"2024-08-28T01:23:36.782127Z","iopub.status.idle":"2024-08-28T01:23:36.794488Z","shell.execute_reply":"2024-08-28T01:23:36.793406Z","shell.execute_reply.started":"2024-08-28T01:23:36.782634Z"},"id":"YvOyFTkYkoK6","trusted":true},"outputs":[],"source":["# def train_model_mixed_precision(model, dataloader, optimizer, scheduler, epochs, device, scaler):\n","#     model.train()\n","\n","#     for epoch in range(epochs):\n","#         print(f'Epoch {epoch + 1}/{epochs}')\n","#         logging.info(f'Epoch {epoch + 1}/{epochs}')\n","\n","#         total_loss = 0\n","#         step = 1\n","#         for batch in dataloader:\n","#             input_ids, attention_masks, labels = batch\n","#             input_ids = input_ids.to(device)\n","#             attention_masks = attention_masks.to(device)\n","#             labels = labels.to(device)\n","\n","#             optimizer.zero_grad()\n","\n","#             with torch.cuda.amp.autocast():\n","#                 outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n","#                 loss = outputs[0]  # Access the loss from the tuple\n","#                 total_loss += loss.item()\n","\n","#             scaler.scale(loss).backward()\n","#             scaler.step(optimizer)\n","#             scaler.update()\n","#             scheduler.step()\n","\n","#             # Print and log the loss for each batch\n","#             if step % 30 == 0:  # Adjust the frequency of logging if necessary\n","#                 print(f\"Epoch {epoch + 1}/{epochs} | Batch {step + 1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n","#                 logging.info(f\"Epoch {epoch + 1}/{epochs} | Batch {step + 1}/{len(dataloader)} | Loss: {loss.item():.4f}\")\n","\n","#             step += 1\n","\n","#         avg_train_loss = total_loss / len(dataloader)\n","#         print(f'Average training loss: {avg_train_loss:.2f}')\n","#         logging.info(f'Average training loss: {avg_train_loss:.2f}')\n","\n","#         if avg_train_loss < 0.001:\n","#             break\n","\n","#     print('Training complete')\n","#     logging.info('Training complete')\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T01:23:36.796285Z","iopub.status.busy":"2024-08-28T01:23:36.795875Z","iopub.status.idle":"2024-08-28T04:22:33.601973Z","shell.execute_reply":"2024-08-28T04:22:33.600894Z","shell.execute_reply.started":"2024-08-28T01:23:36.796239Z"},"id":"E4nUGb57koK6","outputId":"7ac670d7-c996-4138-b1bf-21eef9dee9a3","scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_36/3149539752.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n","/tmp/ipykernel_36/3234688868.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","Epoch 1/25 | Batch 31/3680 | Loss: 1.8215\n","Epoch 1/25 | Batch 61/3680 | Loss: 1.7143\n","Epoch 1/25 | Batch 91/3680 | Loss: 1.8127\n","Epoch 1/25 | Batch 121/3680 | Loss: 1.6770\n","Epoch 1/25 | Batch 151/3680 | Loss: 1.7113\n","Epoch 1/25 | Batch 181/3680 | Loss: 1.6437\n","Epoch 1/25 | Batch 211/3680 | Loss: 1.6113\n","Epoch 1/25 | Batch 241/3680 | Loss: 1.7286\n","Epoch 1/25 | Batch 271/3680 | Loss: 1.5656\n","Epoch 1/25 | Batch 301/3680 | Loss: 1.6014\n","Epoch 1/25 | Batch 331/3680 | Loss: 1.4939\n","Epoch 1/25 | Batch 361/3680 | Loss: 1.3276\n","Epoch 1/25 | Batch 391/3680 | Loss: 1.4052\n","Epoch 1/25 | Batch 421/3680 | Loss: 1.6882\n","Epoch 1/25 | Batch 451/3680 | Loss: 1.3678\n","Epoch 1/25 | Batch 481/3680 | Loss: 1.3150\n","Epoch 1/25 | Batch 511/3680 | Loss: 1.5649\n","Epoch 1/25 | Batch 541/3680 | Loss: 0.7916\n","Epoch 1/25 | Batch 571/3680 | Loss: 1.5370\n","Epoch 1/25 | Batch 601/3680 | Loss: 1.3169\n","Epoch 1/25 | Batch 631/3680 | Loss: 1.2132\n","Epoch 1/25 | Batch 661/3680 | Loss: 0.7390\n","Epoch 1/25 | Batch 691/3680 | Loss: 0.7799\n","Epoch 1/25 | Batch 721/3680 | Loss: 1.1549\n","Epoch 1/25 | Batch 751/3680 | Loss: 1.3597\n","Epoch 1/25 | Batch 781/3680 | Loss: 1.5071\n","Epoch 1/25 | Batch 811/3680 | Loss: 1.2857\n","Epoch 1/25 | Batch 841/3680 | Loss: 1.1654\n","Epoch 1/25 | Batch 871/3680 | Loss: 0.6681\n","Epoch 1/25 | Batch 901/3680 | Loss: 0.8192\n","Epoch 1/25 | Batch 931/3680 | Loss: 0.7670\n","Epoch 1/25 | Batch 961/3680 | Loss: 1.1646\n","Epoch 1/25 | Batch 991/3680 | Loss: 0.8389\n","Epoch 1/25 | Batch 1021/3680 | Loss: 1.6397\n","Epoch 1/25 | Batch 1051/3680 | Loss: 0.9770\n","Epoch 1/25 | Batch 1081/3680 | Loss: 1.1616\n","Epoch 1/25 | Batch 1111/3680 | Loss: 0.8721\n","Epoch 1/25 | Batch 1141/3680 | Loss: 0.8687\n","Epoch 1/25 | Batch 1171/3680 | Loss: 0.6454\n","Epoch 1/25 | Batch 1201/3680 | Loss: 1.0845\n","Epoch 1/25 | Batch 1231/3680 | Loss: 0.9729\n","Epoch 1/25 | Batch 1261/3680 | Loss: 1.2097\n","Epoch 1/25 | Batch 1291/3680 | Loss: 0.7025\n","Epoch 1/25 | Batch 1321/3680 | Loss: 1.1803\n","Epoch 1/25 | Batch 1351/3680 | Loss: 0.5768\n","Epoch 1/25 | Batch 1381/3680 | Loss: 1.3000\n","Epoch 1/25 | Batch 1411/3680 | Loss: 1.3637\n","Epoch 1/25 | Batch 1441/3680 | Loss: 0.9294\n","Epoch 1/25 | Batch 1471/3680 | Loss: 1.0143\n","Epoch 1/25 | Batch 1501/3680 | Loss: 0.9251\n","Epoch 1/25 | Batch 1531/3680 | Loss: 0.9237\n","Epoch 1/25 | Batch 1561/3680 | Loss: 1.0860\n","Epoch 1/25 | Batch 1591/3680 | Loss: 1.0490\n","Epoch 1/25 | Batch 1621/3680 | Loss: 1.4837\n","Epoch 1/25 | Batch 1651/3680 | Loss: 0.7057\n","Epoch 1/25 | Batch 1681/3680 | Loss: 1.1524\n","Epoch 1/25 | Batch 1711/3680 | Loss: 0.6882\n","Epoch 1/25 | Batch 1741/3680 | Loss: 0.7681\n","Epoch 1/25 | Batch 1771/3680 | Loss: 0.6803\n","Epoch 1/25 | Batch 1801/3680 | Loss: 1.1321\n","Epoch 1/25 | Batch 1831/3680 | Loss: 0.6671\n","Epoch 1/25 | Batch 1861/3680 | Loss: 1.3083\n","Epoch 1/25 | Batch 1891/3680 | Loss: 1.1890\n","Epoch 1/25 | Batch 1921/3680 | Loss: 0.8246\n","Epoch 1/25 | Batch 1951/3680 | Loss: 0.8771\n","Epoch 1/25 | Batch 1981/3680 | Loss: 1.0190\n","Epoch 1/25 | Batch 2011/3680 | Loss: 1.0404\n","Epoch 1/25 | Batch 2041/3680 | Loss: 1.2335\n","Epoch 1/25 | Batch 2071/3680 | Loss: 0.8207\n","Epoch 1/25 | Batch 2101/3680 | Loss: 0.5892\n","Epoch 1/25 | Batch 2131/3680 | Loss: 0.6623\n","Epoch 1/25 | Batch 2161/3680 | Loss: 0.7631\n","Epoch 1/25 | Batch 2191/3680 | Loss: 0.9674\n","Epoch 1/25 | Batch 2221/3680 | Loss: 0.6976\n","Epoch 1/25 | Batch 2251/3680 | Loss: 0.9842\n","Epoch 1/25 | Batch 2281/3680 | Loss: 1.1813\n","Epoch 1/25 | Batch 2311/3680 | Loss: 0.8188\n","Epoch 1/25 | Batch 2341/3680 | Loss: 0.6062\n","Epoch 1/25 | Batch 2371/3680 | Loss: 1.3117\n","Epoch 1/25 | Batch 2401/3680 | Loss: 0.9640\n","Epoch 1/25 | Batch 2431/3680 | Loss: 0.5171\n","Epoch 1/25 | Batch 2461/3680 | Loss: 1.0076\n","Epoch 1/25 | Batch 2491/3680 | Loss: 1.2700\n","Epoch 1/25 | Batch 2521/3680 | Loss: 0.7657\n","Epoch 1/25 | Batch 2551/3680 | Loss: 0.9900\n","Epoch 1/25 | Batch 2581/3680 | Loss: 1.3282\n","Epoch 1/25 | Batch 2611/3680 | Loss: 0.9543\n","Epoch 1/25 | Batch 2641/3680 | Loss: 0.8702\n","Epoch 1/25 | Batch 2671/3680 | Loss: 0.8857\n","Epoch 1/25 | Batch 2701/3680 | Loss: 0.9186\n","Epoch 1/25 | Batch 2731/3680 | Loss: 0.6501\n","Epoch 1/25 | Batch 2761/3680 | Loss: 1.1173\n","Epoch 1/25 | Batch 2791/3680 | Loss: 0.9084\n","Epoch 1/25 | Batch 2821/3680 | Loss: 0.6615\n","Epoch 1/25 | Batch 2851/3680 | Loss: 0.6419\n","Epoch 1/25 | Batch 2881/3680 | Loss: 0.8866\n","Epoch 1/25 | Batch 2911/3680 | Loss: 0.4852\n","Epoch 1/25 | Batch 2941/3680 | Loss: 0.7984\n","Epoch 1/25 | Batch 2971/3680 | Loss: 0.8259\n","Epoch 1/25 | Batch 3001/3680 | Loss: 0.6103\n","Epoch 1/25 | Batch 3031/3680 | Loss: 1.2727\n","Epoch 1/25 | Batch 3061/3680 | Loss: 0.9433\n","Epoch 1/25 | Batch 3091/3680 | Loss: 0.6501\n","Epoch 1/25 | Batch 3121/3680 | Loss: 0.6755\n","Epoch 1/25 | Batch 3151/3680 | Loss: 0.8787\n","Epoch 1/25 | Batch 3181/3680 | Loss: 0.9604\n","Epoch 1/25 | Batch 3211/3680 | Loss: 0.7588\n","Epoch 1/25 | Batch 3241/3680 | Loss: 0.9894\n","Epoch 1/25 | Batch 3271/3680 | Loss: 0.8788\n","Epoch 1/25 | Batch 3301/3680 | Loss: 1.1238\n","Epoch 1/25 | Batch 3331/3680 | Loss: 0.6638\n","Epoch 1/25 | Batch 3361/3680 | Loss: 1.1520\n","Epoch 1/25 | Batch 3391/3680 | Loss: 0.6230\n","Epoch 1/25 | Batch 3421/3680 | Loss: 0.9669\n","Epoch 1/25 | Batch 3451/3680 | Loss: 0.9947\n","Epoch 1/25 | Batch 3481/3680 | Loss: 0.8573\n","Epoch 1/25 | Batch 3511/3680 | Loss: 0.8764\n","Epoch 1/25 | Batch 3541/3680 | Loss: 0.9582\n","Epoch 1/25 | Batch 3571/3680 | Loss: 0.7963\n","Epoch 1/25 | Batch 3601/3680 | Loss: 1.1168\n","Epoch 1/25 | Batch 3631/3680 | Loss: 0.7338\n","Epoch 1/25 | Batch 3661/3680 | Loss: 0.8528\n","Average training loss: 1.06\n","Epoch 2/25\n","Epoch 2/25 | Batch 31/3680 | Loss: 0.6267\n","Epoch 2/25 | Batch 61/3680 | Loss: 1.0470\n","Epoch 2/25 | Batch 91/3680 | Loss: 1.1936\n","Epoch 2/25 | Batch 121/3680 | Loss: 1.0006\n","Epoch 2/25 | Batch 151/3680 | Loss: 0.8864\n","Epoch 2/25 | Batch 181/3680 | Loss: 0.8026\n","Epoch 2/25 | Batch 211/3680 | Loss: 0.8005\n","Epoch 2/25 | Batch 241/3680 | Loss: 0.3914\n","Epoch 2/25 | Batch 271/3680 | Loss: 0.9060\n","Epoch 2/25 | Batch 301/3680 | Loss: 1.0031\n","Epoch 2/25 | Batch 331/3680 | Loss: 0.9277\n","Epoch 2/25 | Batch 361/3680 | Loss: 0.5932\n","Epoch 2/25 | Batch 391/3680 | Loss: 0.8362\n","Epoch 2/25 | Batch 421/3680 | Loss: 0.8454\n","Epoch 2/25 | Batch 451/3680 | Loss: 0.6064\n","Epoch 2/25 | Batch 481/3680 | Loss: 0.6259\n","Epoch 2/25 | Batch 511/3680 | Loss: 0.8985\n","Epoch 2/25 | Batch 541/3680 | Loss: 0.9019\n","Epoch 2/25 | Batch 571/3680 | Loss: 0.5276\n","Epoch 2/25 | Batch 601/3680 | Loss: 0.9593\n","Epoch 2/25 | Batch 631/3680 | Loss: 0.7216\n","Epoch 2/25 | Batch 661/3680 | Loss: 0.7450\n","Epoch 2/25 | Batch 691/3680 | Loss: 0.4629\n","Epoch 2/25 | Batch 721/3680 | Loss: 0.6747\n","Epoch 2/25 | Batch 751/3680 | Loss: 1.0155\n","Epoch 2/25 | Batch 781/3680 | Loss: 0.7266\n","Epoch 2/25 | Batch 811/3680 | Loss: 1.0025\n","Epoch 2/25 | Batch 841/3680 | Loss: 1.0069\n","Epoch 2/25 | Batch 871/3680 | Loss: 0.4392\n","Epoch 2/25 | Batch 901/3680 | Loss: 0.6224\n","Epoch 2/25 | Batch 931/3680 | Loss: 0.8398\n","Epoch 2/25 | Batch 961/3680 | Loss: 0.7423\n","Epoch 2/25 | Batch 991/3680 | Loss: 0.9991\n","Epoch 2/25 | Batch 1021/3680 | Loss: 0.3717\n","Epoch 2/25 | Batch 1051/3680 | Loss: 0.6246\n","Epoch 2/25 | Batch 1081/3680 | Loss: 0.8177\n","Epoch 2/25 | Batch 1111/3680 | Loss: 0.3111\n","Epoch 2/25 | Batch 1141/3680 | Loss: 0.5741\n","Epoch 2/25 | Batch 1171/3680 | Loss: 0.4003\n","Epoch 2/25 | Batch 1201/3680 | Loss: 0.4566\n","Epoch 2/25 | Batch 1231/3680 | Loss: 0.4988\n","Epoch 2/25 | Batch 1261/3680 | Loss: 0.4978\n","Epoch 2/25 | Batch 1291/3680 | Loss: 0.8129\n","Epoch 2/25 | Batch 1321/3680 | Loss: 0.8771\n","Epoch 2/25 | Batch 1351/3680 | Loss: 0.5543\n","Epoch 2/25 | Batch 1381/3680 | Loss: 0.5280\n","Epoch 2/25 | Batch 1411/3680 | Loss: 0.4103\n","Epoch 2/25 | Batch 1441/3680 | Loss: 0.8700\n","Epoch 2/25 | Batch 1471/3680 | Loss: 1.0664\n","Epoch 2/25 | Batch 1501/3680 | Loss: 0.9662\n","Epoch 2/25 | Batch 1531/3680 | Loss: 0.7923\n","Epoch 2/25 | Batch 1561/3680 | Loss: 1.3517\n","Epoch 2/25 | Batch 1591/3680 | Loss: 0.8067\n","Epoch 2/25 | Batch 1621/3680 | Loss: 0.5728\n","Epoch 2/25 | Batch 1651/3680 | Loss: 0.6402\n","Epoch 2/25 | Batch 1681/3680 | Loss: 1.0530\n","Epoch 2/25 | Batch 1711/3680 | Loss: 0.7472\n","Epoch 2/25 | Batch 1741/3680 | Loss: 0.6943\n","Epoch 2/25 | Batch 1771/3680 | Loss: 0.7957\n","Epoch 2/25 | Batch 1801/3680 | Loss: 0.6158\n","Epoch 2/25 | Batch 1831/3680 | Loss: 0.5636\n","Epoch 2/25 | Batch 1861/3680 | Loss: 0.3824\n","Epoch 2/25 | Batch 1891/3680 | Loss: 0.6365\n","Epoch 2/25 | Batch 1921/3680 | Loss: 0.5370\n","Epoch 2/25 | Batch 1951/3680 | Loss: 0.8269\n","Epoch 2/25 | Batch 1981/3680 | Loss: 0.9157\n","Epoch 2/25 | Batch 2011/3680 | Loss: 0.5262\n","Epoch 2/25 | Batch 2041/3680 | Loss: 0.7737\n","Epoch 2/25 | Batch 2071/3680 | Loss: 0.5199\n","Epoch 2/25 | Batch 2101/3680 | Loss: 0.4728\n","Epoch 2/25 | Batch 2131/3680 | Loss: 0.9196\n","Epoch 2/25 | Batch 2161/3680 | Loss: 0.6550\n","Epoch 2/25 | Batch 2191/3680 | Loss: 0.5908\n","Epoch 2/25 | Batch 2221/3680 | Loss: 0.7189\n","Epoch 2/25 | Batch 2251/3680 | Loss: 0.6989\n","Epoch 2/25 | Batch 2281/3680 | Loss: 0.3385\n","Epoch 2/25 | Batch 2311/3680 | Loss: 0.8421\n","Epoch 2/25 | Batch 2341/3680 | Loss: 0.7780\n","Epoch 2/25 | Batch 2371/3680 | Loss: 0.5395\n","Epoch 2/25 | Batch 2401/3680 | Loss: 0.3922\n","Epoch 2/25 | Batch 2431/3680 | Loss: 0.7896\n","Epoch 2/25 | Batch 2461/3680 | Loss: 0.7505\n","Epoch 2/25 | Batch 2491/3680 | Loss: 0.7530\n","Epoch 2/25 | Batch 2521/3680 | Loss: 0.6652\n","Epoch 2/25 | Batch 2551/3680 | Loss: 1.0714\n","Epoch 2/25 | Batch 2581/3680 | Loss: 0.9189\n","Epoch 2/25 | Batch 2611/3680 | Loss: 0.6283\n","Epoch 2/25 | Batch 2641/3680 | Loss: 0.5894\n","Epoch 2/25 | Batch 2671/3680 | Loss: 0.4904\n","Epoch 2/25 | Batch 2701/3680 | Loss: 0.5678\n","Epoch 2/25 | Batch 2731/3680 | Loss: 0.4084\n","Epoch 2/25 | Batch 2761/3680 | Loss: 0.9051\n","Epoch 2/25 | Batch 2791/3680 | Loss: 0.7754\n","Epoch 2/25 | Batch 2821/3680 | Loss: 0.9198\n","Epoch 2/25 | Batch 2851/3680 | Loss: 0.8275\n","Epoch 2/25 | Batch 2881/3680 | Loss: 0.4364\n","Epoch 2/25 | Batch 2911/3680 | Loss: 0.3352\n","Epoch 2/25 | Batch 2941/3680 | Loss: 0.5324\n","Epoch 2/25 | Batch 2971/3680 | Loss: 1.0804\n","Epoch 2/25 | Batch 3001/3680 | Loss: 0.7082\n","Epoch 2/25 | Batch 3031/3680 | Loss: 1.0404\n","Epoch 2/25 | Batch 3061/3680 | Loss: 0.7260\n","Epoch 2/25 | Batch 3091/3680 | Loss: 0.2934\n","Epoch 2/25 | Batch 3121/3680 | Loss: 0.5978\n","Epoch 2/25 | Batch 3151/3680 | Loss: 0.5367\n","Epoch 2/25 | Batch 3181/3680 | Loss: 0.6823\n","Epoch 2/25 | Batch 3211/3680 | Loss: 0.5795\n","Epoch 2/25 | Batch 3241/3680 | Loss: 0.6929\n","Epoch 2/25 | Batch 3271/3680 | Loss: 0.4133\n","Epoch 2/25 | Batch 3301/3680 | Loss: 0.3403\n","Epoch 2/25 | Batch 3331/3680 | Loss: 0.8432\n","Epoch 2/25 | Batch 3361/3680 | Loss: 0.5766\n","Epoch 2/25 | Batch 3391/3680 | Loss: 0.6759\n","Epoch 2/25 | Batch 3421/3680 | Loss: 0.8555\n","Epoch 2/25 | Batch 3451/3680 | Loss: 1.2132\n","Epoch 2/25 | Batch 3481/3680 | Loss: 0.5894\n","Epoch 2/25 | Batch 3511/3680 | Loss: 0.6588\n","Epoch 2/25 | Batch 3541/3680 | Loss: 0.7949\n","Epoch 2/25 | Batch 3571/3680 | Loss: 0.4841\n","Epoch 2/25 | Batch 3601/3680 | Loss: 0.6510\n","Epoch 2/25 | Batch 3631/3680 | Loss: 0.5776\n","Epoch 2/25 | Batch 3661/3680 | Loss: 0.5531\n","Average training loss: 0.69\n","Epoch 3/25\n","Epoch 3/25 | Batch 31/3680 | Loss: 0.4674\n","Epoch 3/25 | Batch 61/3680 | Loss: 0.5579\n","Epoch 3/25 | Batch 91/3680 | Loss: 0.5179\n","Epoch 3/25 | Batch 121/3680 | Loss: 0.4538\n","Epoch 3/25 | Batch 151/3680 | Loss: 0.5246\n","Epoch 3/25 | Batch 181/3680 | Loss: 0.5013\n","Epoch 3/25 | Batch 211/3680 | Loss: 0.2009\n","Epoch 3/25 | Batch 241/3680 | Loss: 0.7185\n","Epoch 3/25 | Batch 271/3680 | Loss: 0.2635\n","Epoch 3/25 | Batch 301/3680 | Loss: 0.4161\n","Epoch 3/25 | Batch 331/3680 | Loss: 0.4201\n","Epoch 3/25 | Batch 361/3680 | Loss: 0.5381\n","Epoch 3/25 | Batch 391/3680 | Loss: 0.3173\n","Epoch 3/25 | Batch 421/3680 | Loss: 0.4838\n","Epoch 3/25 | Batch 451/3680 | Loss: 0.2724\n","Epoch 3/25 | Batch 481/3680 | Loss: 0.4596\n","Epoch 3/25 | Batch 511/3680 | Loss: 0.0880\n","Epoch 3/25 | Batch 541/3680 | Loss: 0.6019\n","Epoch 3/25 | Batch 571/3680 | Loss: 0.7819\n","Epoch 3/25 | Batch 601/3680 | Loss: 0.4773\n","Epoch 3/25 | Batch 631/3680 | Loss: 0.5377\n","Epoch 3/25 | Batch 661/3680 | Loss: 0.4860\n","Epoch 3/25 | Batch 691/3680 | Loss: 0.5166\n","Epoch 3/25 | Batch 721/3680 | Loss: 1.0598\n","Epoch 3/25 | Batch 751/3680 | Loss: 0.1346\n","Epoch 3/25 | Batch 781/3680 | Loss: 0.3184\n","Epoch 3/25 | Batch 811/3680 | Loss: 0.1653\n","Epoch 3/25 | Batch 841/3680 | Loss: 0.3967\n","Epoch 3/25 | Batch 871/3680 | Loss: 0.3423\n","Epoch 3/25 | Batch 901/3680 | Loss: 0.3740\n","Epoch 3/25 | Batch 931/3680 | Loss: 0.4525\n","Epoch 3/25 | Batch 961/3680 | Loss: 1.0180\n","Epoch 3/25 | Batch 991/3680 | Loss: 0.3438\n","Epoch 3/25 | Batch 1021/3680 | Loss: 0.3198\n","Epoch 3/25 | Batch 1051/3680 | Loss: 0.3085\n","Epoch 3/25 | Batch 1081/3680 | Loss: 0.4993\n","Epoch 3/25 | Batch 1111/3680 | Loss: 0.6487\n","Epoch 3/25 | Batch 1141/3680 | Loss: 0.3158\n","Epoch 3/25 | Batch 1171/3680 | Loss: 0.2974\n","Epoch 3/25 | Batch 1201/3680 | Loss: 0.5202\n","Epoch 3/25 | Batch 1231/3680 | Loss: 0.6634\n","Epoch 3/25 | Batch 1261/3680 | Loss: 0.6134\n","Epoch 3/25 | Batch 1291/3680 | Loss: 0.6027\n","Epoch 3/25 | Batch 1321/3680 | Loss: 0.4221\n","Epoch 3/25 | Batch 1351/3680 | Loss: 0.4300\n","Epoch 3/25 | Batch 1381/3680 | Loss: 0.3922\n","Epoch 3/25 | Batch 1411/3680 | Loss: 0.3432\n","Epoch 3/25 | Batch 1441/3680 | Loss: 0.2738\n","Epoch 3/25 | Batch 1471/3680 | Loss: 0.1013\n","Epoch 3/25 | Batch 1501/3680 | Loss: 0.1701\n","Epoch 3/25 | Batch 1531/3680 | Loss: 0.3300\n","Epoch 3/25 | Batch 1561/3680 | Loss: 0.5825\n","Epoch 3/25 | Batch 1591/3680 | Loss: 0.3737\n","Epoch 3/25 | Batch 1621/3680 | Loss: 0.3843\n","Epoch 3/25 | Batch 1651/3680 | Loss: 0.5096\n","Epoch 3/25 | Batch 1681/3680 | Loss: 0.8126\n","Epoch 3/25 | Batch 1711/3680 | Loss: 0.3657\n","Epoch 3/25 | Batch 1741/3680 | Loss: 0.3777\n","Epoch 3/25 | Batch 1771/3680 | Loss: 0.8513\n","Epoch 3/25 | Batch 1801/3680 | Loss: 0.4516\n","Epoch 3/25 | Batch 1831/3680 | Loss: 0.5905\n","Epoch 3/25 | Batch 1861/3680 | Loss: 0.4380\n","Epoch 3/25 | Batch 1891/3680 | Loss: 0.5098\n","Epoch 3/25 | Batch 1921/3680 | Loss: 0.7786\n","Epoch 3/25 | Batch 1951/3680 | Loss: 0.4734\n","Epoch 3/25 | Batch 1981/3680 | Loss: 0.5043\n","Epoch 3/25 | Batch 2011/3680 | Loss: 0.6100\n","Epoch 3/25 | Batch 2041/3680 | Loss: 0.5248\n","Epoch 3/25 | Batch 2071/3680 | Loss: 0.5109\n","Epoch 3/25 | Batch 2101/3680 | Loss: 0.5687\n","Epoch 3/25 | Batch 2131/3680 | Loss: 0.9156\n","Epoch 3/25 | Batch 2161/3680 | Loss: 0.6067\n","Epoch 3/25 | Batch 2191/3680 | Loss: 0.4843\n","Epoch 3/25 | Batch 2221/3680 | Loss: 0.4913\n","Epoch 3/25 | Batch 2251/3680 | Loss: 0.2686\n","Epoch 3/25 | Batch 2281/3680 | Loss: 0.7018\n","Epoch 3/25 | Batch 2311/3680 | Loss: 0.4071\n","Epoch 3/25 | Batch 2341/3680 | Loss: 0.2491\n","Epoch 3/25 | Batch 2371/3680 | Loss: 0.2939\n","Epoch 3/25 | Batch 2401/3680 | Loss: 0.4696\n","Epoch 3/25 | Batch 2431/3680 | Loss: 0.4024\n","Epoch 3/25 | Batch 2461/3680 | Loss: 0.3477\n","Epoch 3/25 | Batch 2491/3680 | Loss: 0.2934\n","Epoch 3/25 | Batch 2521/3680 | Loss: 0.4351\n","Epoch 3/25 | Batch 2551/3680 | Loss: 0.7155\n","Epoch 3/25 | Batch 2581/3680 | Loss: 0.7916\n","Epoch 3/25 | Batch 2611/3680 | Loss: 0.3522\n","Epoch 3/25 | Batch 2641/3680 | Loss: 0.5993\n","Epoch 3/25 | Batch 2671/3680 | Loss: 0.4644\n","Epoch 3/25 | Batch 2701/3680 | Loss: 0.4902\n","Epoch 3/25 | Batch 2731/3680 | Loss: 0.2374\n","Epoch 3/25 | Batch 2761/3680 | Loss: 0.1886\n","Epoch 3/25 | Batch 2791/3680 | Loss: 0.5857\n","Epoch 3/25 | Batch 2821/3680 | Loss: 0.4603\n","Epoch 3/25 | Batch 2851/3680 | Loss: 0.5967\n","Epoch 3/25 | Batch 2881/3680 | Loss: 0.6806\n","Epoch 3/25 | Batch 2911/3680 | Loss: 0.6346\n","Epoch 3/25 | Batch 2941/3680 | Loss: 0.5991\n","Epoch 3/25 | Batch 2971/3680 | Loss: 0.3533\n","Epoch 3/25 | Batch 3001/3680 | Loss: 0.4686\n","Epoch 3/25 | Batch 3031/3680 | Loss: 0.3902\n","Epoch 3/25 | Batch 3061/3680 | Loss: 0.3878\n","Epoch 3/25 | Batch 3091/3680 | Loss: 0.5997\n","Epoch 3/25 | Batch 3121/3680 | Loss: 0.6071\n","Epoch 3/25 | Batch 3151/3680 | Loss: 0.2488\n","Epoch 3/25 | Batch 3181/3680 | Loss: 0.4142\n","Epoch 3/25 | Batch 3211/3680 | Loss: 0.4011\n","Epoch 3/25 | Batch 3241/3680 | Loss: 0.5586\n","Epoch 3/25 | Batch 3271/3680 | Loss: 0.3344\n","Epoch 3/25 | Batch 3301/3680 | Loss: 0.1779\n","Epoch 3/25 | Batch 3331/3680 | Loss: 0.3114\n","Epoch 3/25 | Batch 3361/3680 | Loss: 0.1897\n","Epoch 3/25 | Batch 3391/3680 | Loss: 0.4843\n","Epoch 3/25 | Batch 3421/3680 | Loss: 0.1951\n","Epoch 3/25 | Batch 3451/3680 | Loss: 0.4858\n","Epoch 3/25 | Batch 3481/3680 | Loss: 0.5552\n","Epoch 3/25 | Batch 3511/3680 | Loss: 0.6470\n","Epoch 3/25 | Batch 3541/3680 | Loss: 0.2627\n","Epoch 3/25 | Batch 3571/3680 | Loss: 0.3291\n","Epoch 3/25 | Batch 3601/3680 | Loss: 0.3731\n","Epoch 3/25 | Batch 3631/3680 | Loss: 0.6120\n","Epoch 3/25 | Batch 3661/3680 | Loss: 0.4431\n","Average training loss: 0.45\n","Epoch 4/25\n","Epoch 4/25 | Batch 31/3680 | Loss: 0.2717\n","Epoch 4/25 | Batch 61/3680 | Loss: 0.5008\n","Epoch 4/25 | Batch 91/3680 | Loss: 0.1918\n","Epoch 4/25 | Batch 121/3680 | Loss: 0.2771\n","Epoch 4/25 | Batch 151/3680 | Loss: 0.2161\n","Epoch 4/25 | Batch 181/3680 | Loss: 0.2119\n","Epoch 4/25 | Batch 211/3680 | Loss: 0.5320\n","Epoch 4/25 | Batch 241/3680 | Loss: 0.2260\n","Epoch 4/25 | Batch 271/3680 | Loss: 0.1530\n","Epoch 4/25 | Batch 301/3680 | Loss: 0.0428\n","Epoch 4/25 | Batch 331/3680 | Loss: 0.3377\n","Epoch 4/25 | Batch 361/3680 | Loss: 0.4882\n","Epoch 4/25 | Batch 391/3680 | Loss: 0.4159\n","Epoch 4/25 | Batch 421/3680 | Loss: 0.3774\n","Epoch 4/25 | Batch 451/3680 | Loss: 0.1370\n","Epoch 4/25 | Batch 481/3680 | Loss: 0.2001\n","Epoch 4/25 | Batch 511/3680 | Loss: 0.1828\n","Epoch 4/25 | Batch 541/3680 | Loss: 0.4160\n","Epoch 4/25 | Batch 571/3680 | Loss: 0.2657\n","Epoch 4/25 | Batch 601/3680 | Loss: 0.0749\n","Epoch 4/25 | Batch 631/3680 | Loss: 0.1704\n","Epoch 4/25 | Batch 661/3680 | Loss: 0.1765\n","Epoch 4/25 | Batch 691/3680 | Loss: 0.6569\n","Epoch 4/25 | Batch 721/3680 | Loss: 0.2920\n","Epoch 4/25 | Batch 751/3680 | Loss: 0.0963\n","Epoch 4/25 | Batch 781/3680 | Loss: 0.3876\n","Epoch 4/25 | Batch 811/3680 | Loss: 0.3240\n","Epoch 4/25 | Batch 841/3680 | Loss: 0.1757\n","Epoch 4/25 | Batch 871/3680 | Loss: 0.1569\n","Epoch 4/25 | Batch 901/3680 | Loss: 0.1922\n","Epoch 4/25 | Batch 931/3680 | Loss: 0.2157\n","Epoch 4/25 | Batch 961/3680 | Loss: 0.5674\n","Epoch 4/25 | Batch 991/3680 | Loss: 0.1366\n","Epoch 4/25 | Batch 1021/3680 | Loss: 0.0391\n","Epoch 4/25 | Batch 1051/3680 | Loss: 0.2381\n","Epoch 4/25 | Batch 1081/3680 | Loss: 0.1564\n","Epoch 4/25 | Batch 1111/3680 | Loss: 0.1159\n","Epoch 4/25 | Batch 1141/3680 | Loss: 0.2487\n","Epoch 4/25 | Batch 1171/3680 | Loss: 0.3770\n","Epoch 4/25 | Batch 1201/3680 | Loss: 0.4507\n","Epoch 4/25 | Batch 1231/3680 | Loss: 0.2969\n","Epoch 4/25 | Batch 1261/3680 | Loss: 0.3502\n","Epoch 4/25 | Batch 1291/3680 | Loss: 0.4592\n","Epoch 4/25 | Batch 1321/3680 | Loss: 0.6056\n","Epoch 4/25 | Batch 1351/3680 | Loss: 0.0998\n","Epoch 4/25 | Batch 1381/3680 | Loss: 0.3113\n","Epoch 4/25 | Batch 1411/3680 | Loss: 0.2207\n","Epoch 4/25 | Batch 1441/3680 | Loss: 0.2062\n","Epoch 4/25 | Batch 1471/3680 | Loss: 0.1661\n","Epoch 4/25 | Batch 1501/3680 | Loss: 0.2207\n","Epoch 4/25 | Batch 1531/3680 | Loss: 0.3325\n","Epoch 4/25 | Batch 1561/3680 | Loss: 0.1126\n","Epoch 4/25 | Batch 1591/3680 | Loss: 0.0603\n","Epoch 4/25 | Batch 1621/3680 | Loss: 0.1158\n","Epoch 4/25 | Batch 1651/3680 | Loss: 0.5124\n","Epoch 4/25 | Batch 1681/3680 | Loss: 0.0856\n","Epoch 4/25 | Batch 1711/3680 | Loss: 0.1758\n","Epoch 4/25 | Batch 1741/3680 | Loss: 0.3672\n","Epoch 4/25 | Batch 1771/3680 | Loss: 0.1451\n","Epoch 4/25 | Batch 1801/3680 | Loss: 0.2135\n","Epoch 4/25 | Batch 1831/3680 | Loss: 0.1391\n","Epoch 4/25 | Batch 1861/3680 | Loss: 0.1475\n","Epoch 4/25 | Batch 1891/3680 | Loss: 0.3533\n","Epoch 4/25 | Batch 1921/3680 | Loss: 0.3082\n","Epoch 4/25 | Batch 1951/3680 | Loss: 0.1932\n","Epoch 4/25 | Batch 1981/3680 | Loss: 0.3383\n","Epoch 4/25 | Batch 2011/3680 | Loss: 0.4017\n","Epoch 4/25 | Batch 2041/3680 | Loss: 0.1102\n","Epoch 4/25 | Batch 2071/3680 | Loss: 0.2371\n","Epoch 4/25 | Batch 2101/3680 | Loss: 0.3784\n","Epoch 4/25 | Batch 2131/3680 | Loss: 0.3167\n","Epoch 4/25 | Batch 2161/3680 | Loss: 0.0820\n","Epoch 4/25 | Batch 2191/3680 | Loss: 0.4369\n","Epoch 4/25 | Batch 2221/3680 | Loss: 0.3559\n","Epoch 4/25 | Batch 2251/3680 | Loss: 0.2691\n","Epoch 4/25 | Batch 2281/3680 | Loss: 0.2131\n","Epoch 4/25 | Batch 2311/3680 | Loss: 0.1749\n","Epoch 4/25 | Batch 2341/3680 | Loss: 0.2029\n","Epoch 4/25 | Batch 2371/3680 | Loss: 0.1640\n","Epoch 4/25 | Batch 2401/3680 | Loss: 0.1830\n","Epoch 4/25 | Batch 2431/3680 | Loss: 0.1880\n","Epoch 4/25 | Batch 2461/3680 | Loss: 0.1797\n","Epoch 4/25 | Batch 2491/3680 | Loss: 0.4871\n","Epoch 4/25 | Batch 2521/3680 | Loss: 0.4355\n","Epoch 4/25 | Batch 2551/3680 | Loss: 0.3759\n","Epoch 4/25 | Batch 2581/3680 | Loss: 0.2152\n","Epoch 4/25 | Batch 2611/3680 | Loss: 0.2247\n","Epoch 4/25 | Batch 2641/3680 | Loss: 0.2765\n","Epoch 4/25 | Batch 2671/3680 | Loss: 0.1509\n","Epoch 4/25 | Batch 2701/3680 | Loss: 0.0582\n","Epoch 4/25 | Batch 2731/3680 | Loss: 0.1097\n","Epoch 4/25 | Batch 2761/3680 | Loss: 0.1115\n","Epoch 4/25 | Batch 2791/3680 | Loss: 0.2296\n","Epoch 4/25 | Batch 2821/3680 | Loss: 0.1897\n","Epoch 4/25 | Batch 2851/3680 | Loss: 0.3512\n","Epoch 4/25 | Batch 2881/3680 | Loss: 0.0682\n","Epoch 4/25 | Batch 2911/3680 | Loss: 0.1108\n","Epoch 4/25 | Batch 2941/3680 | Loss: 0.5453\n","Epoch 4/25 | Batch 2971/3680 | Loss: 0.0472\n","Epoch 4/25 | Batch 3001/3680 | Loss: 0.4767\n","Epoch 4/25 | Batch 3031/3680 | Loss: 0.1394\n","Epoch 4/25 | Batch 3061/3680 | Loss: 0.2072\n","Epoch 4/25 | Batch 3091/3680 | Loss: 0.2449\n","Epoch 4/25 | Batch 3121/3680 | Loss: 0.4164\n","Epoch 4/25 | Batch 3151/3680 | Loss: 0.2122\n","Epoch 4/25 | Batch 3181/3680 | Loss: 0.3033\n","Epoch 4/25 | Batch 3211/3680 | Loss: 0.2901\n","Epoch 4/25 | Batch 3241/3680 | Loss: 0.2874\n","Epoch 4/25 | Batch 3271/3680 | Loss: 0.0412\n","Epoch 4/25 | Batch 3301/3680 | Loss: 0.4039\n","Epoch 4/25 | Batch 3331/3680 | Loss: 0.5482\n","Epoch 4/25 | Batch 3361/3680 | Loss: 0.5271\n","Epoch 4/25 | Batch 3391/3680 | Loss: 0.5218\n","Epoch 4/25 | Batch 3421/3680 | Loss: 0.4100\n","Epoch 4/25 | Batch 3451/3680 | Loss: 0.2698\n","Epoch 4/25 | Batch 3481/3680 | Loss: 0.3943\n","Epoch 4/25 | Batch 3511/3680 | Loss: 0.4614\n","Epoch 4/25 | Batch 3541/3680 | Loss: 0.1689\n","Epoch 4/25 | Batch 3571/3680 | Loss: 0.2017\n","Epoch 4/25 | Batch 3601/3680 | Loss: 0.1397\n","Epoch 4/25 | Batch 3631/3680 | Loss: 0.4528\n","Epoch 4/25 | Batch 3661/3680 | Loss: 0.6467\n","Average training loss: 0.28\n","Epoch 5/25\n","Epoch 5/25 | Batch 31/3680 | Loss: 0.0951\n","Epoch 5/25 | Batch 61/3680 | Loss: 0.1049\n","Epoch 5/25 | Batch 91/3680 | Loss: 0.1906\n","Epoch 5/25 | Batch 121/3680 | Loss: 0.0923\n","Epoch 5/25 | Batch 151/3680 | Loss: 0.2182\n","Epoch 5/25 | Batch 181/3680 | Loss: 0.0537\n","Epoch 5/25 | Batch 211/3680 | Loss: 0.1913\n","Epoch 5/25 | Batch 241/3680 | Loss: 0.0482\n","Epoch 5/25 | Batch 271/3680 | Loss: 0.1413\n","Epoch 5/25 | Batch 301/3680 | Loss: 0.1304\n","Epoch 5/25 | Batch 331/3680 | Loss: 0.2176\n","Epoch 5/25 | Batch 361/3680 | Loss: 0.2284\n","Epoch 5/25 | Batch 391/3680 | Loss: 0.2584\n","Epoch 5/25 | Batch 421/3680 | Loss: 0.1099\n","Epoch 5/25 | Batch 451/3680 | Loss: 0.2996\n","Epoch 5/25 | Batch 481/3680 | Loss: 0.1222\n","Epoch 5/25 | Batch 511/3680 | Loss: 0.1898\n","Epoch 5/25 | Batch 541/3680 | Loss: 0.1955\n","Epoch 5/25 | Batch 571/3680 | Loss: 0.1540\n","Epoch 5/25 | Batch 601/3680 | Loss: 0.2416\n","Epoch 5/25 | Batch 631/3680 | Loss: 0.0642\n","Epoch 5/25 | Batch 661/3680 | Loss: 0.3169\n","Epoch 5/25 | Batch 691/3680 | Loss: 0.4141\n","Epoch 5/25 | Batch 721/3680 | Loss: 0.0513\n","Epoch 5/25 | Batch 751/3680 | Loss: 0.3349\n","Epoch 5/25 | Batch 781/3680 | Loss: 0.3297\n","Epoch 5/25 | Batch 811/3680 | Loss: 0.1806\n","Epoch 5/25 | Batch 841/3680 | Loss: 0.1101\n","Epoch 5/25 | Batch 871/3680 | Loss: 0.0904\n","Epoch 5/25 | Batch 901/3680 | Loss: 0.2419\n","Epoch 5/25 | Batch 931/3680 | Loss: 0.1793\n","Epoch 5/25 | Batch 961/3680 | Loss: 0.1590\n","Epoch 5/25 | Batch 991/3680 | Loss: 0.0489\n","Epoch 5/25 | Batch 1021/3680 | Loss: 0.1937\n","Epoch 5/25 | Batch 1051/3680 | Loss: 0.1928\n","Epoch 5/25 | Batch 1081/3680 | Loss: 0.1275\n","Epoch 5/25 | Batch 1111/3680 | Loss: 0.0550\n","Epoch 5/25 | Batch 1141/3680 | Loss: 0.0405\n","Epoch 5/25 | Batch 1171/3680 | Loss: 0.2765\n","Epoch 5/25 | Batch 1201/3680 | Loss: 0.0877\n","Epoch 5/25 | Batch 1231/3680 | Loss: 0.1569\n","Epoch 5/25 | Batch 1261/3680 | Loss: 0.2233\n","Epoch 5/25 | Batch 1291/3680 | Loss: 0.0271\n","Epoch 5/25 | Batch 1321/3680 | Loss: 0.2367\n","Epoch 5/25 | Batch 1351/3680 | Loss: 0.2183\n","Epoch 5/25 | Batch 1381/3680 | Loss: 0.1304\n","Epoch 5/25 | Batch 1411/3680 | Loss: 0.7436\n","Epoch 5/25 | Batch 1441/3680 | Loss: 0.2015\n","Epoch 5/25 | Batch 1471/3680 | Loss: 0.1626\n","Epoch 5/25 | Batch 1501/3680 | Loss: 0.1283\n","Epoch 5/25 | Batch 1531/3680 | Loss: 0.0826\n","Epoch 5/25 | Batch 1561/3680 | Loss: 0.2759\n","Epoch 5/25 | Batch 1591/3680 | Loss: 0.0284\n","Epoch 5/25 | Batch 1621/3680 | Loss: 0.1822\n","Epoch 5/25 | Batch 1651/3680 | Loss: 0.1409\n","Epoch 5/25 | Batch 1681/3680 | Loss: 0.4066\n","Epoch 5/25 | Batch 1711/3680 | Loss: 0.2542\n","Epoch 5/25 | Batch 1741/3680 | Loss: 0.2284\n","Epoch 5/25 | Batch 1771/3680 | Loss: 0.1675\n","Epoch 5/25 | Batch 1801/3680 | Loss: 0.0276\n","Epoch 5/25 | Batch 1831/3680 | Loss: 0.1688\n","Epoch 5/25 | Batch 1861/3680 | Loss: 0.1007\n","Epoch 5/25 | Batch 1891/3680 | Loss: 0.3003\n","Epoch 5/25 | Batch 1921/3680 | Loss: 0.1035\n","Epoch 5/25 | Batch 1951/3680 | Loss: 0.0735\n","Epoch 5/25 | Batch 1981/3680 | Loss: 0.0604\n","Epoch 5/25 | Batch 2011/3680 | Loss: 0.1299\n","Epoch 5/25 | Batch 2041/3680 | Loss: 0.2363\n","Epoch 5/25 | Batch 2071/3680 | Loss: 0.1064\n","Epoch 5/25 | Batch 2101/3680 | Loss: 0.1271\n","Epoch 5/25 | Batch 2131/3680 | Loss: 0.1140\n","Epoch 5/25 | Batch 2161/3680 | Loss: 0.0853\n","Epoch 5/25 | Batch 2191/3680 | Loss: 0.1811\n","Epoch 5/25 | Batch 2221/3680 | Loss: 0.0298\n","Epoch 5/25 | Batch 2251/3680 | Loss: 0.4304\n","Epoch 5/25 | Batch 2281/3680 | Loss: 0.1913\n","Epoch 5/25 | Batch 2311/3680 | Loss: 0.1344\n","Epoch 5/25 | Batch 2341/3680 | Loss: 0.1141\n","Epoch 5/25 | Batch 2371/3680 | Loss: 0.1412\n","Epoch 5/25 | Batch 2401/3680 | Loss: 0.1104\n","Epoch 5/25 | Batch 2431/3680 | Loss: 0.1129\n","Epoch 5/25 | Batch 2461/3680 | Loss: 0.1643\n","Epoch 5/25 | Batch 2491/3680 | Loss: 0.0284\n","Epoch 5/25 | Batch 2521/3680 | Loss: 0.3316\n","Epoch 5/25 | Batch 2551/3680 | Loss: 0.1450\n","Epoch 5/25 | Batch 2581/3680 | Loss: 0.0940\n","Epoch 5/25 | Batch 2611/3680 | Loss: 0.0714\n","Epoch 5/25 | Batch 2641/3680 | Loss: 0.3184\n","Epoch 5/25 | Batch 2671/3680 | Loss: 0.0480\n","Epoch 5/25 | Batch 2701/3680 | Loss: 0.1693\n","Epoch 5/25 | Batch 2731/3680 | Loss: 0.2595\n","Epoch 5/25 | Batch 2761/3680 | Loss: 0.0866\n","Epoch 5/25 | Batch 2791/3680 | Loss: 0.3824\n","Epoch 5/25 | Batch 2821/3680 | Loss: 0.0756\n","Epoch 5/25 | Batch 2851/3680 | Loss: 0.1038\n","Epoch 5/25 | Batch 2881/3680 | Loss: 0.1322\n","Epoch 5/25 | Batch 2911/3680 | Loss: 0.4153\n","Epoch 5/25 | Batch 2941/3680 | Loss: 0.4078\n","Epoch 5/25 | Batch 2971/3680 | Loss: 0.0387\n","Epoch 5/25 | Batch 3001/3680 | Loss: 0.1029\n","Epoch 5/25 | Batch 3031/3680 | Loss: 0.1015\n","Epoch 5/25 | Batch 3061/3680 | Loss: 0.0611\n","Epoch 5/25 | Batch 3091/3680 | Loss: 0.6829\n","Epoch 5/25 | Batch 3121/3680 | Loss: 0.0821\n","Epoch 5/25 | Batch 3151/3680 | Loss: 0.1105\n","Epoch 5/25 | Batch 3181/3680 | Loss: 0.0249\n","Epoch 5/25 | Batch 3211/3680 | Loss: 0.5799\n","Epoch 5/25 | Batch 3241/3680 | Loss: 0.6004\n","Epoch 5/25 | Batch 3271/3680 | Loss: 0.4369\n","Epoch 5/25 | Batch 3301/3680 | Loss: 0.0978\n","Epoch 5/25 | Batch 3331/3680 | Loss: 0.0167\n","Epoch 5/25 | Batch 3361/3680 | Loss: 0.1048\n","Epoch 5/25 | Batch 3391/3680 | Loss: 0.1503\n","Epoch 5/25 | Batch 3421/3680 | Loss: 0.2093\n","Epoch 5/25 | Batch 3451/3680 | Loss: 0.2314\n","Epoch 5/25 | Batch 3481/3680 | Loss: 0.1696\n","Epoch 5/25 | Batch 3511/3680 | Loss: 0.2323\n","Epoch 5/25 | Batch 3541/3680 | Loss: 0.0901\n","Epoch 5/25 | Batch 3571/3680 | Loss: 0.1892\n","Epoch 5/25 | Batch 3601/3680 | Loss: 0.2259\n","Epoch 5/25 | Batch 3631/3680 | Loss: 0.0345\n","Epoch 5/25 | Batch 3661/3680 | Loss: 0.2072\n","Average training loss: 0.18\n","Epoch 6/25\n","Epoch 6/25 | Batch 31/3680 | Loss: 0.0364\n","Epoch 6/25 | Batch 61/3680 | Loss: 0.0567\n","Epoch 6/25 | Batch 91/3680 | Loss: 0.2475\n","Epoch 6/25 | Batch 121/3680 | Loss: 0.0430\n","Epoch 6/25 | Batch 151/3680 | Loss: 0.1152\n","Epoch 6/25 | Batch 181/3680 | Loss: 0.0702\n","Epoch 6/25 | Batch 211/3680 | Loss: 0.1248\n","Epoch 6/25 | Batch 241/3680 | Loss: 0.2676\n","Epoch 6/25 | Batch 271/3680 | Loss: 0.0842\n","Epoch 6/25 | Batch 301/3680 | Loss: 0.1027\n","Epoch 6/25 | Batch 331/3680 | Loss: 0.0121\n","Epoch 6/25 | Batch 361/3680 | Loss: 0.2091\n","Epoch 6/25 | Batch 391/3680 | Loss: 0.3401\n","Epoch 6/25 | Batch 421/3680 | Loss: 0.1600\n","Epoch 6/25 | Batch 451/3680 | Loss: 0.0724\n","Epoch 6/25 | Batch 481/3680 | Loss: 0.0343\n","Epoch 6/25 | Batch 511/3680 | Loss: 0.1657\n","Epoch 6/25 | Batch 541/3680 | Loss: 0.0083\n","Epoch 6/25 | Batch 571/3680 | Loss: 0.1817\n","Epoch 6/25 | Batch 601/3680 | Loss: 0.1058\n","Epoch 6/25 | Batch 631/3680 | Loss: 0.0475\n","Epoch 6/25 | Batch 661/3680 | Loss: 0.0712\n","Epoch 6/25 | Batch 691/3680 | Loss: 0.1108\n","Epoch 6/25 | Batch 721/3680 | Loss: 0.1893\n","Epoch 6/25 | Batch 751/3680 | Loss: 0.0300\n","Epoch 6/25 | Batch 781/3680 | Loss: 0.0959\n","Epoch 6/25 | Batch 811/3680 | Loss: 0.1276\n","Epoch 6/25 | Batch 841/3680 | Loss: 0.0449\n","Epoch 6/25 | Batch 871/3680 | Loss: 0.1536\n","Epoch 6/25 | Batch 901/3680 | Loss: 0.4295\n","Epoch 6/25 | Batch 931/3680 | Loss: 0.2404\n","Epoch 6/25 | Batch 961/3680 | Loss: 0.2450\n","Epoch 6/25 | Batch 991/3680 | Loss: 0.0564\n","Epoch 6/25 | Batch 1021/3680 | Loss: 0.0645\n","Epoch 6/25 | Batch 1051/3680 | Loss: 0.0680\n","Epoch 6/25 | Batch 1081/3680 | Loss: 0.0813\n","Epoch 6/25 | Batch 1111/3680 | Loss: 0.1158\n","Epoch 6/25 | Batch 1141/3680 | Loss: 0.0218\n","Epoch 6/25 | Batch 1171/3680 | Loss: 0.0118\n","Epoch 6/25 | Batch 1201/3680 | Loss: 0.0191\n","Epoch 6/25 | Batch 1231/3680 | Loss: 0.1828\n","Epoch 6/25 | Batch 1261/3680 | Loss: 0.0568\n","Epoch 6/25 | Batch 1291/3680 | Loss: 0.2170\n","Epoch 6/25 | Batch 1321/3680 | Loss: 0.2588\n","Epoch 6/25 | Batch 1351/3680 | Loss: 0.1471\n","Epoch 6/25 | Batch 1381/3680 | Loss: 0.0165\n","Epoch 6/25 | Batch 1411/3680 | Loss: 0.2043\n","Epoch 6/25 | Batch 1441/3680 | Loss: 0.1180\n","Epoch 6/25 | Batch 1471/3680 | Loss: 0.2322\n","Epoch 6/25 | Batch 1501/3680 | Loss: 0.0687\n","Epoch 6/25 | Batch 1531/3680 | Loss: 0.5936\n","Epoch 6/25 | Batch 1561/3680 | Loss: 0.0655\n","Epoch 6/25 | Batch 1591/3680 | Loss: 0.0033\n","Epoch 6/25 | Batch 1621/3680 | Loss: 0.2192\n","Epoch 6/25 | Batch 1651/3680 | Loss: 0.2259\n","Epoch 6/25 | Batch 1681/3680 | Loss: 0.0071\n","Epoch 6/25 | Batch 1711/3680 | Loss: 0.0218\n","Epoch 6/25 | Batch 1741/3680 | Loss: 0.0415\n","Epoch 6/25 | Batch 1771/3680 | Loss: 0.0175\n","Epoch 6/25 | Batch 1801/3680 | Loss: 0.0826\n","Epoch 6/25 | Batch 1831/3680 | Loss: 0.0186\n","Epoch 6/25 | Batch 1861/3680 | Loss: 0.1236\n","Epoch 6/25 | Batch 1891/3680 | Loss: 0.1260\n","Epoch 6/25 | Batch 1921/3680 | Loss: 0.3013\n","Epoch 6/25 | Batch 1951/3680 | Loss: 0.1067\n","Epoch 6/25 | Batch 1981/3680 | Loss: 0.0465\n","Epoch 6/25 | Batch 2011/3680 | Loss: 0.0574\n","Epoch 6/25 | Batch 2041/3680 | Loss: 0.1144\n","Epoch 6/25 | Batch 2071/3680 | Loss: 0.1015\n","Epoch 6/25 | Batch 2101/3680 | Loss: 0.1012\n","Epoch 6/25 | Batch 2131/3680 | Loss: 0.0688\n","Epoch 6/25 | Batch 2161/3680 | Loss: 0.1196\n","Epoch 6/25 | Batch 2191/3680 | Loss: 0.2620\n","Epoch 6/25 | Batch 2221/3680 | Loss: 0.0551\n","Epoch 6/25 | Batch 2251/3680 | Loss: 0.0462\n","Epoch 6/25 | Batch 2281/3680 | Loss: 0.0684\n","Epoch 6/25 | Batch 2311/3680 | Loss: 0.1653\n","Epoch 6/25 | Batch 2341/3680 | Loss: 0.3372\n","Epoch 6/25 | Batch 2371/3680 | Loss: 0.0026\n","Epoch 6/25 | Batch 2401/3680 | Loss: 0.3081\n","Epoch 6/25 | Batch 2431/3680 | Loss: 0.0855\n","Epoch 6/25 | Batch 2461/3680 | Loss: 0.3075\n","Epoch 6/25 | Batch 2491/3680 | Loss: 0.0969\n","Epoch 6/25 | Batch 2521/3680 | Loss: 0.1031\n","Epoch 6/25 | Batch 2551/3680 | Loss: 0.6205\n","Epoch 6/25 | Batch 2581/3680 | Loss: 0.0475\n","Epoch 6/25 | Batch 2611/3680 | Loss: 0.2700\n","Epoch 6/25 | Batch 2641/3680 | Loss: 0.1393\n","Epoch 6/25 | Batch 2671/3680 | Loss: 0.1291\n","Epoch 6/25 | Batch 2701/3680 | Loss: 0.3702\n","Epoch 6/25 | Batch 2731/3680 | Loss: 0.1278\n","Epoch 6/25 | Batch 2761/3680 | Loss: 0.2102\n","Epoch 6/25 | Batch 2791/3680 | Loss: 0.0909\n","Epoch 6/25 | Batch 2821/3680 | Loss: 0.2550\n","Epoch 6/25 | Batch 2851/3680 | Loss: 0.0079\n","Epoch 6/25 | Batch 2881/3680 | Loss: 0.2329\n","Epoch 6/25 | Batch 2911/3680 | Loss: 0.2407\n","Epoch 6/25 | Batch 2941/3680 | Loss: 0.0429\n","Epoch 6/25 | Batch 2971/3680 | Loss: 0.2430\n","Epoch 6/25 | Batch 3001/3680 | Loss: 0.2272\n","Epoch 6/25 | Batch 3031/3680 | Loss: 0.0432\n","Epoch 6/25 | Batch 3061/3680 | Loss: 0.1291\n","Epoch 6/25 | Batch 3091/3680 | Loss: 0.2287\n","Epoch 6/25 | Batch 3121/3680 | Loss: 0.1067\n","Epoch 6/25 | Batch 3151/3680 | Loss: 0.2813\n","Epoch 6/25 | Batch 3181/3680 | Loss: 0.1483\n","Epoch 6/25 | Batch 3211/3680 | Loss: 0.0171\n","Epoch 6/25 | Batch 3241/3680 | Loss: 0.0370\n","Epoch 6/25 | Batch 3271/3680 | Loss: 0.1864\n","Epoch 6/25 | Batch 3301/3680 | Loss: 0.0144\n","Epoch 6/25 | Batch 3331/3680 | Loss: 0.1498\n","Epoch 6/25 | Batch 3361/3680 | Loss: 0.0333\n","Epoch 6/25 | Batch 3391/3680 | Loss: 0.0886\n","Epoch 6/25 | Batch 3421/3680 | Loss: 0.2374\n","Epoch 6/25 | Batch 3451/3680 | Loss: 0.0495\n","Epoch 6/25 | Batch 3481/3680 | Loss: 0.0106\n","Epoch 6/25 | Batch 3511/3680 | Loss: 0.1495\n","Epoch 6/25 | Batch 3541/3680 | Loss: 0.0354\n","Epoch 6/25 | Batch 3571/3680 | Loss: 0.1182\n","Epoch 6/25 | Batch 3601/3680 | Loss: 0.0303\n","Epoch 6/25 | Batch 3631/3680 | Loss: 0.2380\n","Epoch 6/25 | Batch 3661/3680 | Loss: 0.0319\n","Average training loss: 0.13\n","Epoch 7/25\n","Epoch 7/25 | Batch 31/3680 | Loss: 0.0315\n","Epoch 7/25 | Batch 61/3680 | Loss: 0.0750\n","Epoch 7/25 | Batch 91/3680 | Loss: 0.0226\n","Epoch 7/25 | Batch 121/3680 | Loss: 0.0728\n","Epoch 7/25 | Batch 151/3680 | Loss: 0.0819\n","Epoch 7/25 | Batch 181/3680 | Loss: 0.0069\n","Epoch 7/25 | Batch 211/3680 | Loss: 0.0256\n","Epoch 7/25 | Batch 241/3680 | Loss: 0.0951\n","Epoch 7/25 | Batch 271/3680 | Loss: 0.2565\n","Epoch 7/25 | Batch 301/3680 | Loss: 0.0068\n","Epoch 7/25 | Batch 331/3680 | Loss: 0.0664\n","Epoch 7/25 | Batch 361/3680 | Loss: 0.0850\n","Epoch 7/25 | Batch 391/3680 | Loss: 0.0142\n","Epoch 7/25 | Batch 421/3680 | Loss: 0.0038\n","Epoch 7/25 | Batch 451/3680 | Loss: 0.0350\n","Epoch 7/25 | Batch 481/3680 | Loss: 0.0445\n","Epoch 7/25 | Batch 511/3680 | Loss: 0.0725\n","Epoch 7/25 | Batch 541/3680 | Loss: 0.2202\n","Epoch 7/25 | Batch 571/3680 | Loss: 0.0476\n","Epoch 7/25 | Batch 601/3680 | Loss: 0.0822\n","Epoch 7/25 | Batch 631/3680 | Loss: 0.1333\n","Epoch 7/25 | Batch 661/3680 | Loss: 0.0089\n","Epoch 7/25 | Batch 691/3680 | Loss: 0.0398\n","Epoch 7/25 | Batch 721/3680 | Loss: 0.0410\n","Epoch 7/25 | Batch 751/3680 | Loss: 0.0854\n","Epoch 7/25 | Batch 781/3680 | Loss: 0.3745\n","Epoch 7/25 | Batch 811/3680 | Loss: 0.0213\n","Epoch 7/25 | Batch 841/3680 | Loss: 0.0720\n","Epoch 7/25 | Batch 871/3680 | Loss: 0.0261\n","Epoch 7/25 | Batch 901/3680 | Loss: 0.0905\n","Epoch 7/25 | Batch 931/3680 | Loss: 0.0494\n","Epoch 7/25 | Batch 961/3680 | Loss: 0.0046\n","Epoch 7/25 | Batch 991/3680 | Loss: 0.1643\n","Epoch 7/25 | Batch 1021/3680 | Loss: 0.0124\n","Epoch 7/25 | Batch 1051/3680 | Loss: 0.0335\n","Epoch 7/25 | Batch 1081/3680 | Loss: 0.0727\n","Epoch 7/25 | Batch 1111/3680 | Loss: 0.0172\n","Epoch 7/25 | Batch 1141/3680 | Loss: 0.0059\n","Epoch 7/25 | Batch 1171/3680 | Loss: 0.0024\n","Epoch 7/25 | Batch 1201/3680 | Loss: 0.0499\n","Epoch 7/25 | Batch 1231/3680 | Loss: 0.0367\n","Epoch 7/25 | Batch 1261/3680 | Loss: 0.0731\n","Epoch 7/25 | Batch 1291/3680 | Loss: 0.0773\n","Epoch 7/25 | Batch 1321/3680 | Loss: 0.0212\n","Epoch 7/25 | Batch 1351/3680 | Loss: 0.0257\n","Epoch 7/25 | Batch 1381/3680 | Loss: 0.1543\n","Epoch 7/25 | Batch 1411/3680 | Loss: 0.1749\n","Epoch 7/25 | Batch 1441/3680 | Loss: 0.2984\n","Epoch 7/25 | Batch 1471/3680 | Loss: 0.0584\n","Epoch 7/25 | Batch 1501/3680 | Loss: 0.0749\n","Epoch 7/25 | Batch 1531/3680 | Loss: 0.1744\n","Epoch 7/25 | Batch 1561/3680 | Loss: 0.0070\n","Epoch 7/25 | Batch 1591/3680 | Loss: 0.0765\n","Epoch 7/25 | Batch 1621/3680 | Loss: 0.1413\n","Epoch 7/25 | Batch 1651/3680 | Loss: 0.3046\n","Epoch 7/25 | Batch 1681/3680 | Loss: 0.3915\n","Epoch 7/25 | Batch 1711/3680 | Loss: 0.0327\n","Epoch 7/25 | Batch 1741/3680 | Loss: 0.1075\n","Epoch 7/25 | Batch 1771/3680 | Loss: 0.0276\n","Epoch 7/25 | Batch 1801/3680 | Loss: 0.0861\n","Epoch 7/25 | Batch 1831/3680 | Loss: 0.0373\n","Epoch 7/25 | Batch 1861/3680 | Loss: 0.1487\n","Epoch 7/25 | Batch 1891/3680 | Loss: 0.0136\n","Epoch 7/25 | Batch 1921/3680 | Loss: 0.3024\n","Epoch 7/25 | Batch 1951/3680 | Loss: 0.0282\n","Epoch 7/25 | Batch 1981/3680 | Loss: 0.0142\n","Epoch 7/25 | Batch 2011/3680 | Loss: 0.4468\n","Epoch 7/25 | Batch 2041/3680 | Loss: 0.0154\n","Epoch 7/25 | Batch 2071/3680 | Loss: 0.0690\n","Epoch 7/25 | Batch 2101/3680 | Loss: 0.0053\n","Epoch 7/25 | Batch 2131/3680 | Loss: 0.4046\n","Epoch 7/25 | Batch 2161/3680 | Loss: 0.0620\n","Epoch 7/25 | Batch 2191/3680 | Loss: 0.0125\n","Epoch 7/25 | Batch 2221/3680 | Loss: 0.0023\n","Epoch 7/25 | Batch 2251/3680 | Loss: 0.0414\n","Epoch 7/25 | Batch 2281/3680 | Loss: 0.1816\n","Epoch 7/25 | Batch 2311/3680 | Loss: 0.0103\n","Epoch 7/25 | Batch 2341/3680 | Loss: 0.0548\n","Epoch 7/25 | Batch 2371/3680 | Loss: 0.0439\n","Epoch 7/25 | Batch 2401/3680 | Loss: 0.0868\n","Epoch 7/25 | Batch 2431/3680 | Loss: 0.2658\n","Epoch 7/25 | Batch 2461/3680 | Loss: 0.0850\n","Epoch 7/25 | Batch 2491/3680 | Loss: 0.0133\n","Epoch 7/25 | Batch 2521/3680 | Loss: 0.0875\n","Epoch 7/25 | Batch 2551/3680 | Loss: 0.0125\n","Epoch 7/25 | Batch 2581/3680 | Loss: 0.0425\n","Epoch 7/25 | Batch 2611/3680 | Loss: 0.0068\n","Epoch 7/25 | Batch 2641/3680 | Loss: 0.1897\n","Epoch 7/25 | Batch 2671/3680 | Loss: 0.1509\n","Epoch 7/25 | Batch 2701/3680 | Loss: 0.1494\n","Epoch 7/25 | Batch 2731/3680 | Loss: 0.1862\n","Epoch 7/25 | Batch 2761/3680 | Loss: 0.0409\n","Epoch 7/25 | Batch 2791/3680 | Loss: 0.1486\n","Epoch 7/25 | Batch 2821/3680 | Loss: 0.0296\n","Epoch 7/25 | Batch 2851/3680 | Loss: 0.0234\n","Epoch 7/25 | Batch 2881/3680 | Loss: 0.0968\n","Epoch 7/25 | Batch 2911/3680 | Loss: 0.0917\n","Epoch 7/25 | Batch 2941/3680 | Loss: 0.1385\n","Epoch 7/25 | Batch 2971/3680 | Loss: 0.0116\n","Epoch 7/25 | Batch 3001/3680 | Loss: 0.0032\n","Epoch 7/25 | Batch 3031/3680 | Loss: 0.0170\n","Epoch 7/25 | Batch 3061/3680 | Loss: 0.0896\n","Epoch 7/25 | Batch 3091/3680 | Loss: 0.0058\n","Epoch 7/25 | Batch 3121/3680 | Loss: 0.0767\n","Epoch 7/25 | Batch 3151/3680 | Loss: 0.0081\n","Epoch 7/25 | Batch 3181/3680 | Loss: 0.1090\n","Epoch 7/25 | Batch 3211/3680 | Loss: 0.0044\n","Epoch 7/25 | Batch 3241/3680 | Loss: 0.2128\n","Epoch 7/25 | Batch 3271/3680 | Loss: 0.0939\n","Epoch 7/25 | Batch 3301/3680 | Loss: 0.0059\n","Epoch 7/25 | Batch 3331/3680 | Loss: 0.1608\n","Epoch 7/25 | Batch 3361/3680 | Loss: 0.5523\n","Epoch 7/25 | Batch 3391/3680 | Loss: 0.1314\n","Epoch 7/25 | Batch 3421/3680 | Loss: 0.2153\n","Epoch 7/25 | Batch 3451/3680 | Loss: 0.1361\n","Epoch 7/25 | Batch 3481/3680 | Loss: 0.0510\n","Epoch 7/25 | Batch 3511/3680 | Loss: 0.0769\n","Epoch 7/25 | Batch 3541/3680 | Loss: 0.1998\n","Epoch 7/25 | Batch 3571/3680 | Loss: 0.1525\n","Epoch 7/25 | Batch 3601/3680 | Loss: 0.0549\n","Epoch 7/25 | Batch 3631/3680 | Loss: 0.0418\n","Epoch 7/25 | Batch 3661/3680 | Loss: 0.0278\n","Average training loss: 0.10\n","Epoch 8/25\n","Epoch 8/25 | Batch 31/3680 | Loss: 0.1484\n","Epoch 8/25 | Batch 61/3680 | Loss: 0.0104\n","Epoch 8/25 | Batch 91/3680 | Loss: 0.0234\n","Epoch 8/25 | Batch 121/3680 | Loss: 0.1255\n","Epoch 8/25 | Batch 151/3680 | Loss: 0.0193\n","Epoch 8/25 | Batch 181/3680 | Loss: 0.0365\n","Epoch 8/25 | Batch 211/3680 | Loss: 0.0508\n","Epoch 8/25 | Batch 241/3680 | Loss: 0.0157\n","Epoch 8/25 | Batch 271/3680 | Loss: 0.0346\n","Epoch 8/25 | Batch 301/3680 | Loss: 0.0038\n","Epoch 8/25 | Batch 331/3680 | Loss: 0.0648\n","Epoch 8/25 | Batch 361/3680 | Loss: 0.0022\n","Epoch 8/25 | Batch 391/3680 | Loss: 0.0026\n","Epoch 8/25 | Batch 421/3680 | Loss: 0.1696\n","Epoch 8/25 | Batch 451/3680 | Loss: 0.0050\n","Epoch 8/25 | Batch 481/3680 | Loss: 0.0089\n","Epoch 8/25 | Batch 511/3680 | Loss: 0.0038\n","Epoch 8/25 | Batch 541/3680 | Loss: 0.0058\n","Epoch 8/25 | Batch 571/3680 | Loss: 0.1401\n","Epoch 8/25 | Batch 601/3680 | Loss: 0.0432\n","Epoch 8/25 | Batch 631/3680 | Loss: 0.0384\n","Epoch 8/25 | Batch 661/3680 | Loss: 0.1050\n","Epoch 8/25 | Batch 691/3680 | Loss: 0.0228\n","Epoch 8/25 | Batch 721/3680 | Loss: 0.0300\n","Epoch 8/25 | Batch 751/3680 | Loss: 0.0448\n","Epoch 8/25 | Batch 781/3680 | Loss: 0.0962\n","Epoch 8/25 | Batch 811/3680 | Loss: 0.0067\n","Epoch 8/25 | Batch 841/3680 | Loss: 0.0054\n","Epoch 8/25 | Batch 871/3680 | Loss: 0.0055\n","Epoch 8/25 | Batch 901/3680 | Loss: 0.0279\n","Epoch 8/25 | Batch 931/3680 | Loss: 0.0867\n","Epoch 8/25 | Batch 961/3680 | Loss: 0.0688\n","Epoch 8/25 | Batch 991/3680 | Loss: 0.0788\n","Epoch 8/25 | Batch 1021/3680 | Loss: 0.1179\n","Epoch 8/25 | Batch 1051/3680 | Loss: 0.0063\n","Epoch 8/25 | Batch 1081/3680 | Loss: 0.1845\n","Epoch 8/25 | Batch 1111/3680 | Loss: 0.1086\n","Epoch 8/25 | Batch 1141/3680 | Loss: 0.0146\n","Epoch 8/25 | Batch 1171/3680 | Loss: 0.0284\n","Epoch 8/25 | Batch 1201/3680 | Loss: 0.0849\n","Epoch 8/25 | Batch 1231/3680 | Loss: 0.1091\n","Epoch 8/25 | Batch 1261/3680 | Loss: 0.0056\n","Epoch 8/25 | Batch 1291/3680 | Loss: 0.0812\n","Epoch 8/25 | Batch 1321/3680 | Loss: 0.0365\n","Epoch 8/25 | Batch 1351/3680 | Loss: 0.0050\n","Epoch 8/25 | Batch 1381/3680 | Loss: 0.2470\n","Epoch 8/25 | Batch 1411/3680 | Loss: 0.1883\n","Epoch 8/25 | Batch 1441/3680 | Loss: 0.0012\n","Epoch 8/25 | Batch 1471/3680 | Loss: 0.0212\n","Epoch 8/25 | Batch 1501/3680 | Loss: 0.3992\n","Epoch 8/25 | Batch 1531/3680 | Loss: 0.0251\n","Epoch 8/25 | Batch 1561/3680 | Loss: 0.1612\n","Epoch 8/25 | Batch 1591/3680 | Loss: 0.0158\n","Epoch 8/25 | Batch 1621/3680 | Loss: 0.0051\n","Epoch 8/25 | Batch 1651/3680 | Loss: 0.0523\n","Epoch 8/25 | Batch 1681/3680 | Loss: 0.1249\n","Epoch 8/25 | Batch 1711/3680 | Loss: 0.0148\n","Epoch 8/25 | Batch 1741/3680 | Loss: 0.0247\n","Epoch 8/25 | Batch 1771/3680 | Loss: 0.0025\n","Epoch 8/25 | Batch 1801/3680 | Loss: 0.0462\n","Epoch 8/25 | Batch 1831/3680 | Loss: 0.0158\n","Epoch 8/25 | Batch 1861/3680 | Loss: 0.0315\n","Epoch 8/25 | Batch 1891/3680 | Loss: 0.0040\n","Epoch 8/25 | Batch 1921/3680 | Loss: 0.1027\n","Epoch 8/25 | Batch 1951/3680 | Loss: 0.0073\n","Epoch 8/25 | Batch 1981/3680 | Loss: 0.0085\n","Epoch 8/25 | Batch 2011/3680 | Loss: 0.2077\n","Epoch 8/25 | Batch 2041/3680 | Loss: 0.1701\n","Epoch 8/25 | Batch 2071/3680 | Loss: 0.2127\n","Epoch 8/25 | Batch 2101/3680 | Loss: 0.0528\n","Epoch 8/25 | Batch 2131/3680 | Loss: 0.0139\n","Epoch 8/25 | Batch 2161/3680 | Loss: 0.0631\n","Epoch 8/25 | Batch 2191/3680 | Loss: 0.0265\n","Epoch 8/25 | Batch 2221/3680 | Loss: 0.0691\n","Epoch 8/25 | Batch 2251/3680 | Loss: 0.1041\n","Epoch 8/25 | Batch 2281/3680 | Loss: 0.0088\n","Epoch 8/25 | Batch 2311/3680 | Loss: 0.1811\n","Epoch 8/25 | Batch 2341/3680 | Loss: 0.0529\n","Epoch 8/25 | Batch 2371/3680 | Loss: 0.0055\n","Epoch 8/25 | Batch 2401/3680 | Loss: 0.0061\n","Epoch 8/25 | Batch 2431/3680 | Loss: 0.0534\n","Epoch 8/25 | Batch 2461/3680 | Loss: 0.0794\n","Epoch 8/25 | Batch 2491/3680 | Loss: 0.0030\n","Epoch 8/25 | Batch 2521/3680 | Loss: 0.0064\n","Epoch 8/25 | Batch 2551/3680 | Loss: 0.0150\n","Epoch 8/25 | Batch 2581/3680 | Loss: 0.0074\n","Epoch 8/25 | Batch 2611/3680 | Loss: 0.0699\n","Epoch 8/25 | Batch 2641/3680 | Loss: 0.0048\n","Epoch 8/25 | Batch 2671/3680 | Loss: 0.2259\n","Epoch 8/25 | Batch 2701/3680 | Loss: 0.0098\n","Epoch 8/25 | Batch 2731/3680 | Loss: 0.0286\n","Epoch 8/25 | Batch 2761/3680 | Loss: 0.0120\n","Epoch 8/25 | Batch 2791/3680 | Loss: 0.0159\n","Epoch 8/25 | Batch 2821/3680 | Loss: 0.0129\n","Epoch 8/25 | Batch 2851/3680 | Loss: 0.1586\n","Epoch 8/25 | Batch 2881/3680 | Loss: 0.0381\n","Epoch 8/25 | Batch 2911/3680 | Loss: 0.2712\n","Epoch 8/25 | Batch 2941/3680 | Loss: 0.1562\n","Epoch 8/25 | Batch 2971/3680 | Loss: 0.0372\n","Epoch 8/25 | Batch 3001/3680 | Loss: 0.0031\n","Epoch 8/25 | Batch 3031/3680 | Loss: 0.1189\n","Epoch 8/25 | Batch 3061/3680 | Loss: 0.2187\n","Epoch 8/25 | Batch 3091/3680 | Loss: 0.2597\n","Epoch 8/25 | Batch 3121/3680 | Loss: 0.4447\n","Epoch 8/25 | Batch 3151/3680 | Loss: 0.0028\n","Epoch 8/25 | Batch 3181/3680 | Loss: 0.0036\n","Epoch 8/25 | Batch 3211/3680 | Loss: 0.0076\n","Epoch 8/25 | Batch 3241/3680 | Loss: 0.0075\n","Epoch 8/25 | Batch 3271/3680 | Loss: 0.2762\n","Epoch 8/25 | Batch 3301/3680 | Loss: 0.0044\n","Epoch 8/25 | Batch 3331/3680 | Loss: 0.1346\n","Epoch 8/25 | Batch 3361/3680 | Loss: 0.1597\n","Epoch 8/25 | Batch 3391/3680 | Loss: 0.0261\n","Epoch 8/25 | Batch 3421/3680 | Loss: 0.0299\n","Epoch 8/25 | Batch 3451/3680 | Loss: 0.1479\n","Epoch 8/25 | Batch 3481/3680 | Loss: 0.0103\n","Epoch 8/25 | Batch 3511/3680 | Loss: 0.0822\n","Epoch 8/25 | Batch 3541/3680 | Loss: 0.2856\n","Epoch 8/25 | Batch 3571/3680 | Loss: 0.0535\n","Epoch 8/25 | Batch 3601/3680 | Loss: 0.5685\n","Epoch 8/25 | Batch 3631/3680 | Loss: 0.0546\n","Epoch 8/25 | Batch 3661/3680 | Loss: 0.0086\n","Average training loss: 0.08\n","Epoch 9/25\n","Epoch 9/25 | Batch 31/3680 | Loss: 0.0561\n","Epoch 9/25 | Batch 61/3680 | Loss: 0.0208\n","Epoch 9/25 | Batch 91/3680 | Loss: 0.2069\n","Epoch 9/25 | Batch 121/3680 | Loss: 0.0524\n","Epoch 9/25 | Batch 151/3680 | Loss: 0.0443\n","Epoch 9/25 | Batch 181/3680 | Loss: 0.1141\n","Epoch 9/25 | Batch 211/3680 | Loss: 0.0050\n","Epoch 9/25 | Batch 241/3680 | Loss: 0.0031\n","Epoch 9/25 | Batch 271/3680 | Loss: 0.0135\n","Epoch 9/25 | Batch 301/3680 | Loss: 0.0146\n","Epoch 9/25 | Batch 331/3680 | Loss: 0.0133\n","Epoch 9/25 | Batch 361/3680 | Loss: 0.0052\n","Epoch 9/25 | Batch 391/3680 | Loss: 0.0938\n","Epoch 9/25 | Batch 421/3680 | Loss: 0.1287\n","Epoch 9/25 | Batch 451/3680 | Loss: 0.0127\n","Epoch 9/25 | Batch 481/3680 | Loss: 0.0048\n","Epoch 9/25 | Batch 511/3680 | Loss: 0.0559\n","Epoch 9/25 | Batch 541/3680 | Loss: 0.0073\n","Epoch 9/25 | Batch 571/3680 | Loss: 0.0038\n","Epoch 9/25 | Batch 601/3680 | Loss: 0.0114\n","Epoch 9/25 | Batch 631/3680 | Loss: 0.0510\n","Epoch 9/25 | Batch 661/3680 | Loss: 0.1650\n","Epoch 9/25 | Batch 691/3680 | Loss: 0.0113\n","Epoch 9/25 | Batch 721/3680 | Loss: 0.0036\n","Epoch 9/25 | Batch 751/3680 | Loss: 0.0095\n","Epoch 9/25 | Batch 781/3680 | Loss: 0.2237\n","Epoch 9/25 | Batch 811/3680 | Loss: 0.0049\n","Epoch 9/25 | Batch 841/3680 | Loss: 0.1589\n","Epoch 9/25 | Batch 871/3680 | Loss: 0.0027\n","Epoch 9/25 | Batch 901/3680 | Loss: 0.0263\n","Epoch 9/25 | Batch 931/3680 | Loss: 0.1102\n","Epoch 9/25 | Batch 961/3680 | Loss: 0.0013\n","Epoch 9/25 | Batch 991/3680 | Loss: 0.0059\n","Epoch 9/25 | Batch 1021/3680 | Loss: 0.0074\n","Epoch 9/25 | Batch 1051/3680 | Loss: 0.0163\n","Epoch 9/25 | Batch 1081/3680 | Loss: 0.0148\n","Epoch 9/25 | Batch 1111/3680 | Loss: 0.0036\n","Epoch 9/25 | Batch 1141/3680 | Loss: 0.0029\n","Epoch 9/25 | Batch 1171/3680 | Loss: 0.0051\n","Epoch 9/25 | Batch 1201/3680 | Loss: 0.1594\n","Epoch 9/25 | Batch 1231/3680 | Loss: 0.1709\n","Epoch 9/25 | Batch 1261/3680 | Loss: 0.0115\n","Epoch 9/25 | Batch 1291/3680 | Loss: 0.0040\n","Epoch 9/25 | Batch 1321/3680 | Loss: 0.1025\n","Epoch 9/25 | Batch 1351/3680 | Loss: 0.0208\n","Epoch 9/25 | Batch 1381/3680 | Loss: 0.0115\n","Epoch 9/25 | Batch 1411/3680 | Loss: 0.0973\n","Epoch 9/25 | Batch 1441/3680 | Loss: 0.0118\n","Epoch 9/25 | Batch 1471/3680 | Loss: 0.0685\n","Epoch 9/25 | Batch 1501/3680 | Loss: 0.0058\n","Epoch 9/25 | Batch 1531/3680 | Loss: 0.0149\n","Epoch 9/25 | Batch 1561/3680 | Loss: 0.0187\n","Epoch 9/25 | Batch 1591/3680 | Loss: 0.0872\n","Epoch 9/25 | Batch 1621/3680 | Loss: 0.0128\n","Epoch 9/25 | Batch 1651/3680 | Loss: 0.0889\n","Epoch 9/25 | Batch 1681/3680 | Loss: 0.1772\n","Epoch 9/25 | Batch 1711/3680 | Loss: 0.1653\n","Epoch 9/25 | Batch 1741/3680 | Loss: 0.0071\n","Epoch 9/25 | Batch 1771/3680 | Loss: 0.0135\n","Epoch 9/25 | Batch 1801/3680 | Loss: 0.0254\n","Epoch 9/25 | Batch 1831/3680 | Loss: 0.0017\n","Epoch 9/25 | Batch 1861/3680 | Loss: 0.0379\n","Epoch 9/25 | Batch 1891/3680 | Loss: 0.0044\n","Epoch 9/25 | Batch 1921/3680 | Loss: 0.0570\n","Epoch 9/25 | Batch 1951/3680 | Loss: 0.0116\n","Epoch 9/25 | Batch 1981/3680 | Loss: 0.1043\n","Epoch 9/25 | Batch 2011/3680 | Loss: 0.0044\n","Epoch 9/25 | Batch 2041/3680 | Loss: 0.0360\n","Epoch 9/25 | Batch 2071/3680 | Loss: 0.1519\n","Epoch 9/25 | Batch 2101/3680 | Loss: 0.0015\n","Epoch 9/25 | Batch 2131/3680 | Loss: 0.0298\n","Epoch 9/25 | Batch 2161/3680 | Loss: 0.0063\n","Epoch 9/25 | Batch 2191/3680 | Loss: 0.0043\n","Epoch 9/25 | Batch 2221/3680 | Loss: 0.0019\n","Epoch 9/25 | Batch 2251/3680 | Loss: 0.0130\n","Epoch 9/25 | Batch 2281/3680 | Loss: 0.0068\n","Epoch 9/25 | Batch 2311/3680 | Loss: 0.2680\n","Epoch 9/25 | Batch 2341/3680 | Loss: 0.0022\n","Epoch 9/25 | Batch 2371/3680 | Loss: 0.0617\n","Epoch 9/25 | Batch 2401/3680 | Loss: 0.0137\n","Epoch 9/25 | Batch 2431/3680 | Loss: 0.0292\n","Epoch 9/25 | Batch 2461/3680 | Loss: 0.0793\n","Epoch 9/25 | Batch 2491/3680 | Loss: 0.0583\n","Epoch 9/25 | Batch 2521/3680 | Loss: 0.0319\n","Epoch 9/25 | Batch 2551/3680 | Loss: 0.0552\n","Epoch 9/25 | Batch 2581/3680 | Loss: 0.0013\n","Epoch 9/25 | Batch 2611/3680 | Loss: 0.0035\n","Epoch 9/25 | Batch 2641/3680 | Loss: 0.0013\n","Epoch 9/25 | Batch 2671/3680 | Loss: 0.0098\n","Epoch 9/25 | Batch 2701/3680 | Loss: 0.1879\n","Epoch 9/25 | Batch 2731/3680 | Loss: 0.0167\n","Epoch 9/25 | Batch 2761/3680 | Loss: 0.0141\n","Epoch 9/25 | Batch 2791/3680 | Loss: 0.0047\n","Epoch 9/25 | Batch 2821/3680 | Loss: 0.1136\n","Epoch 9/25 | Batch 2851/3680 | Loss: 0.0520\n","Epoch 9/25 | Batch 2881/3680 | Loss: 0.0219\n","Epoch 9/25 | Batch 2911/3680 | Loss: 0.0170\n","Epoch 9/25 | Batch 2941/3680 | Loss: 0.0030\n","Epoch 9/25 | Batch 2971/3680 | Loss: 0.0931\n","Epoch 9/25 | Batch 3001/3680 | Loss: 0.0899\n","Epoch 9/25 | Batch 3031/3680 | Loss: 0.0387\n","Epoch 9/25 | Batch 3061/3680 | Loss: 0.0080\n","Epoch 9/25 | Batch 3091/3680 | Loss: 0.2379\n","Epoch 9/25 | Batch 3121/3680 | Loss: 0.0042\n","Epoch 9/25 | Batch 3151/3680 | Loss: 0.1656\n","Epoch 9/25 | Batch 3181/3680 | Loss: 0.3265\n","Epoch 9/25 | Batch 3211/3680 | Loss: 0.0055\n","Epoch 9/25 | Batch 3241/3680 | Loss: 0.0899\n","Epoch 9/25 | Batch 3271/3680 | Loss: 0.0755\n","Epoch 9/25 | Batch 3301/3680 | Loss: 0.4445\n","Epoch 9/25 | Batch 3331/3680 | Loss: 0.0562\n","Epoch 9/25 | Batch 3361/3680 | Loss: 0.0259\n","Epoch 9/25 | Batch 3391/3680 | Loss: 0.0698\n","Epoch 9/25 | Batch 3421/3680 | Loss: 0.0158\n","Epoch 9/25 | Batch 3451/3680 | Loss: 0.0066\n","Epoch 9/25 | Batch 3481/3680 | Loss: 0.0126\n","Epoch 9/25 | Batch 3511/3680 | Loss: 0.0508\n","Epoch 9/25 | Batch 3541/3680 | Loss: 0.1107\n","Epoch 9/25 | Batch 3571/3680 | Loss: 0.0110\n","Epoch 9/25 | Batch 3601/3680 | Loss: 0.0268\n","Epoch 9/25 | Batch 3631/3680 | Loss: 0.1604\n","Epoch 9/25 | Batch 3661/3680 | Loss: 0.0408\n","Average training loss: 0.07\n","Epoch 10/25\n","Epoch 10/25 | Batch 31/3680 | Loss: 0.0220\n","Epoch 10/25 | Batch 61/3680 | Loss: 0.0889\n","Epoch 10/25 | Batch 91/3680 | Loss: 0.0622\n","Epoch 10/25 | Batch 121/3680 | Loss: 0.1172\n","Epoch 10/25 | Batch 151/3680 | Loss: 0.0032\n","Epoch 10/25 | Batch 181/3680 | Loss: 0.0430\n","Epoch 10/25 | Batch 211/3680 | Loss: 0.0014\n","Epoch 10/25 | Batch 241/3680 | Loss: 0.0267\n","Epoch 10/25 | Batch 271/3680 | Loss: 0.0331\n","Epoch 10/25 | Batch 301/3680 | Loss: 0.0062\n","Epoch 10/25 | Batch 331/3680 | Loss: 0.0016\n","Epoch 10/25 | Batch 361/3680 | Loss: 0.1232\n","Epoch 10/25 | Batch 391/3680 | Loss: 0.0412\n","Epoch 10/25 | Batch 421/3680 | Loss: 0.0359\n","Epoch 10/25 | Batch 451/3680 | Loss: 0.1670\n","Epoch 10/25 | Batch 481/3680 | Loss: 0.0831\n","Epoch 10/25 | Batch 511/3680 | Loss: 0.0169\n","Epoch 10/25 | Batch 541/3680 | Loss: 0.0395\n","Epoch 10/25 | Batch 571/3680 | Loss: 0.2214\n","Epoch 10/25 | Batch 601/3680 | Loss: 0.1167\n","Epoch 10/25 | Batch 631/3680 | Loss: 0.0006\n","Epoch 10/25 | Batch 661/3680 | Loss: 0.0319\n","Epoch 10/25 | Batch 691/3680 | Loss: 0.0273\n","Epoch 10/25 | Batch 721/3680 | Loss: 0.0645\n","Epoch 10/25 | Batch 751/3680 | Loss: 0.0034\n","Epoch 10/25 | Batch 781/3680 | Loss: 0.0020\n","Epoch 10/25 | Batch 811/3680 | Loss: 0.0123\n","Epoch 10/25 | Batch 841/3680 | Loss: 0.0293\n","Epoch 10/25 | Batch 871/3680 | Loss: 0.0046\n","Epoch 10/25 | Batch 901/3680 | Loss: 0.0298\n","Epoch 10/25 | Batch 931/3680 | Loss: 0.0274\n","Epoch 10/25 | Batch 961/3680 | Loss: 0.0428\n","Epoch 10/25 | Batch 991/3680 | Loss: 0.0007\n","Epoch 10/25 | Batch 1021/3680 | Loss: 0.0515\n","Epoch 10/25 | Batch 1051/3680 | Loss: 0.0308\n","Epoch 10/25 | Batch 1081/3680 | Loss: 0.1651\n","Epoch 10/25 | Batch 1111/3680 | Loss: 0.0776\n","Epoch 10/25 | Batch 1141/3680 | Loss: 0.1495\n","Epoch 10/25 | Batch 1171/3680 | Loss: 0.0565\n","Epoch 10/25 | Batch 1201/3680 | Loss: 0.0121\n","Epoch 10/25 | Batch 1231/3680 | Loss: 0.0015\n","Epoch 10/25 | Batch 1261/3680 | Loss: 0.0791\n","Epoch 10/25 | Batch 1291/3680 | Loss: 0.0119\n","Epoch 10/25 | Batch 1321/3680 | Loss: 0.0013\n","Epoch 10/25 | Batch 1351/3680 | Loss: 0.0040\n","Epoch 10/25 | Batch 1381/3680 | Loss: 0.0557\n","Epoch 10/25 | Batch 1411/3680 | Loss: 0.0618\n","Epoch 10/25 | Batch 1441/3680 | Loss: 0.0308\n","Epoch 10/25 | Batch 1471/3680 | Loss: 0.0344\n","Epoch 10/25 | Batch 1501/3680 | Loss: 0.0368\n","Epoch 10/25 | Batch 1531/3680 | Loss: 0.0016\n","Epoch 10/25 | Batch 1561/3680 | Loss: 0.1703\n","Epoch 10/25 | Batch 1591/3680 | Loss: 0.0018\n","Epoch 10/25 | Batch 1621/3680 | Loss: 0.2353\n","Epoch 10/25 | Batch 1651/3680 | Loss: 0.0937\n","Epoch 10/25 | Batch 1681/3680 | Loss: 0.0024\n","Epoch 10/25 | Batch 1711/3680 | Loss: 0.1584\n","Epoch 10/25 | Batch 1741/3680 | Loss: 0.1803\n","Epoch 10/25 | Batch 1771/3680 | Loss: 0.0011\n","Epoch 10/25 | Batch 1801/3680 | Loss: 0.0593\n","Epoch 10/25 | Batch 1831/3680 | Loss: 0.0901\n","Epoch 10/25 | Batch 1861/3680 | Loss: 0.0298\n","Epoch 10/25 | Batch 1891/3680 | Loss: 0.0194\n","Epoch 10/25 | Batch 1921/3680 | Loss: 0.0162\n","Epoch 10/25 | Batch 1951/3680 | Loss: 0.0103\n","Epoch 10/25 | Batch 1981/3680 | Loss: 0.0040\n","Epoch 10/25 | Batch 2011/3680 | Loss: 0.0775\n","Epoch 10/25 | Batch 2041/3680 | Loss: 0.0836\n","Epoch 10/25 | Batch 2071/3680 | Loss: 0.0451\n","Epoch 10/25 | Batch 2101/3680 | Loss: 0.0154\n","Epoch 10/25 | Batch 2131/3680 | Loss: 0.2439\n","Epoch 10/25 | Batch 2161/3680 | Loss: 0.1983\n","Epoch 10/25 | Batch 2191/3680 | Loss: 0.0090\n","Epoch 10/25 | Batch 2221/3680 | Loss: 0.0243\n","Epoch 10/25 | Batch 2251/3680 | Loss: 0.2642\n","Epoch 10/25 | Batch 2281/3680 | Loss: 0.0027\n","Epoch 10/25 | Batch 2311/3680 | Loss: 0.2414\n","Epoch 10/25 | Batch 2341/3680 | Loss: 0.0960\n","Epoch 10/25 | Batch 2371/3680 | Loss: 0.1303\n","Epoch 10/25 | Batch 2401/3680 | Loss: 0.0045\n","Epoch 10/25 | Batch 2431/3680 | Loss: 0.3892\n","Epoch 10/25 | Batch 2461/3680 | Loss: 0.0491\n","Epoch 10/25 | Batch 2491/3680 | Loss: 0.0187\n","Epoch 10/25 | Batch 2521/3680 | Loss: 0.0025\n","Epoch 10/25 | Batch 2551/3680 | Loss: 0.1367\n","Epoch 10/25 | Batch 2581/3680 | Loss: 0.0018\n","Epoch 10/25 | Batch 2611/3680 | Loss: 0.0031\n","Epoch 10/25 | Batch 2641/3680 | Loss: 0.2657\n","Epoch 10/25 | Batch 2671/3680 | Loss: 0.0046\n","Epoch 10/25 | Batch 2701/3680 | Loss: 0.0389\n","Epoch 10/25 | Batch 2731/3680 | Loss: 0.0076\n","Epoch 10/25 | Batch 2761/3680 | Loss: 0.2097\n","Epoch 10/25 | Batch 2791/3680 | Loss: 0.0187\n","Epoch 10/25 | Batch 2821/3680 | Loss: 0.0664\n","Epoch 10/25 | Batch 2851/3680 | Loss: 0.2197\n","Epoch 10/25 | Batch 2881/3680 | Loss: 0.0967\n","Epoch 10/25 | Batch 2911/3680 | Loss: 0.0305\n","Epoch 10/25 | Batch 2941/3680 | Loss: 0.0212\n","Epoch 10/25 | Batch 2971/3680 | Loss: 0.0025\n","Epoch 10/25 | Batch 3001/3680 | Loss: 0.0274\n","Epoch 10/25 | Batch 3031/3680 | Loss: 0.0883\n","Epoch 10/25 | Batch 3061/3680 | Loss: 0.0033\n","Epoch 10/25 | Batch 3091/3680 | Loss: 0.0031\n","Epoch 10/25 | Batch 3121/3680 | Loss: 0.0029\n","Epoch 10/25 | Batch 3151/3680 | Loss: 0.0296\n","Epoch 10/25 | Batch 3181/3680 | Loss: 0.0037\n","Epoch 10/25 | Batch 3211/3680 | Loss: 0.0515\n","Epoch 10/25 | Batch 3241/3680 | Loss: 0.1341\n","Epoch 10/25 | Batch 3271/3680 | Loss: 0.0235\n","Epoch 10/25 | Batch 3301/3680 | Loss: 0.0081\n","Epoch 10/25 | Batch 3331/3680 | Loss: 0.0258\n","Epoch 10/25 | Batch 3361/3680 | Loss: 0.0163\n","Epoch 10/25 | Batch 3391/3680 | Loss: 0.0020\n","Epoch 10/25 | Batch 3421/3680 | Loss: 0.0845\n","Epoch 10/25 | Batch 3451/3680 | Loss: 0.1716\n","Epoch 10/25 | Batch 3481/3680 | Loss: 0.0027\n","Epoch 10/25 | Batch 3511/3680 | Loss: 0.0043\n","Epoch 10/25 | Batch 3541/3680 | Loss: 0.0273\n","Epoch 10/25 | Batch 3571/3680 | Loss: 0.0020\n","Epoch 10/25 | Batch 3601/3680 | Loss: 0.0294\n","Epoch 10/25 | Batch 3631/3680 | Loss: 0.0717\n","Epoch 10/25 | Batch 3661/3680 | Loss: 0.1962\n","Average training loss: 0.06\n","Epoch 11/25\n","Epoch 11/25 | Batch 31/3680 | Loss: 0.0065\n","Epoch 11/25 | Batch 61/3680 | Loss: 0.0760\n","Epoch 11/25 | Batch 91/3680 | Loss: 0.0792\n","Epoch 11/25 | Batch 121/3680 | Loss: 0.0552\n","Epoch 11/25 | Batch 151/3680 | Loss: 0.0024\n","Epoch 11/25 | Batch 181/3680 | Loss: 0.0639\n","Epoch 11/25 | Batch 211/3680 | Loss: 0.0411\n","Epoch 11/25 | Batch 241/3680 | Loss: 0.1488\n","Epoch 11/25 | Batch 271/3680 | Loss: 0.2384\n","Epoch 11/25 | Batch 301/3680 | Loss: 0.0941\n","Epoch 11/25 | Batch 331/3680 | Loss: 0.0111\n","Epoch 11/25 | Batch 361/3680 | Loss: 0.1708\n","Epoch 11/25 | Batch 391/3680 | Loss: 0.0330\n","Epoch 11/25 | Batch 421/3680 | Loss: 0.0202\n","Epoch 11/25 | Batch 451/3680 | Loss: 0.0790\n","Epoch 11/25 | Batch 481/3680 | Loss: 0.0047\n","Epoch 11/25 | Batch 511/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 541/3680 | Loss: 0.0558\n","Epoch 11/25 | Batch 571/3680 | Loss: 0.0014\n","Epoch 11/25 | Batch 601/3680 | Loss: 0.0142\n","Epoch 11/25 | Batch 631/3680 | Loss: 0.1537\n","Epoch 11/25 | Batch 661/3680 | Loss: 0.0351\n","Epoch 11/25 | Batch 691/3680 | Loss: 0.0063\n","Epoch 11/25 | Batch 721/3680 | Loss: 0.0018\n","Epoch 11/25 | Batch 751/3680 | Loss: 0.0051\n","Epoch 11/25 | Batch 781/3680 | Loss: 0.0017\n","Epoch 11/25 | Batch 811/3680 | Loss: 0.2361\n","Epoch 11/25 | Batch 841/3680 | Loss: 0.2602\n","Epoch 11/25 | Batch 871/3680 | Loss: 0.0051\n","Epoch 11/25 | Batch 901/3680 | Loss: 0.2250\n","Epoch 11/25 | Batch 931/3680 | Loss: 0.0009\n","Epoch 11/25 | Batch 961/3680 | Loss: 0.0160\n","Epoch 11/25 | Batch 991/3680 | Loss: 0.0584\n","Epoch 11/25 | Batch 1021/3680 | Loss: 0.0495\n","Epoch 11/25 | Batch 1051/3680 | Loss: 0.0007\n","Epoch 11/25 | Batch 1081/3680 | Loss: 0.0455\n","Epoch 11/25 | Batch 1111/3680 | Loss: 0.0071\n","Epoch 11/25 | Batch 1141/3680 | Loss: 0.0016\n","Epoch 11/25 | Batch 1171/3680 | Loss: 0.1199\n","Epoch 11/25 | Batch 1201/3680 | Loss: 0.0166\n","Epoch 11/25 | Batch 1231/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 1261/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 1291/3680 | Loss: 0.0161\n","Epoch 11/25 | Batch 1321/3680 | Loss: 0.0125\n","Epoch 11/25 | Batch 1351/3680 | Loss: 0.0975\n","Epoch 11/25 | Batch 1381/3680 | Loss: 0.0021\n","Epoch 11/25 | Batch 1411/3680 | Loss: 0.0806\n","Epoch 11/25 | Batch 1441/3680 | Loss: 0.0950\n","Epoch 11/25 | Batch 1471/3680 | Loss: 0.0318\n","Epoch 11/25 | Batch 1501/3680 | Loss: 0.0015\n","Epoch 11/25 | Batch 1531/3680 | Loss: 0.0116\n","Epoch 11/25 | Batch 1561/3680 | Loss: 0.0783\n","Epoch 11/25 | Batch 1591/3680 | Loss: 0.0705\n","Epoch 11/25 | Batch 1621/3680 | Loss: 0.0008\n","Epoch 11/25 | Batch 1651/3680 | Loss: 0.0557\n","Epoch 11/25 | Batch 1681/3680 | Loss: 0.1080\n","Epoch 11/25 | Batch 1711/3680 | Loss: 0.0510\n","Epoch 11/25 | Batch 1741/3680 | Loss: 0.0004\n","Epoch 11/25 | Batch 1771/3680 | Loss: 0.1151\n","Epoch 11/25 | Batch 1801/3680 | Loss: 0.0568\n","Epoch 11/25 | Batch 1831/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 1861/3680 | Loss: 0.0192\n","Epoch 11/25 | Batch 1891/3680 | Loss: 0.0025\n","Epoch 11/25 | Batch 1921/3680 | Loss: 0.0799\n","Epoch 11/25 | Batch 1951/3680 | Loss: 0.0432\n","Epoch 11/25 | Batch 1981/3680 | Loss: 0.0055\n","Epoch 11/25 | Batch 2011/3680 | Loss: 0.0467\n","Epoch 11/25 | Batch 2041/3680 | Loss: 0.0027\n","Epoch 11/25 | Batch 2071/3680 | Loss: 0.0510\n","Epoch 11/25 | Batch 2101/3680 | Loss: 0.0076\n","Epoch 11/25 | Batch 2131/3680 | Loss: 0.1199\n","Epoch 11/25 | Batch 2161/3680 | Loss: 0.1236\n","Epoch 11/25 | Batch 2191/3680 | Loss: 0.0093\n","Epoch 11/25 | Batch 2221/3680 | Loss: 0.1994\n","Epoch 11/25 | Batch 2251/3680 | Loss: 0.0365\n","Epoch 11/25 | Batch 2281/3680 | Loss: 0.0708\n","Epoch 11/25 | Batch 2311/3680 | Loss: 0.0059\n","Epoch 11/25 | Batch 2341/3680 | Loss: 0.0463\n","Epoch 11/25 | Batch 2371/3680 | Loss: 0.0066\n","Epoch 11/25 | Batch 2401/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 2431/3680 | Loss: 0.0010\n","Epoch 11/25 | Batch 2461/3680 | Loss: 0.1552\n","Epoch 11/25 | Batch 2491/3680 | Loss: 0.0020\n","Epoch 11/25 | Batch 2521/3680 | Loss: 0.1806\n","Epoch 11/25 | Batch 2551/3680 | Loss: 0.0017\n","Epoch 11/25 | Batch 2581/3680 | Loss: 0.0568\n","Epoch 11/25 | Batch 2611/3680 | Loss: 0.0797\n","Epoch 11/25 | Batch 2641/3680 | Loss: 0.0137\n","Epoch 11/25 | Batch 2671/3680 | Loss: 0.0138\n","Epoch 11/25 | Batch 2701/3680 | Loss: 0.1003\n","Epoch 11/25 | Batch 2731/3680 | Loss: 0.1201\n","Epoch 11/25 | Batch 2761/3680 | Loss: 0.0015\n","Epoch 11/25 | Batch 2791/3680 | Loss: 0.2254\n","Epoch 11/25 | Batch 2821/3680 | Loss: 0.0927\n","Epoch 11/25 | Batch 2851/3680 | Loss: 0.0021\n","Epoch 11/25 | Batch 2881/3680 | Loss: 0.1351\n","Epoch 11/25 | Batch 2911/3680 | Loss: 0.1652\n","Epoch 11/25 | Batch 2941/3680 | Loss: 0.0573\n","Epoch 11/25 | Batch 2971/3680 | Loss: 0.0436\n","Epoch 11/25 | Batch 3001/3680 | Loss: 0.0022\n","Epoch 11/25 | Batch 3031/3680 | Loss: 0.0052\n","Epoch 11/25 | Batch 3061/3680 | Loss: 0.0989\n","Epoch 11/25 | Batch 3091/3680 | Loss: 0.1342\n","Epoch 11/25 | Batch 3121/3680 | Loss: 0.0083\n","Epoch 11/25 | Batch 3151/3680 | Loss: 0.0030\n","Epoch 11/25 | Batch 3181/3680 | Loss: 0.0538\n","Epoch 11/25 | Batch 3211/3680 | Loss: 0.0015\n","Epoch 11/25 | Batch 3241/3680 | Loss: 0.2716\n","Epoch 11/25 | Batch 3271/3680 | Loss: 0.0021\n","Epoch 11/25 | Batch 3301/3680 | Loss: 0.0474\n","Epoch 11/25 | Batch 3331/3680 | Loss: 0.0427\n","Epoch 11/25 | Batch 3361/3680 | Loss: 0.0016\n","Epoch 11/25 | Batch 3391/3680 | Loss: 0.0460\n","Epoch 11/25 | Batch 3421/3680 | Loss: 0.0110\n","Epoch 11/25 | Batch 3451/3680 | Loss: 0.1353\n","Epoch 11/25 | Batch 3481/3680 | Loss: 0.0165\n","Epoch 11/25 | Batch 3511/3680 | Loss: 0.0439\n","Epoch 11/25 | Batch 3541/3680 | Loss: 0.0285\n","Epoch 11/25 | Batch 3571/3680 | Loss: 0.0162\n","Epoch 11/25 | Batch 3601/3680 | Loss: 0.0863\n","Epoch 11/25 | Batch 3631/3680 | Loss: 0.2064\n","Epoch 11/25 | Batch 3661/3680 | Loss: 0.0017\n","Average training loss: 0.06\n","Epoch 12/25\n","Epoch 12/25 | Batch 31/3680 | Loss: 0.1240\n","Epoch 12/25 | Batch 61/3680 | Loss: 0.0255\n","Epoch 12/25 | Batch 91/3680 | Loss: 0.1565\n","Epoch 12/25 | Batch 121/3680 | Loss: 0.1271\n","Epoch 12/25 | Batch 151/3680 | Loss: 0.0013\n","Epoch 12/25 | Batch 181/3680 | Loss: 0.0057\n","Epoch 12/25 | Batch 211/3680 | Loss: 0.0143\n","Epoch 12/25 | Batch 241/3680 | Loss: 0.0027\n","Epoch 12/25 | Batch 271/3680 | Loss: 0.0013\n","Epoch 12/25 | Batch 301/3680 | Loss: 0.0068\n","Epoch 12/25 | Batch 331/3680 | Loss: 0.1176\n","Epoch 12/25 | Batch 361/3680 | Loss: 0.0023\n","Epoch 12/25 | Batch 391/3680 | Loss: 0.1611\n","Epoch 12/25 | Batch 421/3680 | Loss: 0.0011\n","Epoch 12/25 | Batch 451/3680 | Loss: 0.0011\n","Epoch 12/25 | Batch 481/3680 | Loss: 0.0838\n","Epoch 12/25 | Batch 511/3680 | Loss: 0.0030\n","Epoch 12/25 | Batch 541/3680 | Loss: 0.0515\n","Epoch 12/25 | Batch 571/3680 | Loss: 0.0064\n","Epoch 12/25 | Batch 601/3680 | Loss: 0.0014\n","Epoch 12/25 | Batch 631/3680 | Loss: 0.1243\n","Epoch 12/25 | Batch 661/3680 | Loss: 0.0402\n","Epoch 12/25 | Batch 691/3680 | Loss: 0.0007\n","Epoch 12/25 | Batch 721/3680 | Loss: 0.0638\n","Epoch 12/25 | Batch 751/3680 | Loss: 0.0079\n","Epoch 12/25 | Batch 781/3680 | Loss: 0.0013\n","Epoch 12/25 | Batch 811/3680 | Loss: 0.0158\n","Epoch 12/25 | Batch 841/3680 | Loss: 0.0017\n","Epoch 12/25 | Batch 871/3680 | Loss: 0.0163\n","Epoch 12/25 | Batch 901/3680 | Loss: 0.0240\n","Epoch 12/25 | Batch 931/3680 | Loss: 0.1322\n","Epoch 12/25 | Batch 961/3680 | Loss: 0.0736\n","Epoch 12/25 | Batch 991/3680 | Loss: 0.0082\n","Epoch 12/25 | Batch 1021/3680 | Loss: 0.0920\n","Epoch 12/25 | Batch 1051/3680 | Loss: 0.0399\n","Epoch 12/25 | Batch 1081/3680 | Loss: 0.0987\n","Epoch 12/25 | Batch 1111/3680 | Loss: 0.0658\n","Epoch 12/25 | Batch 1141/3680 | Loss: 0.0007\n","Epoch 12/25 | Batch 1171/3680 | Loss: 0.1324\n","Epoch 12/25 | Batch 1201/3680 | Loss: 0.0028\n","Epoch 12/25 | Batch 1231/3680 | Loss: 0.0160\n","Epoch 12/25 | Batch 1261/3680 | Loss: 0.0067\n","Epoch 12/25 | Batch 1291/3680 | Loss: 0.0115\n","Epoch 12/25 | Batch 1321/3680 | Loss: 0.0363\n","Epoch 12/25 | Batch 1351/3680 | Loss: 0.0170\n","Epoch 12/25 | Batch 1381/3680 | Loss: 0.1288\n","Epoch 12/25 | Batch 1411/3680 | Loss: 0.0013\n","Epoch 12/25 | Batch 1441/3680 | Loss: 0.0044\n","Epoch 12/25 | Batch 1471/3680 | Loss: 0.0393\n","Epoch 12/25 | Batch 1501/3680 | Loss: 0.0016\n","Epoch 12/25 | Batch 1531/3680 | Loss: 0.1238\n","Epoch 12/25 | Batch 1561/3680 | Loss: 0.0021\n","Epoch 12/25 | Batch 1591/3680 | Loss: 0.1145\n","Epoch 12/25 | Batch 1621/3680 | Loss: 0.0189\n","Epoch 12/25 | Batch 1651/3680 | Loss: 0.0406\n","Epoch 12/25 | Batch 1681/3680 | Loss: 0.0106\n","Epoch 12/25 | Batch 1711/3680 | Loss: 0.0007\n","Epoch 12/25 | Batch 1741/3680 | Loss: 0.0061\n","Epoch 12/25 | Batch 1771/3680 | Loss: 0.0442\n","Epoch 12/25 | Batch 1801/3680 | Loss: 0.0017\n","Epoch 12/25 | Batch 1831/3680 | Loss: 0.1182\n","Epoch 12/25 | Batch 1861/3680 | Loss: 0.0962\n","Epoch 12/25 | Batch 1891/3680 | Loss: 0.0821\n","Epoch 12/25 | Batch 1921/3680 | Loss: 0.0030\n","Epoch 12/25 | Batch 1951/3680 | Loss: 0.0022\n","Epoch 12/25 | Batch 1981/3680 | Loss: 0.0880\n","Epoch 12/25 | Batch 2011/3680 | Loss: 0.1067\n","Epoch 12/25 | Batch 2041/3680 | Loss: 0.0016\n","Epoch 12/25 | Batch 2071/3680 | Loss: 0.0026\n","Epoch 12/25 | Batch 2101/3680 | Loss: 0.0655\n","Epoch 12/25 | Batch 2131/3680 | Loss: 0.0108\n","Epoch 12/25 | Batch 2161/3680 | Loss: 0.0366\n","Epoch 12/25 | Batch 2191/3680 | Loss: 0.3230\n","Epoch 12/25 | Batch 2221/3680 | Loss: 0.0901\n","Epoch 12/25 | Batch 2251/3680 | Loss: 0.0043\n","Epoch 12/25 | Batch 2281/3680 | Loss: 0.1704\n","Epoch 12/25 | Batch 2311/3680 | Loss: 0.0185\n","Epoch 12/25 | Batch 2341/3680 | Loss: 0.0217\n","Epoch 12/25 | Batch 2371/3680 | Loss: 0.2502\n","Epoch 12/25 | Batch 2401/3680 | Loss: 0.0145\n","Epoch 12/25 | Batch 2431/3680 | Loss: 0.1168\n","Epoch 12/25 | Batch 2461/3680 | Loss: 0.0135\n","Epoch 12/25 | Batch 2491/3680 | Loss: 0.0713\n","Epoch 12/25 | Batch 2521/3680 | Loss: 0.0016\n","Epoch 12/25 | Batch 2551/3680 | Loss: 0.0008\n","Epoch 12/25 | Batch 2581/3680 | Loss: 0.0072\n","Epoch 12/25 | Batch 2611/3680 | Loss: 0.0849\n","Epoch 12/25 | Batch 2641/3680 | Loss: 0.0318\n","Epoch 12/25 | Batch 2671/3680 | Loss: 0.0312\n","Epoch 12/25 | Batch 2701/3680 | Loss: 0.0391\n","Epoch 12/25 | Batch 2731/3680 | Loss: 0.0736\n","Epoch 12/25 | Batch 2761/3680 | Loss: 0.0561\n","Epoch 12/25 | Batch 2791/3680 | Loss: 0.0023\n","Epoch 12/25 | Batch 2821/3680 | Loss: 0.0032\n","Epoch 12/25 | Batch 2851/3680 | Loss: 0.0031\n","Epoch 12/25 | Batch 2881/3680 | Loss: 0.0023\n","Epoch 12/25 | Batch 2911/3680 | Loss: 0.1172\n","Epoch 12/25 | Batch 2941/3680 | Loss: 0.0046\n","Epoch 12/25 | Batch 2971/3680 | Loss: 0.0010\n","Epoch 12/25 | Batch 3001/3680 | Loss: 0.2038\n","Epoch 12/25 | Batch 3031/3680 | Loss: 0.0239\n","Epoch 12/25 | Batch 3061/3680 | Loss: 0.0011\n","Epoch 12/25 | Batch 3091/3680 | Loss: 0.0050\n","Epoch 12/25 | Batch 3121/3680 | Loss: 0.0346\n","Epoch 12/25 | Batch 3151/3680 | Loss: 0.0022\n","Epoch 12/25 | Batch 3181/3680 | Loss: 0.0015\n","Epoch 12/25 | Batch 3211/3680 | Loss: 0.3093\n","Epoch 12/25 | Batch 3241/3680 | Loss: 0.0132\n","Epoch 12/25 | Batch 3271/3680 | Loss: 0.0879\n","Epoch 12/25 | Batch 3301/3680 | Loss: 0.1270\n","Epoch 12/25 | Batch 3331/3680 | Loss: 0.0030\n","Epoch 12/25 | Batch 3361/3680 | Loss: 0.0077\n","Epoch 12/25 | Batch 3391/3680 | Loss: 0.0624\n","Epoch 12/25 | Batch 3421/3680 | Loss: 0.0064\n","Epoch 12/25 | Batch 3451/3680 | Loss: 0.1007\n","Epoch 12/25 | Batch 3481/3680 | Loss: 0.0850\n","Epoch 12/25 | Batch 3511/3680 | Loss: 0.1391\n","Epoch 12/25 | Batch 3541/3680 | Loss: 0.1562\n","Epoch 12/25 | Batch 3571/3680 | Loss: 0.0027\n","Epoch 12/25 | Batch 3601/3680 | Loss: 0.0316\n","Epoch 12/25 | Batch 3631/3680 | Loss: 0.3117\n","Epoch 12/25 | Batch 3661/3680 | Loss: 0.1999\n","Average training loss: 0.06\n","Epoch 13/25\n","Epoch 13/25 | Batch 31/3680 | Loss: 0.0297\n","Epoch 13/25 | Batch 61/3680 | Loss: 0.0394\n","Epoch 13/25 | Batch 91/3680 | Loss: 0.3401\n","Epoch 13/25 | Batch 121/3680 | Loss: 0.0012\n","Epoch 13/25 | Batch 151/3680 | Loss: 0.0011\n","Epoch 13/25 | Batch 181/3680 | Loss: 0.0543\n","Epoch 13/25 | Batch 211/3680 | Loss: 0.1388\n","Epoch 13/25 | Batch 241/3680 | Loss: 0.0064\n","Epoch 13/25 | Batch 271/3680 | Loss: 0.0069\n","Epoch 13/25 | Batch 301/3680 | Loss: 0.0023\n","Epoch 13/25 | Batch 331/3680 | Loss: 0.0059\n","Epoch 13/25 | Batch 361/3680 | Loss: 0.0008\n","Epoch 13/25 | Batch 391/3680 | Loss: 0.0241\n","Epoch 13/25 | Batch 421/3680 | Loss: 0.0095\n","Epoch 13/25 | Batch 451/3680 | Loss: 0.0019\n","Epoch 13/25 | Batch 481/3680 | Loss: 0.0050\n","Epoch 13/25 | Batch 511/3680 | Loss: 0.0326\n","Epoch 13/25 | Batch 541/3680 | Loss: 0.1009\n","Epoch 13/25 | Batch 571/3680 | Loss: 0.0685\n","Epoch 13/25 | Batch 601/3680 | Loss: 0.0885\n","Epoch 13/25 | Batch 631/3680 | Loss: 0.0768\n","Epoch 13/25 | Batch 661/3680 | Loss: 0.0759\n","Epoch 13/25 | Batch 691/3680 | Loss: 0.0042\n","Epoch 13/25 | Batch 721/3680 | Loss: 0.0153\n","Epoch 13/25 | Batch 751/3680 | Loss: 0.0030\n","Epoch 13/25 | Batch 781/3680 | Loss: 0.0420\n","Epoch 13/25 | Batch 811/3680 | Loss: 0.0015\n","Epoch 13/25 | Batch 841/3680 | Loss: 0.5898\n","Epoch 13/25 | Batch 871/3680 | Loss: 0.0017\n","Epoch 13/25 | Batch 901/3680 | Loss: 0.1168\n","Epoch 13/25 | Batch 931/3680 | Loss: 0.0030\n","Epoch 13/25 | Batch 961/3680 | Loss: 0.0008\n","Epoch 13/25 | Batch 991/3680 | Loss: 0.2961\n","Epoch 13/25 | Batch 1021/3680 | Loss: 0.3520\n","Epoch 13/25 | Batch 1051/3680 | Loss: 0.0017\n","Epoch 13/25 | Batch 1081/3680 | Loss: 0.1374\n","Epoch 13/25 | Batch 1111/3680 | Loss: 0.0124\n","Epoch 13/25 | Batch 1141/3680 | Loss: 0.2155\n","Epoch 13/25 | Batch 1171/3680 | Loss: 0.0172\n","Epoch 13/25 | Batch 1201/3680 | Loss: 0.0014\n","Epoch 13/25 | Batch 1231/3680 | Loss: 0.0403\n","Epoch 13/25 | Batch 1261/3680 | Loss: 0.0607\n","Epoch 13/25 | Batch 1291/3680 | Loss: 0.0066\n","Epoch 13/25 | Batch 1321/3680 | Loss: 0.1787\n","Epoch 13/25 | Batch 1351/3680 | Loss: 0.0389\n","Epoch 13/25 | Batch 1381/3680 | Loss: 0.0552\n","Epoch 13/25 | Batch 1411/3680 | Loss: 0.0541\n","Epoch 13/25 | Batch 1441/3680 | Loss: 0.1122\n","Epoch 13/25 | Batch 1471/3680 | Loss: 0.0587\n","Epoch 13/25 | Batch 1501/3680 | Loss: 0.0906\n","Epoch 13/25 | Batch 1531/3680 | Loss: 0.0138\n","Epoch 13/25 | Batch 1561/3680 | Loss: 0.2581\n","Epoch 13/25 | Batch 1591/3680 | Loss: 0.0609\n","Epoch 13/25 | Batch 1621/3680 | Loss: 0.1746\n","Epoch 13/25 | Batch 1651/3680 | Loss: 0.0067\n","Epoch 13/25 | Batch 1681/3680 | Loss: 0.0099\n","Epoch 13/25 | Batch 1711/3680 | Loss: 0.0025\n","Epoch 13/25 | Batch 1741/3680 | Loss: 0.0469\n","Epoch 13/25 | Batch 1771/3680 | Loss: 0.0237\n","Epoch 13/25 | Batch 1801/3680 | Loss: 0.1011\n","Epoch 13/25 | Batch 1831/3680 | Loss: 0.0815\n","Epoch 13/25 | Batch 1861/3680 | Loss: 0.0050\n","Epoch 13/25 | Batch 1891/3680 | Loss: 0.0014\n","Epoch 13/25 | Batch 1921/3680 | Loss: 0.0309\n","Epoch 13/25 | Batch 1951/3680 | Loss: 0.0278\n","Epoch 13/25 | Batch 1981/3680 | Loss: 0.0051\n","Epoch 13/25 | Batch 2011/3680 | Loss: 0.1144\n","Epoch 13/25 | Batch 2041/3680 | Loss: 0.4455\n","Epoch 13/25 | Batch 2071/3680 | Loss: 0.2801\n","Epoch 13/25 | Batch 2101/3680 | Loss: 0.1417\n","Epoch 13/25 | Batch 2131/3680 | Loss: 0.0056\n","Epoch 13/25 | Batch 2161/3680 | Loss: 0.0029\n","Epoch 13/25 | Batch 2191/3680 | Loss: 0.1330\n","Epoch 13/25 | Batch 2221/3680 | Loss: 0.0428\n","Epoch 13/25 | Batch 2251/3680 | Loss: 0.0110\n","Epoch 13/25 | Batch 2281/3680 | Loss: 0.2015\n","Epoch 13/25 | Batch 2311/3680 | Loss: 0.0007\n","Epoch 13/25 | Batch 2341/3680 | Loss: 0.0082\n","Epoch 13/25 | Batch 2371/3680 | Loss: 0.0033\n","Epoch 13/25 | Batch 2401/3680 | Loss: 0.0024\n","Epoch 13/25 | Batch 2431/3680 | Loss: 0.0694\n","Epoch 13/25 | Batch 2461/3680 | Loss: 0.0018\n","Epoch 13/25 | Batch 2491/3680 | Loss: 0.3776\n","Epoch 13/25 | Batch 2521/3680 | Loss: 0.0054\n","Epoch 13/25 | Batch 2551/3680 | Loss: 0.0780\n","Epoch 13/25 | Batch 2581/3680 | Loss: 0.0337\n","Epoch 13/25 | Batch 2611/3680 | Loss: 0.0047\n","Epoch 13/25 | Batch 2641/3680 | Loss: 0.0191\n","Epoch 13/25 | Batch 2671/3680 | Loss: 0.0830\n","Epoch 13/25 | Batch 2701/3680 | Loss: 0.0139\n","Epoch 13/25 | Batch 2731/3680 | Loss: 0.2849\n","Epoch 13/25 | Batch 2761/3680 | Loss: 0.0007\n","Epoch 13/25 | Batch 2791/3680 | Loss: 0.0027\n","Epoch 13/25 | Batch 2821/3680 | Loss: 0.0047\n","Epoch 13/25 | Batch 2851/3680 | Loss: 0.1151\n","Epoch 13/25 | Batch 2881/3680 | Loss: 0.1178\n","Epoch 13/25 | Batch 2911/3680 | Loss: 0.0213\n","Epoch 13/25 | Batch 2941/3680 | Loss: 0.1603\n","Epoch 13/25 | Batch 2971/3680 | Loss: 0.0038\n","Epoch 13/25 | Batch 3001/3680 | Loss: 0.0006\n","Epoch 13/25 | Batch 3031/3680 | Loss: 0.0751\n","Epoch 13/25 | Batch 3061/3680 | Loss: 0.0008\n","Epoch 13/25 | Batch 3091/3680 | Loss: 0.0097\n","Epoch 13/25 | Batch 3121/3680 | Loss: 0.1238\n","Epoch 13/25 | Batch 3151/3680 | Loss: 0.0417\n","Epoch 13/25 | Batch 3181/3680 | Loss: 0.1017\n","Epoch 13/25 | Batch 3211/3680 | Loss: 0.0048\n","Epoch 13/25 | Batch 3241/3680 | Loss: 0.1132\n","Epoch 13/25 | Batch 3271/3680 | Loss: 0.0039\n","Epoch 13/25 | Batch 3301/3680 | Loss: 0.0014\n","Epoch 13/25 | Batch 3331/3680 | Loss: 0.0012\n","Epoch 13/25 | Batch 3361/3680 | Loss: 0.0255\n","Epoch 13/25 | Batch 3391/3680 | Loss: 0.0376\n","Epoch 13/25 | Batch 3421/3680 | Loss: 0.0385\n","Epoch 13/25 | Batch 3451/3680 | Loss: 0.1402\n","Epoch 13/25 | Batch 3481/3680 | Loss: 0.1153\n","Epoch 13/25 | Batch 3511/3680 | Loss: 0.0017\n","Epoch 13/25 | Batch 3541/3680 | Loss: 0.0306\n","Epoch 13/25 | Batch 3571/3680 | Loss: 0.0005\n","Epoch 13/25 | Batch 3601/3680 | Loss: 0.0183\n","Epoch 13/25 | Batch 3631/3680 | Loss: 0.0012\n","Epoch 13/25 | Batch 3661/3680 | Loss: 0.0561\n","Average training loss: 0.05\n","Epoch 14/25\n","Epoch 14/25 | Batch 31/3680 | Loss: 0.0341\n","Epoch 14/25 | Batch 61/3680 | Loss: 0.0036\n","Epoch 14/25 | Batch 91/3680 | Loss: 0.0604\n","Epoch 14/25 | Batch 121/3680 | Loss: 0.1153\n","Epoch 14/25 | Batch 151/3680 | Loss: 0.0032\n","Epoch 14/25 | Batch 181/3680 | Loss: 0.0335\n","Epoch 14/25 | Batch 211/3680 | Loss: 0.0013\n","Epoch 14/25 | Batch 241/3680 | Loss: 0.2392\n","Epoch 14/25 | Batch 271/3680 | Loss: 0.0975\n","Epoch 14/25 | Batch 301/3680 | Loss: 0.0545\n","Epoch 14/25 | Batch 331/3680 | Loss: 0.0974\n","Epoch 14/25 | Batch 361/3680 | Loss: 0.0200\n","Epoch 14/25 | Batch 391/3680 | Loss: 0.0101\n","Epoch 14/25 | Batch 421/3680 | Loss: 0.0558\n","Epoch 14/25 | Batch 451/3680 | Loss: 0.1328\n","Epoch 14/25 | Batch 481/3680 | Loss: 0.2261\n","Epoch 14/25 | Batch 511/3680 | Loss: 0.3372\n","Epoch 14/25 | Batch 541/3680 | Loss: 0.0039\n","Epoch 14/25 | Batch 571/3680 | Loss: 0.1231\n","Epoch 14/25 | Batch 601/3680 | Loss: 0.0361\n","Epoch 14/25 | Batch 631/3680 | Loss: 0.1084\n","Epoch 14/25 | Batch 661/3680 | Loss: 0.0069\n","Epoch 14/25 | Batch 691/3680 | Loss: 0.0015\n","Epoch 14/25 | Batch 721/3680 | Loss: 0.0427\n","Epoch 14/25 | Batch 751/3680 | Loss: 0.0036\n","Epoch 14/25 | Batch 781/3680 | Loss: 0.0422\n","Epoch 14/25 | Batch 811/3680 | Loss: 0.1093\n","Epoch 14/25 | Batch 841/3680 | Loss: 0.2268\n","Epoch 14/25 | Batch 871/3680 | Loss: 0.0677\n","Epoch 14/25 | Batch 901/3680 | Loss: 0.0019\n","Epoch 14/25 | Batch 931/3680 | Loss: 0.0162\n","Epoch 14/25 | Batch 961/3680 | Loss: 0.1380\n","Epoch 14/25 | Batch 991/3680 | Loss: 0.0477\n","Epoch 14/25 | Batch 1021/3680 | Loss: 0.0289\n","Epoch 14/25 | Batch 1051/3680 | Loss: 0.1325\n","Epoch 14/25 | Batch 1081/3680 | Loss: 0.1911\n","Epoch 14/25 | Batch 1111/3680 | Loss: 0.0162\n","Epoch 14/25 | Batch 1141/3680 | Loss: 0.0457\n","Epoch 14/25 | Batch 1171/3680 | Loss: 0.0386\n","Epoch 14/25 | Batch 1201/3680 | Loss: 0.0134\n","Epoch 14/25 | Batch 1231/3680 | Loss: 0.0070\n","Epoch 14/25 | Batch 1261/3680 | Loss: 0.1256\n","Epoch 14/25 | Batch 1291/3680 | Loss: 0.0029\n","Epoch 14/25 | Batch 1321/3680 | Loss: 0.0018\n","Epoch 14/25 | Batch 1351/3680 | Loss: 0.0017\n","Epoch 14/25 | Batch 1381/3680 | Loss: 0.0017\n","Epoch 14/25 | Batch 1411/3680 | Loss: 0.2634\n","Epoch 14/25 | Batch 1441/3680 | Loss: 0.1149\n","Epoch 14/25 | Batch 1471/3680 | Loss: 0.0122\n","Epoch 14/25 | Batch 1501/3680 | Loss: 0.2004\n","Epoch 14/25 | Batch 1531/3680 | Loss: 0.0040\n","Epoch 14/25 | Batch 1561/3680 | Loss: 0.1107\n","Epoch 14/25 | Batch 1591/3680 | Loss: 0.0227\n","Epoch 14/25 | Batch 1621/3680 | Loss: 0.0246\n","Epoch 14/25 | Batch 1651/3680 | Loss: 0.0148\n","Epoch 14/25 | Batch 1681/3680 | Loss: 0.0410\n","Epoch 14/25 | Batch 1711/3680 | Loss: 0.0772\n","Epoch 14/25 | Batch 1741/3680 | Loss: 0.0065\n","Epoch 14/25 | Batch 1771/3680 | Loss: 0.0483\n","Epoch 14/25 | Batch 1801/3680 | Loss: 0.2618\n","Epoch 14/25 | Batch 1831/3680 | Loss: 0.0118\n","Epoch 14/25 | Batch 1861/3680 | Loss: 0.2358\n","Epoch 14/25 | Batch 1891/3680 | Loss: 0.2786\n","Epoch 14/25 | Batch 1921/3680 | Loss: 0.1978\n","Epoch 14/25 | Batch 1951/3680 | Loss: 0.1012\n","Epoch 14/25 | Batch 1981/3680 | Loss: 0.0554\n","Epoch 14/25 | Batch 2011/3680 | Loss: 0.0018\n","Epoch 14/25 | Batch 2041/3680 | Loss: 0.1771\n","Epoch 14/25 | Batch 2071/3680 | Loss: 0.0010\n","Epoch 14/25 | Batch 2101/3680 | Loss: 0.0033\n","Epoch 14/25 | Batch 2131/3680 | Loss: 0.0034\n","Epoch 14/25 | Batch 2161/3680 | Loss: 0.0031\n","Epoch 14/25 | Batch 2191/3680 | Loss: 0.0506\n","Epoch 14/25 | Batch 2221/3680 | Loss: 0.0487\n","Epoch 14/25 | Batch 2251/3680 | Loss: 0.0477\n","Epoch 14/25 | Batch 2281/3680 | Loss: 0.0629\n","Epoch 14/25 | Batch 2311/3680 | Loss: 0.0854\n","Epoch 14/25 | Batch 2341/3680 | Loss: 0.0049\n","Epoch 14/25 | Batch 2371/3680 | Loss: 0.0063\n","Epoch 14/25 | Batch 2401/3680 | Loss: 0.0949\n","Epoch 14/25 | Batch 2431/3680 | Loss: 0.0047\n","Epoch 14/25 | Batch 2461/3680 | Loss: 0.0143\n","Epoch 14/25 | Batch 2491/3680 | Loss: 0.0816\n","Epoch 14/25 | Batch 2521/3680 | Loss: 0.0027\n","Epoch 14/25 | Batch 2551/3680 | Loss: 0.1288\n","Epoch 14/25 | Batch 2581/3680 | Loss: 0.0048\n","Epoch 14/25 | Batch 2611/3680 | Loss: 0.2844\n","Epoch 14/25 | Batch 2641/3680 | Loss: 0.0771\n","Epoch 14/25 | Batch 2671/3680 | Loss: 0.0124\n","Epoch 14/25 | Batch 2701/3680 | Loss: 0.0013\n","Epoch 14/25 | Batch 2731/3680 | Loss: 0.0930\n","Epoch 14/25 | Batch 2761/3680 | Loss: 0.0067\n","Epoch 14/25 | Batch 2791/3680 | Loss: 0.0020\n","Epoch 14/25 | Batch 2821/3680 | Loss: 0.0472\n","Epoch 14/25 | Batch 2851/3680 | Loss: 0.0304\n","Epoch 14/25 | Batch 2881/3680 | Loss: 0.0021\n","Epoch 14/25 | Batch 2911/3680 | Loss: 0.0422\n","Epoch 14/25 | Batch 2941/3680 | Loss: 0.0069\n","Epoch 14/25 | Batch 2971/3680 | Loss: 0.0828\n","Epoch 14/25 | Batch 3001/3680 | Loss: 0.0029\n","Epoch 14/25 | Batch 3031/3680 | Loss: 0.0060\n","Epoch 14/25 | Batch 3061/3680 | Loss: 0.0406\n","Epoch 14/25 | Batch 3091/3680 | Loss: 0.0015\n","Epoch 14/25 | Batch 3121/3680 | Loss: 0.0010\n","Epoch 14/25 | Batch 3151/3680 | Loss: 0.0896\n","Epoch 14/25 | Batch 3181/3680 | Loss: 0.2237\n","Epoch 14/25 | Batch 3211/3680 | Loss: 0.1314\n","Epoch 14/25 | Batch 3241/3680 | Loss: 0.0413\n","Epoch 14/25 | Batch 3271/3680 | Loss: 0.0235\n","Epoch 14/25 | Batch 3301/3680 | Loss: 0.0709\n","Epoch 14/25 | Batch 3331/3680 | Loss: 0.0020\n","Epoch 14/25 | Batch 3361/3680 | Loss: 0.0025\n","Epoch 14/25 | Batch 3391/3680 | Loss: 0.1215\n","Epoch 14/25 | Batch 3421/3680 | Loss: 0.2178\n","Epoch 14/25 | Batch 3451/3680 | Loss: 0.0020\n","Epoch 14/25 | Batch 3481/3680 | Loss: 0.0401\n","Epoch 14/25 | Batch 3511/3680 | Loss: 0.1069\n","Epoch 14/25 | Batch 3541/3680 | Loss: 0.1165\n","Epoch 14/25 | Batch 3571/3680 | Loss: 0.0013\n","Epoch 14/25 | Batch 3601/3680 | Loss: 0.0011\n","Epoch 14/25 | Batch 3631/3680 | Loss: 0.0003\n","Epoch 14/25 | Batch 3661/3680 | Loss: 0.0014\n","Average training loss: 0.05\n","Epoch 15/25\n","Epoch 15/25 | Batch 31/3680 | Loss: 0.0013\n","Epoch 15/25 | Batch 61/3680 | Loss: 0.0037\n","Epoch 15/25 | Batch 91/3680 | Loss: 0.0643\n","Epoch 15/25 | Batch 121/3680 | Loss: 0.0007\n","Epoch 15/25 | Batch 151/3680 | Loss: 0.1267\n","Epoch 15/25 | Batch 181/3680 | Loss: 0.0874\n","Epoch 15/25 | Batch 211/3680 | Loss: 0.0129\n","Epoch 15/25 | Batch 241/3680 | Loss: 0.1211\n","Epoch 15/25 | Batch 271/3680 | Loss: 0.0318\n","Epoch 15/25 | Batch 301/3680 | Loss: 0.0011\n","Epoch 15/25 | Batch 331/3680 | Loss: 0.0041\n","Epoch 15/25 | Batch 361/3680 | Loss: 0.0204\n","Epoch 15/25 | Batch 391/3680 | Loss: 0.0482\n","Epoch 15/25 | Batch 421/3680 | Loss: 0.1026\n","Epoch 15/25 | Batch 451/3680 | Loss: 0.0018\n","Epoch 15/25 | Batch 481/3680 | Loss: 0.0025\n","Epoch 15/25 | Batch 511/3680 | Loss: 0.1116\n","Epoch 15/25 | Batch 541/3680 | Loss: 0.0029\n","Epoch 15/25 | Batch 571/3680 | Loss: 0.1304\n","Epoch 15/25 | Batch 601/3680 | Loss: 0.1029\n","Epoch 15/25 | Batch 631/3680 | Loss: 0.2695\n","Epoch 15/25 | Batch 661/3680 | Loss: 0.0008\n","Epoch 15/25 | Batch 691/3680 | Loss: 0.0019\n","Epoch 15/25 | Batch 721/3680 | Loss: 0.0049\n","Epoch 15/25 | Batch 751/3680 | Loss: 0.0752\n","Epoch 15/25 | Batch 781/3680 | Loss: 0.1734\n","Epoch 15/25 | Batch 811/3680 | Loss: 0.0090\n","Epoch 15/25 | Batch 841/3680 | Loss: 0.0566\n","Epoch 15/25 | Batch 871/3680 | Loss: 0.0012\n","Epoch 15/25 | Batch 901/3680 | Loss: 0.1522\n","Epoch 15/25 | Batch 931/3680 | Loss: 0.1808\n","Epoch 15/25 | Batch 961/3680 | Loss: 0.0042\n","Epoch 15/25 | Batch 991/3680 | Loss: 0.0016\n","Epoch 15/25 | Batch 1021/3680 | Loss: 0.0438\n","Epoch 15/25 | Batch 1051/3680 | Loss: 0.0021\n","Epoch 15/25 | Batch 1081/3680 | Loss: 0.0745\n","Epoch 15/25 | Batch 1111/3680 | Loss: 0.0161\n","Epoch 15/25 | Batch 1141/3680 | Loss: 0.0015\n","Epoch 15/25 | Batch 1171/3680 | Loss: 0.0426\n","Epoch 15/25 | Batch 1201/3680 | Loss: 0.0844\n","Epoch 15/25 | Batch 1231/3680 | Loss: 0.2014\n","Epoch 15/25 | Batch 1261/3680 | Loss: 0.0016\n","Epoch 15/25 | Batch 1291/3680 | Loss: 0.0726\n","Epoch 15/25 | Batch 1321/3680 | Loss: 0.0384\n","Epoch 15/25 | Batch 1351/3680 | Loss: 0.0011\n","Epoch 15/25 | Batch 1381/3680 | Loss: 0.2342\n","Epoch 15/25 | Batch 1411/3680 | Loss: 0.0043\n","Epoch 15/25 | Batch 1441/3680 | Loss: 0.1185\n","Epoch 15/25 | Batch 1471/3680 | Loss: 0.2275\n","Epoch 15/25 | Batch 1501/3680 | Loss: 0.0025\n","Epoch 15/25 | Batch 1531/3680 | Loss: 0.0013\n","Epoch 15/25 | Batch 1561/3680 | Loss: 0.0529\n","Epoch 15/25 | Batch 1591/3680 | Loss: 0.0009\n","Epoch 15/25 | Batch 1621/3680 | Loss: 0.0670\n","Epoch 15/25 | Batch 1651/3680 | Loss: 0.1221\n","Epoch 15/25 | Batch 1681/3680 | Loss: 0.0317\n","Epoch 15/25 | Batch 1711/3680 | Loss: 0.0099\n","Epoch 15/25 | Batch 1741/3680 | Loss: 0.0181\n","Epoch 15/25 | Batch 1771/3680 | Loss: 0.0720\n","Epoch 15/25 | Batch 1801/3680 | Loss: 0.1035\n","Epoch 15/25 | Batch 1831/3680 | Loss: 0.0709\n","Epoch 15/25 | Batch 1861/3680 | Loss: 0.0009\n","Epoch 15/25 | Batch 1891/3680 | Loss: 0.0090\n","Epoch 15/25 | Batch 1921/3680 | Loss: 0.0319\n","Epoch 15/25 | Batch 1951/3680 | Loss: 0.1942\n","Epoch 15/25 | Batch 1981/3680 | Loss: 0.0040\n","Epoch 15/25 | Batch 2011/3680 | Loss: 0.0782\n","Epoch 15/25 | Batch 2041/3680 | Loss: 0.0427\n","Epoch 15/25 | Batch 2071/3680 | Loss: 0.0040\n","Epoch 15/25 | Batch 2101/3680 | Loss: 0.0012\n","Epoch 15/25 | Batch 2131/3680 | Loss: 0.0014\n","Epoch 15/25 | Batch 2161/3680 | Loss: 0.0237\n","Epoch 15/25 | Batch 2191/3680 | Loss: 0.1463\n","Epoch 15/25 | Batch 2221/3680 | Loss: 0.0344\n","Epoch 15/25 | Batch 2251/3680 | Loss: 0.0012\n","Epoch 15/25 | Batch 2281/3680 | Loss: 0.1452\n","Epoch 15/25 | Batch 2311/3680 | Loss: 0.0013\n","Epoch 15/25 | Batch 2341/3680 | Loss: 0.0175\n","Epoch 15/25 | Batch 2371/3680 | Loss: 0.0575\n","Epoch 15/25 | Batch 2401/3680 | Loss: 0.1839\n","Epoch 15/25 | Batch 2431/3680 | Loss: 0.0012\n","Epoch 15/25 | Batch 2461/3680 | Loss: 0.1129\n","Epoch 15/25 | Batch 2491/3680 | Loss: 0.0014\n","Epoch 15/25 | Batch 2521/3680 | Loss: 0.0294\n","Epoch 15/25 | Batch 2551/3680 | Loss: 0.0604\n","Epoch 15/25 | Batch 2581/3680 | Loss: 0.0474\n","Epoch 15/25 | Batch 2611/3680 | Loss: 0.0478\n","Epoch 15/25 | Batch 2641/3680 | Loss: 0.0792\n","Epoch 15/25 | Batch 2671/3680 | Loss: 0.0006\n","Epoch 15/25 | Batch 2701/3680 | Loss: 0.0191\n","Epoch 15/25 | Batch 2731/3680 | Loss: 0.0302\n","Epoch 15/25 | Batch 2761/3680 | Loss: 0.0939\n","Epoch 15/25 | Batch 2791/3680 | Loss: 0.0026\n","Epoch 15/25 | Batch 2821/3680 | Loss: 0.0414\n","Epoch 15/25 | Batch 2851/3680 | Loss: 0.0076\n","Epoch 15/25 | Batch 2881/3680 | Loss: 0.0453\n","Epoch 15/25 | Batch 2911/3680 | Loss: 0.0412\n","Epoch 15/25 | Batch 2941/3680 | Loss: 0.0293\n","Epoch 15/25 | Batch 2971/3680 | Loss: 0.0875\n","Epoch 15/25 | Batch 3001/3680 | Loss: 0.1519\n","Epoch 15/25 | Batch 3031/3680 | Loss: 0.1152\n","Epoch 15/25 | Batch 3061/3680 | Loss: 0.0019\n","Epoch 15/25 | Batch 3091/3680 | Loss: 0.2208\n","Epoch 15/25 | Batch 3121/3680 | Loss: 0.1655\n","Epoch 15/25 | Batch 3151/3680 | Loss: 0.0026\n","Epoch 15/25 | Batch 3181/3680 | Loss: 0.0103\n","Epoch 15/25 | Batch 3211/3680 | Loss: 0.1068\n","Epoch 15/25 | Batch 3241/3680 | Loss: 0.0738\n","Epoch 15/25 | Batch 3271/3680 | Loss: 0.0035\n","Epoch 15/25 | Batch 3301/3680 | Loss: 0.2273\n","Epoch 15/25 | Batch 3331/3680 | Loss: 0.0223\n","Epoch 15/25 | Batch 3361/3680 | Loss: 0.0045\n","Epoch 15/25 | Batch 3391/3680 | Loss: 0.0010\n","Epoch 15/25 | Batch 3421/3680 | Loss: 0.0175\n","Epoch 15/25 | Batch 3451/3680 | Loss: 0.2293\n","Epoch 15/25 | Batch 3481/3680 | Loss: 0.0010\n","Epoch 15/25 | Batch 3511/3680 | Loss: 0.0580\n","Epoch 15/25 | Batch 3541/3680 | Loss: 0.2925\n","Epoch 15/25 | Batch 3571/3680 | Loss: 0.0069\n","Epoch 15/25 | Batch 3601/3680 | Loss: 0.0263\n","Epoch 15/25 | Batch 3631/3680 | Loss: 0.0005\n","Epoch 15/25 | Batch 3661/3680 | Loss: 0.2228\n","Average training loss: 0.06\n","Epoch 16/25\n","Epoch 16/25 | Batch 31/3680 | Loss: 0.0140\n","Epoch 16/25 | Batch 61/3680 | Loss: 0.1001\n","Epoch 16/25 | Batch 91/3680 | Loss: 0.0024\n","Epoch 16/25 | Batch 121/3680 | Loss: 0.0061\n","Epoch 16/25 | Batch 151/3680 | Loss: 0.0022\n","Epoch 16/25 | Batch 181/3680 | Loss: 0.0038\n","Epoch 16/25 | Batch 211/3680 | Loss: 0.0360\n","Epoch 16/25 | Batch 241/3680 | Loss: 0.1824\n","Epoch 16/25 | Batch 271/3680 | Loss: 0.0025\n","Epoch 16/25 | Batch 301/3680 | Loss: 0.0055\n","Epoch 16/25 | Batch 331/3680 | Loss: 0.0009\n","Epoch 16/25 | Batch 361/3680 | Loss: 0.0049\n","Epoch 16/25 | Batch 391/3680 | Loss: 0.0620\n","Epoch 16/25 | Batch 421/3680 | Loss: 0.1425\n","Epoch 16/25 | Batch 451/3680 | Loss: 0.2571\n","Epoch 16/25 | Batch 481/3680 | Loss: 0.0673\n","Epoch 16/25 | Batch 511/3680 | Loss: 0.0030\n","Epoch 16/25 | Batch 541/3680 | Loss: 0.0656\n","Epoch 16/25 | Batch 571/3680 | Loss: 0.0294\n","Epoch 16/25 | Batch 601/3680 | Loss: 0.0294\n","Epoch 16/25 | Batch 631/3680 | Loss: 0.0170\n","Epoch 16/25 | Batch 661/3680 | Loss: 0.0525\n","Epoch 16/25 | Batch 691/3680 | Loss: 0.0175\n","Epoch 16/25 | Batch 721/3680 | Loss: 0.0093\n","Epoch 16/25 | Batch 751/3680 | Loss: 0.2971\n","Epoch 16/25 | Batch 781/3680 | Loss: 0.0032\n","Epoch 16/25 | Batch 811/3680 | Loss: 0.0015\n","Epoch 16/25 | Batch 841/3680 | Loss: 0.0079\n","Epoch 16/25 | Batch 871/3680 | Loss: 0.0919\n","Epoch 16/25 | Batch 901/3680 | Loss: 0.1826\n","Epoch 16/25 | Batch 931/3680 | Loss: 0.0040\n","Epoch 16/25 | Batch 961/3680 | Loss: 0.0067\n","Epoch 16/25 | Batch 991/3680 | Loss: 0.0496\n","Epoch 16/25 | Batch 1021/3680 | Loss: 0.0930\n","Epoch 16/25 | Batch 1051/3680 | Loss: 0.2763\n","Epoch 16/25 | Batch 1081/3680 | Loss: 0.0387\n","Epoch 16/25 | Batch 1111/3680 | Loss: 0.0008\n","Epoch 16/25 | Batch 1141/3680 | Loss: 0.0488\n","Epoch 16/25 | Batch 1171/3680 | Loss: 0.0012\n","Epoch 16/25 | Batch 1201/3680 | Loss: 0.0093\n","Epoch 16/25 | Batch 1231/3680 | Loss: 0.1354\n","Epoch 16/25 | Batch 1261/3680 | Loss: 0.0197\n","Epoch 16/25 | Batch 1291/3680 | Loss: 0.0096\n","Epoch 16/25 | Batch 1321/3680 | Loss: 0.0461\n","Epoch 16/25 | Batch 1351/3680 | Loss: 0.0308\n","Epoch 16/25 | Batch 1381/3680 | Loss: 0.0999\n","Epoch 16/25 | Batch 1411/3680 | Loss: 0.0129\n","Epoch 16/25 | Batch 1441/3680 | Loss: 0.0014\n","Epoch 16/25 | Batch 1471/3680 | Loss: 0.0008\n","Epoch 16/25 | Batch 1501/3680 | Loss: 0.0400\n","Epoch 16/25 | Batch 1531/3680 | Loss: 0.0908\n","Epoch 16/25 | Batch 1561/3680 | Loss: 0.0252\n","Epoch 16/25 | Batch 1591/3680 | Loss: 0.1366\n","Epoch 16/25 | Batch 1621/3680 | Loss: 0.1238\n","Epoch 16/25 | Batch 1651/3680 | Loss: 0.0430\n","Epoch 16/25 | Batch 1681/3680 | Loss: 0.0279\n","Epoch 16/25 | Batch 1711/3680 | Loss: 0.1328\n","Epoch 16/25 | Batch 1741/3680 | Loss: 0.0021\n","Epoch 16/25 | Batch 1771/3680 | Loss: 0.0555\n","Epoch 16/25 | Batch 1801/3680 | Loss: 0.0020\n","Epoch 16/25 | Batch 1831/3680 | Loss: 0.0301\n","Epoch 16/25 | Batch 1861/3680 | Loss: 0.2579\n","Epoch 16/25 | Batch 1891/3680 | Loss: 0.0059\n","Epoch 16/25 | Batch 1921/3680 | Loss: 0.1126\n","Epoch 16/25 | Batch 1951/3680 | Loss: 0.0906\n","Epoch 16/25 | Batch 1981/3680 | Loss: 0.0684\n","Epoch 16/25 | Batch 2011/3680 | Loss: 0.0407\n","Epoch 16/25 | Batch 2041/3680 | Loss: 0.1428\n","Epoch 16/25 | Batch 2071/3680 | Loss: 0.1419\n","Epoch 16/25 | Batch 2101/3680 | Loss: 0.1685\n","Epoch 16/25 | Batch 2131/3680 | Loss: 0.0062\n","Epoch 16/25 | Batch 2161/3680 | Loss: 0.0313\n","Epoch 16/25 | Batch 2191/3680 | Loss: 0.0462\n","Epoch 16/25 | Batch 2221/3680 | Loss: 0.0526\n","Epoch 16/25 | Batch 2251/3680 | Loss: 0.1737\n","Epoch 16/25 | Batch 2281/3680 | Loss: 0.1109\n","Epoch 16/25 | Batch 2311/3680 | Loss: 0.0012\n","Epoch 16/25 | Batch 2341/3680 | Loss: 0.0139\n","Epoch 16/25 | Batch 2371/3680 | Loss: 0.1040\n","Epoch 16/25 | Batch 2401/3680 | Loss: 0.0234\n","Epoch 16/25 | Batch 2431/3680 | Loss: 0.0178\n","Epoch 16/25 | Batch 2461/3680 | Loss: 0.0127\n","Epoch 16/25 | Batch 2491/3680 | Loss: 0.1006\n","Epoch 16/25 | Batch 2521/3680 | Loss: 0.0005\n","Epoch 16/25 | Batch 2551/3680 | Loss: 0.0118\n","Epoch 16/25 | Batch 2581/3680 | Loss: 0.0616\n","Epoch 16/25 | Batch 2611/3680 | Loss: 0.0042\n","Epoch 16/25 | Batch 2641/3680 | Loss: 0.0206\n","Epoch 16/25 | Batch 2671/3680 | Loss: 0.0009\n","Epoch 16/25 | Batch 2701/3680 | Loss: 0.0032\n","Epoch 16/25 | Batch 2731/3680 | Loss: 0.0348\n","Epoch 16/25 | Batch 2761/3680 | Loss: 0.0005\n","Epoch 16/25 | Batch 2791/3680 | Loss: 0.0011\n","Epoch 16/25 | Batch 2821/3680 | Loss: 0.0043\n","Epoch 16/25 | Batch 2851/3680 | Loss: 0.2623\n","Epoch 16/25 | Batch 2881/3680 | Loss: 0.0004\n","Epoch 16/25 | Batch 2911/3680 | Loss: 0.1495\n","Epoch 16/25 | Batch 2941/3680 | Loss: 0.0199\n","Epoch 16/25 | Batch 2971/3680 | Loss: 0.0165\n","Epoch 16/25 | Batch 3001/3680 | Loss: 0.0322\n","Epoch 16/25 | Batch 3031/3680 | Loss: 0.0682\n","Epoch 16/25 | Batch 3061/3680 | Loss: 0.0027\n","Epoch 16/25 | Batch 3091/3680 | Loss: 0.0479\n","Epoch 16/25 | Batch 3121/3680 | Loss: 0.0151\n","Epoch 16/25 | Batch 3151/3680 | Loss: 0.0220\n","Epoch 16/25 | Batch 3181/3680 | Loss: 0.0303\n","Epoch 16/25 | Batch 3211/3680 | Loss: 0.0046\n","Epoch 16/25 | Batch 3241/3680 | Loss: 0.0321\n","Epoch 16/25 | Batch 3271/3680 | Loss: 0.0104\n","Epoch 16/25 | Batch 3301/3680 | Loss: 0.1371\n","Epoch 16/25 | Batch 3331/3680 | Loss: 0.1221\n","Epoch 16/25 | Batch 3361/3680 | Loss: 0.0622\n","Epoch 16/25 | Batch 3391/3680 | Loss: 0.0006\n","Epoch 16/25 | Batch 3421/3680 | Loss: 0.0566\n","Epoch 16/25 | Batch 3451/3680 | Loss: 0.0324\n","Epoch 16/25 | Batch 3481/3680 | Loss: 0.0203\n","Epoch 16/25 | Batch 3511/3680 | Loss: 0.0555\n","Epoch 16/25 | Batch 3541/3680 | Loss: 0.0022\n","Epoch 16/25 | Batch 3571/3680 | Loss: 0.1331\n","Epoch 16/25 | Batch 3601/3680 | Loss: 0.0312\n","Epoch 16/25 | Batch 3631/3680 | Loss: 0.1041\n","Epoch 16/25 | Batch 3661/3680 | Loss: 0.0107\n","Average training loss: 0.06\n","Epoch 17/25\n","Epoch 17/25 | Batch 31/3680 | Loss: 0.5726\n","Epoch 17/25 | Batch 61/3680 | Loss: 0.0351\n","Epoch 17/25 | Batch 91/3680 | Loss: 0.0760\n","Epoch 17/25 | Batch 121/3680 | Loss: 0.0031\n","Epoch 17/25 | Batch 151/3680 | Loss: 0.1019\n","Epoch 17/25 | Batch 181/3680 | Loss: 0.0036\n","Epoch 17/25 | Batch 211/3680 | Loss: 0.0021\n","Epoch 17/25 | Batch 241/3680 | Loss: 0.0145\n","Epoch 17/25 | Batch 271/3680 | Loss: 0.0035\n","Epoch 17/25 | Batch 301/3680 | Loss: 0.1276\n","Epoch 17/25 | Batch 331/3680 | Loss: 0.0684\n","Epoch 17/25 | Batch 361/3680 | Loss: 0.2359\n","Epoch 17/25 | Batch 391/3680 | Loss: 0.0558\n","Epoch 17/25 | Batch 421/3680 | Loss: 0.0011\n","Epoch 17/25 | Batch 451/3680 | Loss: 0.1237\n","Epoch 17/25 | Batch 481/3680 | Loss: 0.0987\n","Epoch 17/25 | Batch 511/3680 | Loss: 0.1027\n","Epoch 17/25 | Batch 541/3680 | Loss: 0.0852\n","Epoch 17/25 | Batch 571/3680 | Loss: 0.0444\n","Epoch 17/25 | Batch 601/3680 | Loss: 0.0210\n","Epoch 17/25 | Batch 631/3680 | Loss: 0.0489\n","Epoch 17/25 | Batch 661/3680 | Loss: 0.0010\n","Epoch 17/25 | Batch 691/3680 | Loss: 0.0505\n","Epoch 17/25 | Batch 721/3680 | Loss: 0.0744\n","Epoch 17/25 | Batch 751/3680 | Loss: 0.0484\n","Epoch 17/25 | Batch 781/3680 | Loss: 0.0367\n","Epoch 17/25 | Batch 811/3680 | Loss: 0.0872\n","Epoch 17/25 | Batch 841/3680 | Loss: 0.0156\n","Epoch 17/25 | Batch 871/3680 | Loss: 0.0011\n","Epoch 17/25 | Batch 901/3680 | Loss: 0.0065\n","Epoch 17/25 | Batch 931/3680 | Loss: 0.0066\n","Epoch 17/25 | Batch 961/3680 | Loss: 0.0700\n","Epoch 17/25 | Batch 991/3680 | Loss: 0.0840\n","Epoch 17/25 | Batch 1021/3680 | Loss: 0.0186\n","Epoch 17/25 | Batch 1051/3680 | Loss: 0.1088\n","Epoch 17/25 | Batch 1081/3680 | Loss: 0.0095\n","Epoch 17/25 | Batch 1111/3680 | Loss: 0.0009\n","Epoch 17/25 | Batch 1141/3680 | Loss: 0.0438\n","Epoch 17/25 | Batch 1171/3680 | Loss: 0.0668\n","Epoch 17/25 | Batch 1201/3680 | Loss: 0.0042\n","Epoch 17/25 | Batch 1231/3680 | Loss: 0.0008\n","Epoch 17/25 | Batch 1261/3680 | Loss: 0.1205\n","Epoch 17/25 | Batch 1291/3680 | Loss: 0.0035\n","Epoch 17/25 | Batch 1321/3680 | Loss: 0.1344\n","Epoch 17/25 | Batch 1351/3680 | Loss: 0.1546\n","Epoch 17/25 | Batch 1381/3680 | Loss: 0.0047\n","Epoch 17/25 | Batch 1411/3680 | Loss: 0.4414\n","Epoch 17/25 | Batch 1441/3680 | Loss: 0.0043\n","Epoch 17/25 | Batch 1471/3680 | Loss: 0.1028\n","Epoch 17/25 | Batch 1501/3680 | Loss: 0.0075\n","Epoch 17/25 | Batch 1531/3680 | Loss: 0.0669\n","Epoch 17/25 | Batch 1561/3680 | Loss: 0.2024\n","Epoch 17/25 | Batch 1591/3680 | Loss: 0.0564\n","Epoch 17/25 | Batch 1621/3680 | Loss: 0.0005\n","Epoch 17/25 | Batch 1651/3680 | Loss: 0.0013\n","Epoch 17/25 | Batch 1681/3680 | Loss: 0.0019\n","Epoch 17/25 | Batch 1711/3680 | Loss: 0.0249\n","Epoch 17/25 | Batch 1741/3680 | Loss: 0.0169\n","Epoch 17/25 | Batch 1771/3680 | Loss: 0.0064\n","Epoch 17/25 | Batch 1801/3680 | Loss: 0.0004\n","Epoch 17/25 | Batch 1831/3680 | Loss: 0.0330\n","Epoch 17/25 | Batch 1861/3680 | Loss: 0.0159\n","Epoch 17/25 | Batch 1891/3680 | Loss: 0.1228\n","Epoch 17/25 | Batch 1921/3680 | Loss: 0.0373\n","Epoch 17/25 | Batch 1951/3680 | Loss: 0.0524\n","Epoch 17/25 | Batch 1981/3680 | Loss: 0.0013\n","Epoch 17/25 | Batch 2011/3680 | Loss: 0.0049\n","Epoch 17/25 | Batch 2041/3680 | Loss: 0.3051\n","Epoch 17/25 | Batch 2071/3680 | Loss: 0.0062\n","Epoch 17/25 | Batch 2101/3680 | Loss: 0.0550\n","Epoch 17/25 | Batch 2131/3680 | Loss: 0.3710\n","Epoch 17/25 | Batch 2161/3680 | Loss: 0.0278\n","Epoch 17/25 | Batch 2191/3680 | Loss: 0.1039\n","Epoch 17/25 | Batch 2221/3680 | Loss: 0.0218\n","Epoch 17/25 | Batch 2251/3680 | Loss: 0.0997\n","Epoch 17/25 | Batch 2281/3680 | Loss: 0.0965\n","Epoch 17/25 | Batch 2311/3680 | Loss: 0.1373\n","Epoch 17/25 | Batch 2341/3680 | Loss: 0.0009\n","Epoch 17/25 | Batch 2371/3680 | Loss: 0.0026\n","Epoch 17/25 | Batch 2401/3680 | Loss: 0.0025\n","Epoch 17/25 | Batch 2431/3680 | Loss: 0.0117\n","Epoch 17/25 | Batch 2461/3680 | Loss: 0.1754\n","Epoch 17/25 | Batch 2491/3680 | Loss: 0.0007\n","Epoch 17/25 | Batch 2521/3680 | Loss: 0.0201\n","Epoch 17/25 | Batch 2551/3680 | Loss: 0.0927\n","Epoch 17/25 | Batch 2581/3680 | Loss: 0.0015\n","Epoch 17/25 | Batch 2611/3680 | Loss: 0.1131\n","Epoch 17/25 | Batch 2641/3680 | Loss: 0.2313\n","Epoch 17/25 | Batch 2671/3680 | Loss: 0.3217\n","Epoch 17/25 | Batch 2701/3680 | Loss: 0.0902\n","Epoch 17/25 | Batch 2731/3680 | Loss: 0.3208\n","Epoch 17/25 | Batch 2761/3680 | Loss: 0.0005\n","Epoch 17/25 | Batch 2791/3680 | Loss: 0.0132\n","Epoch 17/25 | Batch 2821/3680 | Loss: 0.0085\n","Epoch 17/25 | Batch 2851/3680 | Loss: 0.0629\n","Epoch 17/25 | Batch 2881/3680 | Loss: 0.0917\n","Epoch 17/25 | Batch 2911/3680 | Loss: 0.0128\n","Epoch 17/25 | Batch 2941/3680 | Loss: 0.1016\n","Epoch 17/25 | Batch 2971/3680 | Loss: 0.0004\n","Epoch 17/25 | Batch 3001/3680 | Loss: 0.0278\n","Epoch 17/25 | Batch 3031/3680 | Loss: 0.1172\n","Epoch 17/25 | Batch 3061/3680 | Loss: 0.1816\n","Epoch 17/25 | Batch 3091/3680 | Loss: 0.0559\n","Epoch 17/25 | Batch 3121/3680 | Loss: 0.0082\n","Epoch 17/25 | Batch 3151/3680 | Loss: 0.0067\n","Epoch 17/25 | Batch 3181/3680 | Loss: 0.0596\n","Epoch 17/25 | Batch 3211/3680 | Loss: 0.0325\n","Epoch 17/25 | Batch 3241/3680 | Loss: 0.0009\n","Epoch 17/25 | Batch 3271/3680 | Loss: 0.0018\n","Epoch 17/25 | Batch 3301/3680 | Loss: 0.0191\n","Epoch 17/25 | Batch 3331/3680 | Loss: 0.0498\n","Epoch 17/25 | Batch 3361/3680 | Loss: 0.2166\n","Epoch 17/25 | Batch 3391/3680 | Loss: 0.0064\n","Epoch 17/25 | Batch 3421/3680 | Loss: 0.0068\n","Epoch 17/25 | Batch 3451/3680 | Loss: 0.0225\n","Epoch 17/25 | Batch 3481/3680 | Loss: 0.0021\n","Epoch 17/25 | Batch 3511/3680 | Loss: 0.0052\n","Epoch 17/25 | Batch 3541/3680 | Loss: 0.0817\n","Epoch 17/25 | Batch 3571/3680 | Loss: 0.2072\n","Epoch 17/25 | Batch 3601/3680 | Loss: 0.1544\n","Epoch 17/25 | Batch 3631/3680 | Loss: 0.0534\n","Epoch 17/25 | Batch 3661/3680 | Loss: 0.0659\n","Average training loss: 0.06\n","Epoch 18/25\n","Epoch 18/25 | Batch 31/3680 | Loss: 0.2824\n","Epoch 18/25 | Batch 61/3680 | Loss: 0.1052\n","Epoch 18/25 | Batch 91/3680 | Loss: 0.0013\n","Epoch 18/25 | Batch 121/3680 | Loss: 0.0337\n","Epoch 18/25 | Batch 151/3680 | Loss: 0.0060\n","Epoch 18/25 | Batch 181/3680 | Loss: 0.0814\n","Epoch 18/25 | Batch 211/3680 | Loss: 0.0615\n","Epoch 18/25 | Batch 241/3680 | Loss: 0.0091\n","Epoch 18/25 | Batch 271/3680 | Loss: 0.1752\n","Epoch 18/25 | Batch 301/3680 | Loss: 0.0559\n","Epoch 18/25 | Batch 331/3680 | Loss: 0.1736\n","Epoch 18/25 | Batch 361/3680 | Loss: 0.0036\n","Epoch 18/25 | Batch 391/3680 | Loss: 0.0323\n","Epoch 18/25 | Batch 421/3680 | Loss: 0.0985\n","Epoch 18/25 | Batch 451/3680 | Loss: 0.0321\n","Epoch 18/25 | Batch 481/3680 | Loss: 0.4527\n","Epoch 18/25 | Batch 511/3680 | Loss: 0.0205\n","Epoch 18/25 | Batch 541/3680 | Loss: 0.1452\n","Epoch 18/25 | Batch 571/3680 | Loss: 0.0979\n","Epoch 18/25 | Batch 601/3680 | Loss: 0.0008\n","Epoch 18/25 | Batch 631/3680 | Loss: 0.1008\n","Epoch 18/25 | Batch 661/3680 | Loss: 0.1540\n","Epoch 18/25 | Batch 691/3680 | Loss: 0.0164\n","Epoch 18/25 | Batch 721/3680 | Loss: 0.0194\n","Epoch 18/25 | Batch 751/3680 | Loss: 0.2959\n","Epoch 18/25 | Batch 781/3680 | Loss: 0.5251\n","Epoch 18/25 | Batch 811/3680 | Loss: 0.0367\n","Epoch 18/25 | Batch 841/3680 | Loss: 0.0096\n","Epoch 18/25 | Batch 871/3680 | Loss: 0.0125\n","Epoch 18/25 | Batch 901/3680 | Loss: 0.0846\n","Epoch 18/25 | Batch 931/3680 | Loss: 0.0296\n","Epoch 18/25 | Batch 961/3680 | Loss: 0.0081\n","Epoch 18/25 | Batch 991/3680 | Loss: 0.0110\n","Epoch 18/25 | Batch 1021/3680 | Loss: 0.0146\n","Epoch 18/25 | Batch 1051/3680 | Loss: 0.0030\n","Epoch 18/25 | Batch 1081/3680 | Loss: 0.0294\n","Epoch 18/25 | Batch 1111/3680 | Loss: 0.0012\n","Epoch 18/25 | Batch 1141/3680 | Loss: 0.0949\n","Epoch 18/25 | Batch 1171/3680 | Loss: 0.0385\n","Epoch 18/25 | Batch 1201/3680 | Loss: 0.1052\n","Epoch 18/25 | Batch 1231/3680 | Loss: 0.0084\n","Epoch 18/25 | Batch 1261/3680 | Loss: 0.0255\n","Epoch 18/25 | Batch 1291/3680 | Loss: 0.0161\n","Epoch 18/25 | Batch 1321/3680 | Loss: 0.0278\n","Epoch 18/25 | Batch 1351/3680 | Loss: 0.0564\n","Epoch 18/25 | Batch 1381/3680 | Loss: 0.0024\n","Epoch 18/25 | Batch 1411/3680 | Loss: 0.1093\n","Epoch 18/25 | Batch 1441/3680 | Loss: 0.0048\n","Epoch 18/25 | Batch 1471/3680 | Loss: 0.0077\n","Epoch 18/25 | Batch 1501/3680 | Loss: 0.0551\n","Epoch 18/25 | Batch 1531/3680 | Loss: 0.2801\n","Epoch 18/25 | Batch 1561/3680 | Loss: 0.0044\n","Epoch 18/25 | Batch 1591/3680 | Loss: 0.0037\n","Epoch 18/25 | Batch 1621/3680 | Loss: 0.0022\n","Epoch 18/25 | Batch 1651/3680 | Loss: 0.0030\n","Epoch 18/25 | Batch 1681/3680 | Loss: 0.0287\n","Epoch 18/25 | Batch 1711/3680 | Loss: 0.0022\n","Epoch 18/25 | Batch 1741/3680 | Loss: 0.1139\n","Epoch 18/25 | Batch 1771/3680 | Loss: 0.0300\n","Epoch 18/25 | Batch 1801/3680 | Loss: 0.0036\n","Epoch 18/25 | Batch 1831/3680 | Loss: 0.0037\n","Epoch 18/25 | Batch 1861/3680 | Loss: 0.0151\n","Epoch 18/25 | Batch 1891/3680 | Loss: 0.3878\n","Epoch 18/25 | Batch 1921/3680 | Loss: 0.0226\n","Epoch 18/25 | Batch 1951/3680 | Loss: 0.0036\n","Epoch 18/25 | Batch 1981/3680 | Loss: 0.0021\n","Epoch 18/25 | Batch 2011/3680 | Loss: 0.0081\n","Epoch 18/25 | Batch 2041/3680 | Loss: 0.0076\n","Epoch 18/25 | Batch 2071/3680 | Loss: 0.0016\n","Epoch 18/25 | Batch 2101/3680 | Loss: 0.0479\n","Epoch 18/25 | Batch 2131/3680 | Loss: 0.1496\n","Epoch 18/25 | Batch 2161/3680 | Loss: 0.0680\n","Epoch 18/25 | Batch 2191/3680 | Loss: 0.0217\n","Epoch 18/25 | Batch 2221/3680 | Loss: 0.0451\n","Epoch 18/25 | Batch 2251/3680 | Loss: 0.0540\n","Epoch 18/25 | Batch 2281/3680 | Loss: 0.0558\n","Epoch 18/25 | Batch 2311/3680 | Loss: 0.0433\n","Epoch 18/25 | Batch 2341/3680 | Loss: 0.0235\n","Epoch 18/25 | Batch 2371/3680 | Loss: 0.0006\n","Epoch 18/25 | Batch 2401/3680 | Loss: 0.0118\n","Epoch 18/25 | Batch 2431/3680 | Loss: 0.2744\n","Epoch 18/25 | Batch 2461/3680 | Loss: 0.3588\n","Epoch 18/25 | Batch 2491/3680 | Loss: 0.0800\n","Epoch 18/25 | Batch 2521/3680 | Loss: 0.0012\n","Epoch 18/25 | Batch 2551/3680 | Loss: 0.0546\n","Epoch 18/25 | Batch 2581/3680 | Loss: 0.1968\n","Epoch 18/25 | Batch 2611/3680 | Loss: 0.1408\n","Epoch 18/25 | Batch 2641/3680 | Loss: 0.0538\n","Epoch 18/25 | Batch 2671/3680 | Loss: 0.0614\n","Epoch 18/25 | Batch 2701/3680 | Loss: 0.0387\n","Epoch 18/25 | Batch 2731/3680 | Loss: 0.0126\n","Epoch 18/25 | Batch 2761/3680 | Loss: 0.0053\n","Epoch 18/25 | Batch 2791/3680 | Loss: 0.0322\n","Epoch 18/25 | Batch 2821/3680 | Loss: 0.0032\n","Epoch 18/25 | Batch 2851/3680 | Loss: 0.0771\n","Epoch 18/25 | Batch 2881/3680 | Loss: 0.0020\n","Epoch 18/25 | Batch 2911/3680 | Loss: 0.2839\n","Epoch 18/25 | Batch 2941/3680 | Loss: 0.0598\n","Epoch 18/25 | Batch 2971/3680 | Loss: 0.0343\n","Epoch 18/25 | Batch 3001/3680 | Loss: 0.1490\n","Epoch 18/25 | Batch 3031/3680 | Loss: 0.0211\n","Epoch 18/25 | Batch 3061/3680 | Loss: 0.0114\n","Epoch 18/25 | Batch 3091/3680 | Loss: 0.0012\n","Epoch 18/25 | Batch 3121/3680 | Loss: 0.0013\n","Epoch 18/25 | Batch 3151/3680 | Loss: 0.3089\n","Epoch 18/25 | Batch 3181/3680 | Loss: 0.2476\n","Epoch 18/25 | Batch 3211/3680 | Loss: 0.0009\n","Epoch 18/25 | Batch 3241/3680 | Loss: 0.2402\n","Epoch 18/25 | Batch 3271/3680 | Loss: 0.1219\n","Epoch 18/25 | Batch 3301/3680 | Loss: 0.1333\n","Epoch 18/25 | Batch 3331/3680 | Loss: 0.3181\n","Epoch 18/25 | Batch 3361/3680 | Loss: 0.1276\n","Epoch 18/25 | Batch 3391/3680 | Loss: 0.1235\n","Epoch 18/25 | Batch 3421/3680 | Loss: 0.0061\n","Epoch 18/25 | Batch 3451/3680 | Loss: 0.0037\n","Epoch 18/25 | Batch 3481/3680 | Loss: 0.1216\n","Epoch 18/25 | Batch 3511/3680 | Loss: 0.0417\n","Epoch 18/25 | Batch 3541/3680 | Loss: 0.1630\n","Epoch 18/25 | Batch 3571/3680 | Loss: 0.0087\n","Epoch 18/25 | Batch 3601/3680 | Loss: 0.0070\n","Epoch 18/25 | Batch 3631/3680 | Loss: 0.1193\n","Epoch 18/25 | Batch 3661/3680 | Loss: 0.4369\n","Average training loss: 0.06\n","Epoch 19/25\n","Epoch 19/25 | Batch 31/3680 | Loss: 0.0384\n","Epoch 19/25 | Batch 61/3680 | Loss: 0.0060\n","Epoch 19/25 | Batch 91/3680 | Loss: 0.0088\n","Epoch 19/25 | Batch 121/3680 | Loss: 0.1973\n","Epoch 19/25 | Batch 151/3680 | Loss: 0.0112\n","Epoch 19/25 | Batch 181/3680 | Loss: 0.0008\n","Epoch 19/25 | Batch 211/3680 | Loss: 0.0047\n","Epoch 19/25 | Batch 241/3680 | Loss: 0.0140\n","Epoch 19/25 | Batch 271/3680 | Loss: 0.0029\n","Epoch 19/25 | Batch 301/3680 | Loss: 0.0027\n","Epoch 19/25 | Batch 331/3680 | Loss: 0.2103\n","Epoch 19/25 | Batch 361/3680 | Loss: 0.0072\n","Epoch 19/25 | Batch 391/3680 | Loss: 0.0012\n","Epoch 19/25 | Batch 421/3680 | Loss: 0.0004\n","Epoch 19/25 | Batch 451/3680 | Loss: 0.0112\n","Epoch 19/25 | Batch 481/3680 | Loss: 0.0475\n","Epoch 19/25 | Batch 511/3680 | Loss: 0.0021\n","Epoch 19/25 | Batch 541/3680 | Loss: 0.0861\n","Epoch 19/25 | Batch 571/3680 | Loss: 0.0673\n","Epoch 19/25 | Batch 601/3680 | Loss: 0.1516\n","Epoch 19/25 | Batch 631/3680 | Loss: 0.0174\n","Epoch 19/25 | Batch 661/3680 | Loss: 0.1498\n","Epoch 19/25 | Batch 691/3680 | Loss: 0.1277\n","Epoch 19/25 | Batch 721/3680 | Loss: 0.0227\n","Epoch 19/25 | Batch 751/3680 | Loss: 0.1959\n","Epoch 19/25 | Batch 781/3680 | Loss: 0.0110\n","Epoch 19/25 | Batch 811/3680 | Loss: 0.1070\n","Epoch 19/25 | Batch 841/3680 | Loss: 0.0759\n","Epoch 19/25 | Batch 871/3680 | Loss: 0.0020\n","Epoch 19/25 | Batch 901/3680 | Loss: 0.0034\n","Epoch 19/25 | Batch 931/3680 | Loss: 0.0329\n","Epoch 19/25 | Batch 961/3680 | Loss: 0.1182\n","Epoch 19/25 | Batch 991/3680 | Loss: 0.0030\n","Epoch 19/25 | Batch 1021/3680 | Loss: 0.0012\n","Epoch 19/25 | Batch 1051/3680 | Loss: 0.0072\n","Epoch 19/25 | Batch 1081/3680 | Loss: 0.1921\n","Epoch 19/25 | Batch 1111/3680 | Loss: 0.0480\n","Epoch 19/25 | Batch 1141/3680 | Loss: 0.0922\n","Epoch 19/25 | Batch 1171/3680 | Loss: 0.0857\n","Epoch 19/25 | Batch 1201/3680 | Loss: 0.0017\n","Epoch 19/25 | Batch 1231/3680 | Loss: 0.0610\n","Epoch 19/25 | Batch 1261/3680 | Loss: 0.0010\n","Epoch 19/25 | Batch 1291/3680 | Loss: 0.2437\n","Epoch 19/25 | Batch 1321/3680 | Loss: 0.0455\n","Epoch 19/25 | Batch 1351/3680 | Loss: 0.0532\n","Epoch 19/25 | Batch 1381/3680 | Loss: 0.0906\n","Epoch 19/25 | Batch 1411/3680 | Loss: 0.0617\n","Epoch 19/25 | Batch 1441/3680 | Loss: 0.0190\n","Epoch 19/25 | Batch 1471/3680 | Loss: 0.0244\n","Epoch 19/25 | Batch 1501/3680 | Loss: 0.1083\n","Epoch 19/25 | Batch 1531/3680 | Loss: 0.0015\n","Epoch 19/25 | Batch 1561/3680 | Loss: 0.0128\n","Epoch 19/25 | Batch 1591/3680 | Loss: 0.0023\n","Epoch 19/25 | Batch 1621/3680 | Loss: 0.2485\n","Epoch 19/25 | Batch 1651/3680 | Loss: 0.0042\n","Epoch 19/25 | Batch 1681/3680 | Loss: 0.0889\n","Epoch 19/25 | Batch 1711/3680 | Loss: 0.0017\n","Epoch 19/25 | Batch 1741/3680 | Loss: 0.0860\n","Epoch 19/25 | Batch 1771/3680 | Loss: 0.0044\n","Epoch 19/25 | Batch 1801/3680 | Loss: 0.0030\n","Epoch 19/25 | Batch 1831/3680 | Loss: 0.0027\n","Epoch 19/25 | Batch 1861/3680 | Loss: 0.1478\n","Epoch 19/25 | Batch 1891/3680 | Loss: 0.0008\n","Epoch 19/25 | Batch 1921/3680 | Loss: 0.0604\n","Epoch 19/25 | Batch 1951/3680 | Loss: 0.0069\n","Epoch 19/25 | Batch 1981/3680 | Loss: 0.0564\n","Epoch 19/25 | Batch 2011/3680 | Loss: 0.0942\n","Epoch 19/25 | Batch 2041/3680 | Loss: 0.1396\n","Epoch 19/25 | Batch 2071/3680 | Loss: 0.0187\n","Epoch 19/25 | Batch 2101/3680 | Loss: 0.0041\n","Epoch 19/25 | Batch 2131/3680 | Loss: 0.0043\n","Epoch 19/25 | Batch 2161/3680 | Loss: 0.2155\n","Epoch 19/25 | Batch 2191/3680 | Loss: 0.0618\n","Epoch 19/25 | Batch 2221/3680 | Loss: 0.0662\n","Epoch 19/25 | Batch 2251/3680 | Loss: 0.1410\n","Epoch 19/25 | Batch 2281/3680 | Loss: 0.0532\n","Epoch 19/25 | Batch 2311/3680 | Loss: 0.0286\n","Epoch 19/25 | Batch 2341/3680 | Loss: 0.0053\n","Epoch 19/25 | Batch 2371/3680 | Loss: 0.0612\n","Epoch 19/25 | Batch 2401/3680 | Loss: 0.0897\n","Epoch 19/25 | Batch 2431/3680 | Loss: 0.1238\n","Epoch 19/25 | Batch 2461/3680 | Loss: 0.0674\n","Epoch 19/25 | Batch 2491/3680 | Loss: 0.0028\n","Epoch 19/25 | Batch 2521/3680 | Loss: 0.1062\n","Epoch 19/25 | Batch 2551/3680 | Loss: 0.0390\n","Epoch 19/25 | Batch 2581/3680 | Loss: 0.0341\n","Epoch 19/25 | Batch 2611/3680 | Loss: 0.0067\n","Epoch 19/25 | Batch 2641/3680 | Loss: 0.1226\n","Epoch 19/25 | Batch 2671/3680 | Loss: 0.1407\n","Epoch 19/25 | Batch 2701/3680 | Loss: 0.1962\n","Epoch 19/25 | Batch 2731/3680 | Loss: 0.0227\n","Epoch 19/25 | Batch 2761/3680 | Loss: 0.2890\n","Epoch 19/25 | Batch 2791/3680 | Loss: 0.0023\n","Epoch 19/25 | Batch 2821/3680 | Loss: 0.0026\n","Epoch 19/25 | Batch 2851/3680 | Loss: 0.0134\n","Epoch 19/25 | Batch 2881/3680 | Loss: 0.2304\n","Epoch 19/25 | Batch 2911/3680 | Loss: 0.0020\n","Epoch 19/25 | Batch 2941/3680 | Loss: 0.0017\n","Epoch 19/25 | Batch 2971/3680 | Loss: 0.1309\n","Epoch 19/25 | Batch 3001/3680 | Loss: 0.0443\n","Epoch 19/25 | Batch 3031/3680 | Loss: 0.1910\n","Epoch 19/25 | Batch 3061/3680 | Loss: 0.2390\n","Epoch 19/25 | Batch 3091/3680 | Loss: 0.0276\n","Epoch 19/25 | Batch 3121/3680 | Loss: 0.3999\n","Epoch 19/25 | Batch 3151/3680 | Loss: 0.2157\n","Epoch 19/25 | Batch 3181/3680 | Loss: 0.0850\n","Epoch 19/25 | Batch 3211/3680 | Loss: 0.0744\n","Epoch 19/25 | Batch 3241/3680 | Loss: 0.1573\n","Epoch 19/25 | Batch 3271/3680 | Loss: 0.0011\n","Epoch 19/25 | Batch 3301/3680 | Loss: 0.1717\n","Epoch 19/25 | Batch 3331/3680 | Loss: 0.0026\n","Epoch 19/25 | Batch 3361/3680 | Loss: 0.1306\n","Epoch 19/25 | Batch 3391/3680 | Loss: 0.3064\n","Epoch 19/25 | Batch 3421/3680 | Loss: 0.0491\n","Epoch 19/25 | Batch 3451/3680 | Loss: 0.0086\n","Epoch 19/25 | Batch 3481/3680 | Loss: 0.1134\n","Epoch 19/25 | Batch 3511/3680 | Loss: 0.0456\n","Epoch 19/25 | Batch 3541/3680 | Loss: 0.0006\n","Epoch 19/25 | Batch 3571/3680 | Loss: 0.0084\n","Epoch 19/25 | Batch 3601/3680 | Loss: 0.1077\n","Epoch 19/25 | Batch 3631/3680 | Loss: 0.0087\n","Epoch 19/25 | Batch 3661/3680 | Loss: 0.0022\n","Average training loss: 0.07\n","Epoch 20/25\n","Epoch 20/25 | Batch 31/3680 | Loss: 0.0206\n","Epoch 20/25 | Batch 61/3680 | Loss: 0.0100\n","Epoch 20/25 | Batch 91/3680 | Loss: 0.0119\n","Epoch 20/25 | Batch 121/3680 | Loss: 0.0040\n","Epoch 20/25 | Batch 151/3680 | Loss: 0.0275\n","Epoch 20/25 | Batch 181/3680 | Loss: 0.1350\n","Epoch 20/25 | Batch 211/3680 | Loss: 0.0395\n","Epoch 20/25 | Batch 241/3680 | Loss: 0.0214\n","Epoch 20/25 | Batch 271/3680 | Loss: 0.1040\n","Epoch 20/25 | Batch 301/3680 | Loss: 0.1163\n","Epoch 20/25 | Batch 331/3680 | Loss: 0.1603\n","Epoch 20/25 | Batch 361/3680 | Loss: 0.0381\n","Epoch 20/25 | Batch 391/3680 | Loss: 0.0442\n","Epoch 20/25 | Batch 421/3680 | Loss: 0.0928\n","Epoch 20/25 | Batch 451/3680 | Loss: 0.1613\n","Epoch 20/25 | Batch 481/3680 | Loss: 0.0195\n","Epoch 20/25 | Batch 511/3680 | Loss: 0.1536\n","Epoch 20/25 | Batch 541/3680 | Loss: 0.0369\n","Epoch 20/25 | Batch 571/3680 | Loss: 0.0558\n","Epoch 20/25 | Batch 601/3680 | Loss: 0.0030\n","Epoch 20/25 | Batch 631/3680 | Loss: 0.0149\n","Epoch 20/25 | Batch 661/3680 | Loss: 0.0438\n","Epoch 20/25 | Batch 691/3680 | Loss: 0.0244\n","Epoch 20/25 | Batch 721/3680 | Loss: 0.0093\n","Epoch 20/25 | Batch 751/3680 | Loss: 0.0089\n","Epoch 20/25 | Batch 781/3680 | Loss: 0.0766\n","Epoch 20/25 | Batch 811/3680 | Loss: 0.1957\n","Epoch 20/25 | Batch 841/3680 | Loss: 0.0911\n","Epoch 20/25 | Batch 871/3680 | Loss: 0.1619\n","Epoch 20/25 | Batch 901/3680 | Loss: 0.0194\n","Epoch 20/25 | Batch 931/3680 | Loss: 0.0065\n","Epoch 20/25 | Batch 961/3680 | Loss: 0.0126\n","Epoch 20/25 | Batch 991/3680 | Loss: 0.1074\n","Epoch 20/25 | Batch 1021/3680 | Loss: 0.0058\n","Epoch 20/25 | Batch 1051/3680 | Loss: 0.1875\n","Epoch 20/25 | Batch 1081/3680 | Loss: 0.0091\n","Epoch 20/25 | Batch 1111/3680 | Loss: 0.0006\n","Epoch 20/25 | Batch 1141/3680 | Loss: 0.6207\n","Epoch 20/25 | Batch 1171/3680 | Loss: 0.0068\n","Epoch 20/25 | Batch 1201/3680 | Loss: 0.0306\n","Epoch 20/25 | Batch 1231/3680 | Loss: 0.1571\n","Epoch 20/25 | Batch 1261/3680 | Loss: 0.0124\n","Epoch 20/25 | Batch 1291/3680 | Loss: 0.2554\n","Epoch 20/25 | Batch 1321/3680 | Loss: 0.2346\n","Epoch 20/25 | Batch 1351/3680 | Loss: 0.1464\n","Epoch 20/25 | Batch 1381/3680 | Loss: 0.0477\n","Epoch 20/25 | Batch 1411/3680 | Loss: 0.2536\n","Epoch 20/25 | Batch 1441/3680 | Loss: 0.1576\n","Epoch 20/25 | Batch 1471/3680 | Loss: 0.0416\n","Epoch 20/25 | Batch 1501/3680 | Loss: 0.1517\n","Epoch 20/25 | Batch 1531/3680 | Loss: 0.0044\n","Epoch 20/25 | Batch 1561/3680 | Loss: 0.0670\n","Epoch 20/25 | Batch 1591/3680 | Loss: 0.0866\n","Epoch 20/25 | Batch 1621/3680 | Loss: 0.0391\n","Epoch 20/25 | Batch 1651/3680 | Loss: 0.0196\n","Epoch 20/25 | Batch 1681/3680 | Loss: 0.0323\n","Epoch 20/25 | Batch 1711/3680 | Loss: 0.0008\n","Epoch 20/25 | Batch 1741/3680 | Loss: 0.1442\n","Epoch 20/25 | Batch 1771/3680 | Loss: 0.2175\n","Epoch 20/25 | Batch 1801/3680 | Loss: 0.0028\n","Epoch 20/25 | Batch 1831/3680 | Loss: 0.0045\n","Epoch 20/25 | Batch 1861/3680 | Loss: 0.0060\n","Epoch 20/25 | Batch 1891/3680 | Loss: 0.0011\n","Epoch 20/25 | Batch 1921/3680 | Loss: 0.0028\n","Epoch 20/25 | Batch 1951/3680 | Loss: 0.0162\n","Epoch 20/25 | Batch 1981/3680 | Loss: 0.0067\n","Epoch 20/25 | Batch 2011/3680 | Loss: 0.2803\n","Epoch 20/25 | Batch 2041/3680 | Loss: 0.0226\n","Epoch 20/25 | Batch 2071/3680 | Loss: 0.1715\n","Epoch 20/25 | Batch 2101/3680 | Loss: 0.1721\n","Epoch 20/25 | Batch 2131/3680 | Loss: 0.1760\n","Epoch 20/25 | Batch 2161/3680 | Loss: 0.1222\n","Epoch 20/25 | Batch 2191/3680 | Loss: 0.2601\n","Epoch 20/25 | Batch 2221/3680 | Loss: 0.0358\n","Epoch 20/25 | Batch 2251/3680 | Loss: 0.0424\n","Epoch 20/25 | Batch 2281/3680 | Loss: 0.0008\n","Epoch 20/25 | Batch 2311/3680 | Loss: 0.0039\n","Epoch 20/25 | Batch 2341/3680 | Loss: 0.1471\n","Epoch 20/25 | Batch 2371/3680 | Loss: 0.0396\n","Epoch 20/25 | Batch 2401/3680 | Loss: 0.2717\n","Epoch 20/25 | Batch 2431/3680 | Loss: 0.3398\n","Epoch 20/25 | Batch 2461/3680 | Loss: 0.0409\n","Epoch 20/25 | Batch 2491/3680 | Loss: 0.0055\n","Epoch 20/25 | Batch 2521/3680 | Loss: 0.0043\n","Epoch 20/25 | Batch 2551/3680 | Loss: 0.1521\n","Epoch 20/25 | Batch 2581/3680 | Loss: 0.4308\n","Epoch 20/25 | Batch 2611/3680 | Loss: 0.1581\n","Epoch 20/25 | Batch 2641/3680 | Loss: 0.0141\n","Epoch 20/25 | Batch 2671/3680 | Loss: 0.0011\n","Epoch 20/25 | Batch 2701/3680 | Loss: 0.0044\n","Epoch 20/25 | Batch 2731/3680 | Loss: 0.0237\n","Epoch 20/25 | Batch 2761/3680 | Loss: 0.0983\n","Epoch 20/25 | Batch 2791/3680 | Loss: 0.0170\n","Epoch 20/25 | Batch 2821/3680 | Loss: 0.1085\n","Epoch 20/25 | Batch 2851/3680 | Loss: 0.0764\n","Epoch 20/25 | Batch 2881/3680 | Loss: 0.0053\n","Epoch 20/25 | Batch 2911/3680 | Loss: 0.0345\n","Epoch 20/25 | Batch 2941/3680 | Loss: 0.0062\n","Epoch 20/25 | Batch 2971/3680 | Loss: 0.0017\n","Epoch 20/25 | Batch 3001/3680 | Loss: 0.0902\n","Epoch 20/25 | Batch 3031/3680 | Loss: 0.0055\n","Epoch 20/25 | Batch 3061/3680 | Loss: 0.0020\n","Epoch 20/25 | Batch 3091/3680 | Loss: 0.3109\n","Epoch 20/25 | Batch 3121/3680 | Loss: 0.0133\n","Epoch 20/25 | Batch 3151/3680 | Loss: 0.1526\n","Epoch 20/25 | Batch 3181/3680 | Loss: 0.0234\n","Epoch 20/25 | Batch 3211/3680 | Loss: 0.0780\n","Epoch 20/25 | Batch 3241/3680 | Loss: 0.1192\n","Epoch 20/25 | Batch 3271/3680 | Loss: 0.1281\n","Epoch 20/25 | Batch 3301/3680 | Loss: 0.0324\n","Epoch 20/25 | Batch 3331/3680 | Loss: 0.0199\n","Epoch 20/25 | Batch 3361/3680 | Loss: 0.2803\n","Epoch 20/25 | Batch 3391/3680 | Loss: 0.2358\n","Epoch 20/25 | Batch 3421/3680 | Loss: 0.0038\n","Epoch 20/25 | Batch 3451/3680 | Loss: 0.1230\n","Epoch 20/25 | Batch 3481/3680 | Loss: 0.0203\n","Epoch 20/25 | Batch 3511/3680 | Loss: 0.0047\n","Epoch 20/25 | Batch 3541/3680 | Loss: 0.0020\n","Epoch 20/25 | Batch 3571/3680 | Loss: 0.2719\n","Epoch 20/25 | Batch 3601/3680 | Loss: 0.0906\n","Epoch 20/25 | Batch 3631/3680 | Loss: 0.0237\n","Epoch 20/25 | Batch 3661/3680 | Loss: 0.0926\n","Average training loss: 0.08\n","Epoch 21/25\n","Epoch 21/25 | Batch 31/3680 | Loss: 0.0359\n","Epoch 21/25 | Batch 61/3680 | Loss: 0.0751\n","Epoch 21/25 | Batch 91/3680 | Loss: 0.3363\n","Epoch 21/25 | Batch 121/3680 | Loss: 0.0010\n","Epoch 21/25 | Batch 151/3680 | Loss: 0.0311\n","Epoch 21/25 | Batch 181/3680 | Loss: 0.2210\n","Epoch 21/25 | Batch 211/3680 | Loss: 0.0468\n","Epoch 21/25 | Batch 241/3680 | Loss: 0.2329\n","Epoch 21/25 | Batch 271/3680 | Loss: 0.2005\n","Epoch 21/25 | Batch 301/3680 | Loss: 0.0008\n","Epoch 21/25 | Batch 331/3680 | Loss: 0.0358\n","Epoch 21/25 | Batch 361/3680 | Loss: 0.0284\n","Epoch 21/25 | Batch 391/3680 | Loss: 0.0862\n","Epoch 21/25 | Batch 421/3680 | Loss: 0.1384\n","Epoch 21/25 | Batch 451/3680 | Loss: 0.0128\n","Epoch 21/25 | Batch 481/3680 | Loss: 0.0026\n","Epoch 21/25 | Batch 511/3680 | Loss: 0.0116\n","Epoch 21/25 | Batch 541/3680 | Loss: 0.0060\n","Epoch 21/25 | Batch 571/3680 | Loss: 0.0015\n","Epoch 21/25 | Batch 601/3680 | Loss: 0.1542\n","Epoch 21/25 | Batch 631/3680 | Loss: 0.0777\n","Epoch 21/25 | Batch 661/3680 | Loss: 0.0147\n","Epoch 21/25 | Batch 691/3680 | Loss: 0.0031\n","Epoch 21/25 | Batch 721/3680 | Loss: 0.0066\n","Epoch 21/25 | Batch 751/3680 | Loss: 0.0024\n","Epoch 21/25 | Batch 781/3680 | Loss: 0.0343\n","Epoch 21/25 | Batch 811/3680 | Loss: 0.0151\n","Epoch 21/25 | Batch 841/3680 | Loss: 0.0201\n","Epoch 21/25 | Batch 871/3680 | Loss: 0.0073\n","Epoch 21/25 | Batch 901/3680 | Loss: 0.2823\n","Epoch 21/25 | Batch 931/3680 | Loss: 0.1429\n","Epoch 21/25 | Batch 961/3680 | Loss: 0.0092\n","Epoch 21/25 | Batch 991/3680 | Loss: 0.1000\n","Epoch 21/25 | Batch 1021/3680 | Loss: 0.0607\n","Epoch 21/25 | Batch 1051/3680 | Loss: 0.2954\n","Epoch 21/25 | Batch 1081/3680 | Loss: 0.1072\n","Epoch 21/25 | Batch 1111/3680 | Loss: 0.0749\n","Epoch 21/25 | Batch 1141/3680 | Loss: 0.1389\n","Epoch 21/25 | Batch 1171/3680 | Loss: 0.0126\n","Epoch 21/25 | Batch 1201/3680 | Loss: 0.0310\n","Epoch 21/25 | Batch 1231/3680 | Loss: 0.2839\n","Epoch 21/25 | Batch 1261/3680 | Loss: 0.0367\n","Epoch 21/25 | Batch 1291/3680 | Loss: 0.0014\n","Epoch 21/25 | Batch 1321/3680 | Loss: 0.0041\n","Epoch 21/25 | Batch 1351/3680 | Loss: 0.0608\n","Epoch 21/25 | Batch 1381/3680 | Loss: 0.0805\n","Epoch 21/25 | Batch 1411/3680 | Loss: 0.0301\n","Epoch 21/25 | Batch 1441/3680 | Loss: 0.1971\n","Epoch 21/25 | Batch 1471/3680 | Loss: 0.0363\n","Epoch 21/25 | Batch 1501/3680 | Loss: 0.0244\n","Epoch 21/25 | Batch 1531/3680 | Loss: 0.0388\n","Epoch 21/25 | Batch 1561/3680 | Loss: 0.0043\n","Epoch 21/25 | Batch 1591/3680 | Loss: 0.0898\n","Epoch 21/25 | Batch 1621/3680 | Loss: 0.0196\n","Epoch 21/25 | Batch 1651/3680 | Loss: 0.5336\n","Epoch 21/25 | Batch 1681/3680 | Loss: 0.2211\n","Epoch 21/25 | Batch 1711/3680 | Loss: 0.3825\n","Epoch 21/25 | Batch 1741/3680 | Loss: 0.0056\n","Epoch 21/25 | Batch 1771/3680 | Loss: 0.0556\n","Epoch 21/25 | Batch 1801/3680 | Loss: 0.3082\n","Epoch 21/25 | Batch 1831/3680 | Loss: 0.1839\n","Epoch 21/25 | Batch 1861/3680 | Loss: 0.0117\n","Epoch 21/25 | Batch 1891/3680 | Loss: 0.1327\n","Epoch 21/25 | Batch 1921/3680 | Loss: 0.0233\n","Epoch 21/25 | Batch 1951/3680 | Loss: 0.0401\n","Epoch 21/25 | Batch 1981/3680 | Loss: 0.0794\n","Epoch 21/25 | Batch 2011/3680 | Loss: 0.1608\n","Epoch 21/25 | Batch 2041/3680 | Loss: 0.0878\n","Epoch 21/25 | Batch 2071/3680 | Loss: 0.1096\n","Epoch 21/25 | Batch 2101/3680 | Loss: 0.0734\n","Epoch 21/25 | Batch 2131/3680 | Loss: 0.0201\n","Epoch 21/25 | Batch 2161/3680 | Loss: 0.0486\n","Epoch 21/25 | Batch 2191/3680 | Loss: 0.1500\n","Epoch 21/25 | Batch 2221/3680 | Loss: 0.1107\n","Epoch 21/25 | Batch 2251/3680 | Loss: 0.0390\n","Epoch 21/25 | Batch 2281/3680 | Loss: 0.0205\n","Epoch 21/25 | Batch 2311/3680 | Loss: 0.0051\n","Epoch 21/25 | Batch 2341/3680 | Loss: 0.0183\n","Epoch 21/25 | Batch 2371/3680 | Loss: 0.0557\n","Epoch 21/25 | Batch 2401/3680 | Loss: 0.1567\n","Epoch 21/25 | Batch 2431/3680 | Loss: 0.0073\n","Epoch 21/25 | Batch 2461/3680 | Loss: 0.0251\n","Epoch 21/25 | Batch 2491/3680 | Loss: 0.4509\n","Epoch 21/25 | Batch 2521/3680 | Loss: 0.0337\n","Epoch 21/25 | Batch 2551/3680 | Loss: 0.1075\n","Epoch 21/25 | Batch 2581/3680 | Loss: 0.2963\n","Epoch 21/25 | Batch 2611/3680 | Loss: 0.2432\n","Epoch 21/25 | Batch 2641/3680 | Loss: 0.1082\n","Epoch 21/25 | Batch 2671/3680 | Loss: 0.0036\n","Epoch 21/25 | Batch 2701/3680 | Loss: 0.1999\n","Epoch 21/25 | Batch 2731/3680 | Loss: 0.0821\n","Epoch 21/25 | Batch 2761/3680 | Loss: 0.1408\n","Epoch 21/25 | Batch 2791/3680 | Loss: 0.0371\n","Epoch 21/25 | Batch 2821/3680 | Loss: 0.1172\n","Epoch 21/25 | Batch 2851/3680 | Loss: 0.0022\n","Epoch 21/25 | Batch 2881/3680 | Loss: 0.0744\n","Epoch 21/25 | Batch 2911/3680 | Loss: 0.0547\n","Epoch 21/25 | Batch 2941/3680 | Loss: 0.3073\n","Epoch 21/25 | Batch 2971/3680 | Loss: 0.2433\n","Epoch 21/25 | Batch 3001/3680 | Loss: 0.1254\n","Epoch 21/25 | Batch 3031/3680 | Loss: 0.3091\n","Epoch 21/25 | Batch 3061/3680 | Loss: 0.0305\n","Epoch 21/25 | Batch 3091/3680 | Loss: 0.0338\n","Epoch 21/25 | Batch 3121/3680 | Loss: 0.0595\n","Epoch 21/25 | Batch 3151/3680 | Loss: 0.0120\n","Epoch 21/25 | Batch 3181/3680 | Loss: 0.0931\n","Epoch 21/25 | Batch 3211/3680 | Loss: 0.1414\n","Epoch 21/25 | Batch 3241/3680 | Loss: 0.0048\n","Epoch 21/25 | Batch 3271/3680 | Loss: 0.0640\n","Epoch 21/25 | Batch 3301/3680 | Loss: 0.0582\n","Epoch 21/25 | Batch 3331/3680 | Loss: 0.2734\n","Epoch 21/25 | Batch 3361/3680 | Loss: 0.1237\n","Epoch 21/25 | Batch 3391/3680 | Loss: 0.1967\n","Epoch 21/25 | Batch 3421/3680 | Loss: 0.0017\n","Epoch 21/25 | Batch 3451/3680 | Loss: 0.0347\n","Epoch 21/25 | Batch 3481/3680 | Loss: 0.1676\n","Epoch 21/25 | Batch 3511/3680 | Loss: 0.0907\n","Epoch 21/25 | Batch 3541/3680 | Loss: 0.0145\n","Epoch 21/25 | Batch 3571/3680 | Loss: 0.0341\n","Epoch 21/25 | Batch 3601/3680 | Loss: 0.0072\n","Epoch 21/25 | Batch 3631/3680 | Loss: 0.1045\n","Epoch 21/25 | Batch 3661/3680 | Loss: 0.0085\n","Average training loss: 0.08\n","Epoch 22/25\n","Epoch 22/25 | Batch 31/3680 | Loss: 0.1085\n","Epoch 22/25 | Batch 61/3680 | Loss: 0.0383\n","Epoch 22/25 | Batch 91/3680 | Loss: 0.1362\n","Epoch 22/25 | Batch 121/3680 | Loss: 0.0908\n","Epoch 22/25 | Batch 151/3680 | Loss: 0.0049\n","Epoch 22/25 | Batch 181/3680 | Loss: 0.1396\n","Epoch 22/25 | Batch 211/3680 | Loss: 0.1135\n","Epoch 22/25 | Batch 241/3680 | Loss: 0.0197\n","Epoch 22/25 | Batch 271/3680 | Loss: 0.0196\n","Epoch 22/25 | Batch 301/3680 | Loss: 0.0305\n","Epoch 22/25 | Batch 331/3680 | Loss: 0.2457\n","Epoch 22/25 | Batch 361/3680 | Loss: 0.0899\n","Epoch 22/25 | Batch 391/3680 | Loss: 0.0026\n","Epoch 22/25 | Batch 421/3680 | Loss: 0.0988\n","Epoch 22/25 | Batch 451/3680 | Loss: 0.0212\n","Epoch 22/25 | Batch 481/3680 | Loss: 0.0142\n","Epoch 22/25 | Batch 511/3680 | Loss: 0.0400\n","Epoch 22/25 | Batch 541/3680 | Loss: 0.0116\n","Epoch 22/25 | Batch 571/3680 | Loss: 0.0124\n","Epoch 22/25 | Batch 601/3680 | Loss: 0.0044\n","Epoch 22/25 | Batch 631/3680 | Loss: 0.1456\n","Epoch 22/25 | Batch 661/3680 | Loss: 0.1632\n","Epoch 22/25 | Batch 691/3680 | Loss: 0.0646\n","Epoch 22/25 | Batch 721/3680 | Loss: 0.1653\n","Epoch 22/25 | Batch 751/3680 | Loss: 0.0104\n","Epoch 22/25 | Batch 781/3680 | Loss: 0.0127\n","Epoch 22/25 | Batch 811/3680 | Loss: 0.0913\n","Epoch 22/25 | Batch 841/3680 | Loss: 0.1304\n","Epoch 22/25 | Batch 871/3680 | Loss: 0.0038\n","Epoch 22/25 | Batch 901/3680 | Loss: 0.0918\n","Epoch 22/25 | Batch 931/3680 | Loss: 0.1942\n","Epoch 22/25 | Batch 961/3680 | Loss: 0.0016\n","Epoch 22/25 | Batch 991/3680 | Loss: 0.1064\n","Epoch 22/25 | Batch 1021/3680 | Loss: 0.0800\n","Epoch 22/25 | Batch 1051/3680 | Loss: 0.2125\n","Epoch 22/25 | Batch 1081/3680 | Loss: 0.0146\n","Epoch 22/25 | Batch 1111/3680 | Loss: 0.0754\n","Epoch 22/25 | Batch 1141/3680 | Loss: 0.1814\n","Epoch 22/25 | Batch 1171/3680 | Loss: 0.0095\n","Epoch 22/25 | Batch 1201/3680 | Loss: 0.0025\n","Epoch 22/25 | Batch 1231/3680 | Loss: 0.0040\n","Epoch 22/25 | Batch 1261/3680 | Loss: 0.0631\n","Epoch 22/25 | Batch 1291/3680 | Loss: 0.0064\n","Epoch 22/25 | Batch 1321/3680 | Loss: 0.0873\n","Epoch 22/25 | Batch 1351/3680 | Loss: 0.0019\n","Epoch 22/25 | Batch 1381/3680 | Loss: 0.0149\n","Epoch 22/25 | Batch 1411/3680 | Loss: 0.0396\n","Epoch 22/25 | Batch 1441/3680 | Loss: 0.0088\n","Epoch 22/25 | Batch 1471/3680 | Loss: 0.0046\n","Epoch 22/25 | Batch 1501/3680 | Loss: 0.0023\n","Epoch 22/25 | Batch 1531/3680 | Loss: 0.0241\n","Epoch 22/25 | Batch 1561/3680 | Loss: 0.0088\n","Epoch 22/25 | Batch 1591/3680 | Loss: 0.0138\n","Epoch 22/25 | Batch 1621/3680 | Loss: 0.0518\n","Epoch 22/25 | Batch 1651/3680 | Loss: 0.0737\n","Epoch 22/25 | Batch 1681/3680 | Loss: 0.0907\n","Epoch 22/25 | Batch 1711/3680 | Loss: 0.0587\n","Epoch 22/25 | Batch 1741/3680 | Loss: 0.0430\n","Epoch 22/25 | Batch 1771/3680 | Loss: 0.0385\n","Epoch 22/25 | Batch 1801/3680 | Loss: 0.0332\n","Epoch 22/25 | Batch 1831/3680 | Loss: 0.1177\n","Epoch 22/25 | Batch 1861/3680 | Loss: 0.0371\n","Epoch 22/25 | Batch 1891/3680 | Loss: 0.3130\n","Epoch 22/25 | Batch 1921/3680 | Loss: 0.0440\n","Epoch 22/25 | Batch 1951/3680 | Loss: 0.0671\n","Epoch 22/25 | Batch 1981/3680 | Loss: 0.0527\n","Epoch 22/25 | Batch 2011/3680 | Loss: 0.1001\n","Epoch 22/25 | Batch 2041/3680 | Loss: 0.0185\n","Epoch 22/25 | Batch 2071/3680 | Loss: 0.1440\n","Epoch 22/25 | Batch 2101/3680 | Loss: 0.0947\n","Epoch 22/25 | Batch 2131/3680 | Loss: 0.0253\n","Epoch 22/25 | Batch 2161/3680 | Loss: 0.0209\n","Epoch 22/25 | Batch 2191/3680 | Loss: 0.2127\n","Epoch 22/25 | Batch 2221/3680 | Loss: 0.1041\n","Epoch 22/25 | Batch 2251/3680 | Loss: 0.0290\n","Epoch 22/25 | Batch 2281/3680 | Loss: 0.3334\n","Epoch 22/25 | Batch 2311/3680 | Loss: 0.0414\n","Epoch 22/25 | Batch 2341/3680 | Loss: 0.0402\n","Epoch 22/25 | Batch 2371/3680 | Loss: 0.0533\n","Epoch 22/25 | Batch 2401/3680 | Loss: 0.1038\n","Epoch 22/25 | Batch 2431/3680 | Loss: 0.0152\n","Epoch 22/25 | Batch 2461/3680 | Loss: 0.1539\n","Epoch 22/25 | Batch 2491/3680 | Loss: 0.1520\n","Epoch 22/25 | Batch 2521/3680 | Loss: 0.0827\n","Epoch 22/25 | Batch 2551/3680 | Loss: 0.0545\n","Epoch 22/25 | Batch 2581/3680 | Loss: 0.1238\n","Epoch 22/25 | Batch 2611/3680 | Loss: 0.0133\n","Epoch 22/25 | Batch 2641/3680 | Loss: 0.0069\n","Epoch 22/25 | Batch 2671/3680 | Loss: 0.0058\n","Epoch 22/25 | Batch 2701/3680 | Loss: 0.2440\n","Epoch 22/25 | Batch 2731/3680 | Loss: 0.0719\n","Epoch 22/25 | Batch 2761/3680 | Loss: 0.0012\n","Epoch 22/25 | Batch 2791/3680 | Loss: 0.1165\n","Epoch 22/25 | Batch 2821/3680 | Loss: 0.0222\n","Epoch 22/25 | Batch 2851/3680 | Loss: 0.0028\n","Epoch 22/25 | Batch 2881/3680 | Loss: 0.0796\n","Epoch 22/25 | Batch 2911/3680 | Loss: 0.0773\n","Epoch 22/25 | Batch 2941/3680 | Loss: 0.0682\n","Epoch 22/25 | Batch 2971/3680 | Loss: 0.2054\n","Epoch 22/25 | Batch 3001/3680 | Loss: 0.0558\n","Epoch 22/25 | Batch 3031/3680 | Loss: 0.0597\n","Epoch 22/25 | Batch 3061/3680 | Loss: 0.1258\n","Epoch 22/25 | Batch 3091/3680 | Loss: 0.0741\n","Epoch 22/25 | Batch 3121/3680 | Loss: 0.0138\n","Epoch 22/25 | Batch 3151/3680 | Loss: 0.0255\n","Epoch 22/25 | Batch 3181/3680 | Loss: 0.1557\n","Epoch 22/25 | Batch 3211/3680 | Loss: 0.0060\n","Epoch 22/25 | Batch 3241/3680 | Loss: 0.0424\n","Epoch 22/25 | Batch 3271/3680 | Loss: 0.1920\n","Epoch 22/25 | Batch 3301/3680 | Loss: 0.0011\n","Epoch 22/25 | Batch 3331/3680 | Loss: 0.2007\n","Epoch 22/25 | Batch 3361/3680 | Loss: 0.3007\n","Epoch 22/25 | Batch 3391/3680 | Loss: 0.0637\n","Epoch 22/25 | Batch 3421/3680 | Loss: 0.1057\n","Epoch 22/25 | Batch 3451/3680 | Loss: 0.0157\n","Epoch 22/25 | Batch 3481/3680 | Loss: 0.0148\n","Epoch 22/25 | Batch 3511/3680 | Loss: 0.0027\n","Epoch 22/25 | Batch 3541/3680 | Loss: 0.3959\n","Epoch 22/25 | Batch 3571/3680 | Loss: 0.0010\n","Epoch 22/25 | Batch 3601/3680 | Loss: 0.1330\n","Epoch 22/25 | Batch 3631/3680 | Loss: 0.1457\n","Epoch 22/25 | Batch 3661/3680 | Loss: 0.0009\n","Average training loss: 0.08\n","Epoch 23/25\n","Epoch 23/25 | Batch 31/3680 | Loss: 0.0790\n","Epoch 23/25 | Batch 61/3680 | Loss: 0.0683\n","Epoch 23/25 | Batch 91/3680 | Loss: 0.1652\n","Epoch 23/25 | Batch 121/3680 | Loss: 0.1412\n","Epoch 23/25 | Batch 151/3680 | Loss: 0.2267\n","Epoch 23/25 | Batch 181/3680 | Loss: 0.0403\n","Epoch 23/25 | Batch 211/3680 | Loss: 0.0873\n","Epoch 23/25 | Batch 241/3680 | Loss: 0.1458\n","Epoch 23/25 | Batch 271/3680 | Loss: 0.0850\n","Epoch 23/25 | Batch 301/3680 | Loss: 0.0182\n","Epoch 23/25 | Batch 331/3680 | Loss: 0.1924\n","Epoch 23/25 | Batch 361/3680 | Loss: 0.0297\n","Epoch 23/25 | Batch 391/3680 | Loss: 0.0211\n","Epoch 23/25 | Batch 421/3680 | Loss: 0.2046\n","Epoch 23/25 | Batch 451/3680 | Loss: 0.0097\n","Epoch 23/25 | Batch 481/3680 | Loss: 0.0251\n","Epoch 23/25 | Batch 511/3680 | Loss: 0.0908\n","Epoch 23/25 | Batch 541/3680 | Loss: 0.0084\n","Epoch 23/25 | Batch 571/3680 | Loss: 0.0488\n","Epoch 23/25 | Batch 601/3680 | Loss: 0.1380\n","Epoch 23/25 | Batch 631/3680 | Loss: 0.1208\n","Epoch 23/25 | Batch 661/3680 | Loss: 0.1541\n","Epoch 23/25 | Batch 691/3680 | Loss: 0.0619\n","Epoch 23/25 | Batch 721/3680 | Loss: 0.1605\n","Epoch 23/25 | Batch 751/3680 | Loss: 0.2634\n","Epoch 23/25 | Batch 781/3680 | Loss: 0.0026\n","Epoch 23/25 | Batch 811/3680 | Loss: 0.2282\n","Epoch 23/25 | Batch 841/3680 | Loss: 0.0132\n","Epoch 23/25 | Batch 871/3680 | Loss: 0.0283\n","Epoch 23/25 | Batch 901/3680 | Loss: 0.0047\n","Epoch 23/25 | Batch 931/3680 | Loss: 0.0191\n","Epoch 23/25 | Batch 961/3680 | Loss: 0.1950\n","Epoch 23/25 | Batch 991/3680 | Loss: 0.0130\n","Epoch 23/25 | Batch 1021/3680 | Loss: 0.0501\n","Epoch 23/25 | Batch 1051/3680 | Loss: 0.1016\n","Epoch 23/25 | Batch 1081/3680 | Loss: 0.0015\n","Epoch 23/25 | Batch 1111/3680 | Loss: 0.0342\n","Epoch 23/25 | Batch 1141/3680 | Loss: 0.0665\n","Epoch 23/25 | Batch 1171/3680 | Loss: 0.7433\n","Epoch 23/25 | Batch 1201/3680 | Loss: 0.0492\n","Epoch 23/25 | Batch 1231/3680 | Loss: 0.1474\n","Epoch 23/25 | Batch 1261/3680 | Loss: 0.0947\n","Epoch 23/25 | Batch 1291/3680 | Loss: 0.0463\n","Epoch 23/25 | Batch 1321/3680 | Loss: 0.0423\n","Epoch 23/25 | Batch 1351/3680 | Loss: 0.0460\n","Epoch 23/25 | Batch 1381/3680 | Loss: 0.0612\n","Epoch 23/25 | Batch 1411/3680 | Loss: 0.0091\n","Epoch 23/25 | Batch 1441/3680 | Loss: 0.0109\n","Epoch 23/25 | Batch 1471/3680 | Loss: 0.3028\n","Epoch 23/25 | Batch 1501/3680 | Loss: 0.0084\n","Epoch 23/25 | Batch 1531/3680 | Loss: 0.2116\n","Epoch 23/25 | Batch 1561/3680 | Loss: 0.1867\n","Epoch 23/25 | Batch 1591/3680 | Loss: 0.1187\n","Epoch 23/25 | Batch 1621/3680 | Loss: 0.0121\n","Epoch 23/25 | Batch 1651/3680 | Loss: 0.0008\n","Epoch 23/25 | Batch 1681/3680 | Loss: 0.0087\n","Epoch 23/25 | Batch 1711/3680 | Loss: 0.0309\n","Epoch 23/25 | Batch 1741/3680 | Loss: 0.0514\n","Epoch 23/25 | Batch 1771/3680 | Loss: 0.1408\n","Epoch 23/25 | Batch 1801/3680 | Loss: 0.0029\n","Epoch 23/25 | Batch 1831/3680 | Loss: 0.0061\n","Epoch 23/25 | Batch 1861/3680 | Loss: 0.0137\n","Epoch 23/25 | Batch 1891/3680 | Loss: 0.0407\n","Epoch 23/25 | Batch 1921/3680 | Loss: 0.0873\n","Epoch 23/25 | Batch 1951/3680 | Loss: 0.0731\n","Epoch 23/25 | Batch 1981/3680 | Loss: 0.1316\n","Epoch 23/25 | Batch 2011/3680 | Loss: 0.0015\n","Epoch 23/25 | Batch 2041/3680 | Loss: 0.0206\n","Epoch 23/25 | Batch 2071/3680 | Loss: 0.0011\n","Epoch 23/25 | Batch 2101/3680 | Loss: 0.1206\n","Epoch 23/25 | Batch 2131/3680 | Loss: 0.0048\n","Epoch 23/25 | Batch 2161/3680 | Loss: 0.1273\n","Epoch 23/25 | Batch 2191/3680 | Loss: 0.1217\n","Epoch 23/25 | Batch 2221/3680 | Loss: 0.0741\n","Epoch 23/25 | Batch 2251/3680 | Loss: 0.1462\n","Epoch 23/25 | Batch 2281/3680 | Loss: 0.0962\n","Epoch 23/25 | Batch 2311/3680 | Loss: 0.0086\n","Epoch 23/25 | Batch 2341/3680 | Loss: 0.0299\n","Epoch 23/25 | Batch 2371/3680 | Loss: 0.1091\n","Epoch 23/25 | Batch 2401/3680 | Loss: 0.0980\n","Epoch 23/25 | Batch 2431/3680 | Loss: 0.0056\n","Epoch 23/25 | Batch 2461/3680 | Loss: 0.0158\n","Epoch 23/25 | Batch 2491/3680 | Loss: 0.0488\n","Epoch 23/25 | Batch 2521/3680 | Loss: 0.0898\n","Epoch 23/25 | Batch 2551/3680 | Loss: 0.0923\n","Epoch 23/25 | Batch 2581/3680 | Loss: 0.0080\n","Epoch 23/25 | Batch 2611/3680 | Loss: 0.0088\n","Epoch 23/25 | Batch 2641/3680 | Loss: 0.0810\n","Epoch 23/25 | Batch 2671/3680 | Loss: 0.0277\n","Epoch 23/25 | Batch 2701/3680 | Loss: 0.1770\n","Epoch 23/25 | Batch 2731/3680 | Loss: 0.0541\n","Epoch 23/25 | Batch 2761/3680 | Loss: 0.0330\n","Epoch 23/25 | Batch 2791/3680 | Loss: 0.0082\n","Epoch 23/25 | Batch 2821/3680 | Loss: 0.0038\n","Epoch 23/25 | Batch 2851/3680 | Loss: 0.2612\n","Epoch 23/25 | Batch 2881/3680 | Loss: 0.2440\n","Epoch 23/25 | Batch 2911/3680 | Loss: 0.0048\n","Epoch 23/25 | Batch 2941/3680 | Loss: 0.0180\n","Epoch 23/25 | Batch 2971/3680 | Loss: 0.0518\n","Epoch 23/25 | Batch 3001/3680 | Loss: 0.2426\n","Epoch 23/25 | Batch 3031/3680 | Loss: 0.1398\n","Epoch 23/25 | Batch 3061/3680 | Loss: 0.0489\n","Epoch 23/25 | Batch 3091/3680 | Loss: 0.0367\n","Epoch 23/25 | Batch 3121/3680 | Loss: 0.0966\n","Epoch 23/25 | Batch 3151/3680 | Loss: 0.1117\n","Epoch 23/25 | Batch 3181/3680 | Loss: 0.0044\n","Epoch 23/25 | Batch 3211/3680 | Loss: 0.4407\n","Epoch 23/25 | Batch 3241/3680 | Loss: 0.0879\n","Epoch 23/25 | Batch 3271/3680 | Loss: 0.0029\n","Epoch 23/25 | Batch 3301/3680 | Loss: 0.0180\n","Epoch 23/25 | Batch 3331/3680 | Loss: 0.0280\n","Epoch 23/25 | Batch 3361/3680 | Loss: 0.1921\n","Epoch 23/25 | Batch 3391/3680 | Loss: 0.0011\n","Epoch 23/25 | Batch 3421/3680 | Loss: 0.0161\n","Epoch 23/25 | Batch 3451/3680 | Loss: 0.0082\n","Epoch 23/25 | Batch 3481/3680 | Loss: 0.3497\n","Epoch 23/25 | Batch 3511/3680 | Loss: 0.0416\n","Epoch 23/25 | Batch 3541/3680 | Loss: 0.0267\n","Epoch 23/25 | Batch 3571/3680 | Loss: 0.0047\n","Epoch 23/25 | Batch 3601/3680 | Loss: 0.0631\n","Epoch 23/25 | Batch 3631/3680 | Loss: 0.1744\n","Epoch 23/25 | Batch 3661/3680 | Loss: 0.2849\n","Average training loss: 0.09\n","Epoch 24/25\n","Epoch 24/25 | Batch 31/3680 | Loss: 0.0213\n","Epoch 24/25 | Batch 61/3680 | Loss: 0.0016\n","Epoch 24/25 | Batch 91/3680 | Loss: 0.0016\n","Epoch 24/25 | Batch 121/3680 | Loss: 0.3348\n","Epoch 24/25 | Batch 151/3680 | Loss: 0.0628\n","Epoch 24/25 | Batch 181/3680 | Loss: 0.0968\n","Epoch 24/25 | Batch 211/3680 | Loss: 0.0212\n","Epoch 24/25 | Batch 241/3680 | Loss: 0.1045\n","Epoch 24/25 | Batch 271/3680 | Loss: 0.0460\n","Epoch 24/25 | Batch 301/3680 | Loss: 0.0313\n","Epoch 24/25 | Batch 331/3680 | Loss: 0.2655\n","Epoch 24/25 | Batch 361/3680 | Loss: 0.1589\n","Epoch 24/25 | Batch 391/3680 | Loss: 0.1745\n","Epoch 24/25 | Batch 421/3680 | Loss: 0.1343\n","Epoch 24/25 | Batch 451/3680 | Loss: 0.2077\n","Epoch 24/25 | Batch 481/3680 | Loss: 0.0668\n","Epoch 24/25 | Batch 511/3680 | Loss: 0.0011\n","Epoch 24/25 | Batch 541/3680 | Loss: 0.0021\n","Epoch 24/25 | Batch 571/3680 | Loss: 0.0017\n","Epoch 24/25 | Batch 601/3680 | Loss: 0.1249\n","Epoch 24/25 | Batch 631/3680 | Loss: 0.0021\n","Epoch 24/25 | Batch 661/3680 | Loss: 0.1536\n","Epoch 24/25 | Batch 691/3680 | Loss: 0.0851\n","Epoch 24/25 | Batch 721/3680 | Loss: 0.0018\n","Epoch 24/25 | Batch 751/3680 | Loss: 0.2232\n","Epoch 24/25 | Batch 781/3680 | Loss: 0.0016\n","Epoch 24/25 | Batch 811/3680 | Loss: 0.0501\n","Epoch 24/25 | Batch 841/3680 | Loss: 0.1198\n","Epoch 24/25 | Batch 871/3680 | Loss: 0.0606\n","Epoch 24/25 | Batch 901/3680 | Loss: 0.1889\n","Epoch 24/25 | Batch 931/3680 | Loss: 0.0102\n","Epoch 24/25 | Batch 961/3680 | Loss: 0.0068\n","Epoch 24/25 | Batch 991/3680 | Loss: 0.0368\n","Epoch 24/25 | Batch 1021/3680 | Loss: 0.0038\n","Epoch 24/25 | Batch 1051/3680 | Loss: 0.0548\n","Epoch 24/25 | Batch 1081/3680 | Loss: 0.0170\n","Epoch 24/25 | Batch 1111/3680 | Loss: 0.0300\n","Epoch 24/25 | Batch 1141/3680 | Loss: 0.0134\n","Epoch 24/25 | Batch 1171/3680 | Loss: 0.0311\n","Epoch 24/25 | Batch 1201/3680 | Loss: 0.1861\n","Epoch 24/25 | Batch 1231/3680 | Loss: 0.0079\n","Epoch 24/25 | Batch 1261/3680 | Loss: 0.1546\n","Epoch 24/25 | Batch 1291/3680 | Loss: 0.0563\n","Epoch 24/25 | Batch 1321/3680 | Loss: 0.0166\n","Epoch 24/25 | Batch 1351/3680 | Loss: 0.1021\n","Epoch 24/25 | Batch 1381/3680 | Loss: 0.0020\n","Epoch 24/25 | Batch 1411/3680 | Loss: 0.0813\n","Epoch 24/25 | Batch 1441/3680 | Loss: 0.0183\n","Epoch 24/25 | Batch 1471/3680 | Loss: 0.0163\n","Epoch 24/25 | Batch 1501/3680 | Loss: 0.0202\n","Epoch 24/25 | Batch 1531/3680 | Loss: 0.0099\n","Epoch 24/25 | Batch 1561/3680 | Loss: 0.0023\n","Epoch 24/25 | Batch 1591/3680 | Loss: 0.0202\n","Epoch 24/25 | Batch 1621/3680 | Loss: 0.0490\n","Epoch 24/25 | Batch 1651/3680 | Loss: 0.1867\n","Epoch 24/25 | Batch 1681/3680 | Loss: 0.0375\n","Epoch 24/25 | Batch 1711/3680 | Loss: 0.0864\n","Epoch 24/25 | Batch 1741/3680 | Loss: 0.2535\n","Epoch 24/25 | Batch 1771/3680 | Loss: 0.1587\n","Epoch 24/25 | Batch 1801/3680 | Loss: 0.3981\n","Epoch 24/25 | Batch 1831/3680 | Loss: 0.0585\n","Epoch 24/25 | Batch 1861/3680 | Loss: 0.0582\n","Epoch 24/25 | Batch 1891/3680 | Loss: 0.0692\n","Epoch 24/25 | Batch 1921/3680 | Loss: 0.0214\n","Epoch 24/25 | Batch 1951/3680 | Loss: 0.1778\n","Epoch 24/25 | Batch 1981/3680 | Loss: 0.0198\n","Epoch 24/25 | Batch 2011/3680 | Loss: 0.0092\n","Epoch 24/25 | Batch 2041/3680 | Loss: 0.1005\n","Epoch 24/25 | Batch 2071/3680 | Loss: 0.5845\n","Epoch 24/25 | Batch 2101/3680 | Loss: 0.1112\n","Epoch 24/25 | Batch 2131/3680 | Loss: 0.0513\n","Epoch 24/25 | Batch 2161/3680 | Loss: 0.0774\n","Epoch 24/25 | Batch 2191/3680 | Loss: 0.0036\n","Epoch 24/25 | Batch 2221/3680 | Loss: 0.0366\n","Epoch 24/25 | Batch 2251/3680 | Loss: 0.0385\n","Epoch 24/25 | Batch 2281/3680 | Loss: 0.1197\n","Epoch 24/25 | Batch 2311/3680 | Loss: 0.4031\n","Epoch 24/25 | Batch 2341/3680 | Loss: 0.0428\n","Epoch 24/25 | Batch 2371/3680 | Loss: 0.1278\n","Epoch 24/25 | Batch 2401/3680 | Loss: 0.0182\n","Epoch 24/25 | Batch 2431/3680 | Loss: 0.0104\n","Epoch 24/25 | Batch 2461/3680 | Loss: 0.0395\n","Epoch 24/25 | Batch 2491/3680 | Loss: 0.3676\n","Epoch 24/25 | Batch 2521/3680 | Loss: 0.0042\n","Epoch 24/25 | Batch 2551/3680 | Loss: 0.2023\n","Epoch 24/25 | Batch 2581/3680 | Loss: 0.2154\n","Epoch 24/25 | Batch 2611/3680 | Loss: 0.0045\n","Epoch 24/25 | Batch 2641/3680 | Loss: 0.0173\n","Epoch 24/25 | Batch 2671/3680 | Loss: 0.0078\n","Epoch 24/25 | Batch 2701/3680 | Loss: 0.0450\n","Epoch 24/25 | Batch 2731/3680 | Loss: 0.0023\n","Epoch 24/25 | Batch 2761/3680 | Loss: 0.0095\n","Epoch 24/25 | Batch 2791/3680 | Loss: 0.1066\n","Epoch 24/25 | Batch 2821/3680 | Loss: 0.0093\n","Epoch 24/25 | Batch 2851/3680 | Loss: 0.0619\n","Epoch 24/25 | Batch 2881/3680 | Loss: 0.1076\n","Epoch 24/25 | Batch 2911/3680 | Loss: 0.0303\n","Epoch 24/25 | Batch 2941/3680 | Loss: 0.5111\n","Epoch 24/25 | Batch 2971/3680 | Loss: 0.0095\n","Epoch 24/25 | Batch 3001/3680 | Loss: 0.2748\n","Epoch 24/25 | Batch 3031/3680 | Loss: 0.0081\n","Epoch 24/25 | Batch 3061/3680 | Loss: 0.1744\n","Epoch 24/25 | Batch 3091/3680 | Loss: 0.1135\n","Epoch 24/25 | Batch 3121/3680 | Loss: 0.0616\n","Epoch 24/25 | Batch 3151/3680 | Loss: 0.0212\n","Epoch 24/25 | Batch 3181/3680 | Loss: 0.3151\n","Epoch 24/25 | Batch 3211/3680 | Loss: 0.0883\n","Epoch 24/25 | Batch 3241/3680 | Loss: 0.0091\n","Epoch 24/25 | Batch 3271/3680 | Loss: 0.2511\n","Epoch 24/25 | Batch 3301/3680 | Loss: 0.0068\n","Epoch 24/25 | Batch 3331/3680 | Loss: 0.0738\n","Epoch 24/25 | Batch 3361/3680 | Loss: 0.1177\n","Epoch 24/25 | Batch 3391/3680 | Loss: 0.0123\n","Epoch 24/25 | Batch 3421/3680 | Loss: 0.0138\n","Epoch 24/25 | Batch 3451/3680 | Loss: 0.0050\n","Epoch 24/25 | Batch 3481/3680 | Loss: 0.4988\n","Epoch 24/25 | Batch 3511/3680 | Loss: 0.0076\n","Epoch 24/25 | Batch 3541/3680 | Loss: 0.0580\n","Epoch 24/25 | Batch 3571/3680 | Loss: 0.0553\n","Epoch 24/25 | Batch 3601/3680 | Loss: 0.1840\n","Epoch 24/25 | Batch 3631/3680 | Loss: 0.1654\n","Epoch 24/25 | Batch 3661/3680 | Loss: 0.1145\n","Average training loss: 0.09\n","Epoch 25/25\n","Epoch 25/25 | Batch 31/3680 | Loss: 0.0351\n","Epoch 25/25 | Batch 61/3680 | Loss: 0.0357\n","Epoch 25/25 | Batch 91/3680 | Loss: 0.0834\n","Epoch 25/25 | Batch 121/3680 | Loss: 0.1773\n","Epoch 25/25 | Batch 151/3680 | Loss: 0.0592\n","Epoch 25/25 | Batch 181/3680 | Loss: 0.0082\n","Epoch 25/25 | Batch 211/3680 | Loss: 0.0557\n","Epoch 25/25 | Batch 241/3680 | Loss: 0.0897\n","Epoch 25/25 | Batch 271/3680 | Loss: 0.1455\n","Epoch 25/25 | Batch 301/3680 | Loss: 0.0012\n","Epoch 25/25 | Batch 331/3680 | Loss: 0.0200\n","Epoch 25/25 | Batch 361/3680 | Loss: 0.0052\n","Epoch 25/25 | Batch 391/3680 | Loss: 0.0042\n","Epoch 25/25 | Batch 421/3680 | Loss: 0.0553\n","Epoch 25/25 | Batch 451/3680 | Loss: 0.1202\n","Epoch 25/25 | Batch 481/3680 | Loss: 0.0063\n","Epoch 25/25 | Batch 511/3680 | Loss: 0.6258\n","Epoch 25/25 | Batch 541/3680 | Loss: 0.0032\n","Epoch 25/25 | Batch 571/3680 | Loss: 0.0992\n","Epoch 25/25 | Batch 601/3680 | Loss: 0.4681\n","Epoch 25/25 | Batch 631/3680 | Loss: 0.0035\n","Epoch 25/25 | Batch 661/3680 | Loss: 0.0021\n","Epoch 25/25 | Batch 691/3680 | Loss: 0.0036\n","Epoch 25/25 | Batch 721/3680 | Loss: 0.0132\n","Epoch 25/25 | Batch 751/3680 | Loss: 0.0759\n","Epoch 25/25 | Batch 781/3680 | Loss: 0.0281\n","Epoch 25/25 | Batch 811/3680 | Loss: 0.1150\n","Epoch 25/25 | Batch 841/3680 | Loss: 0.0765\n","Epoch 25/25 | Batch 871/3680 | Loss: 0.0417\n","Epoch 25/25 | Batch 901/3680 | Loss: 0.1620\n","Epoch 25/25 | Batch 931/3680 | Loss: 0.0520\n","Epoch 25/25 | Batch 961/3680 | Loss: 0.3499\n","Epoch 25/25 | Batch 991/3680 | Loss: 0.0099\n","Epoch 25/25 | Batch 1021/3680 | Loss: 0.1479\n","Epoch 25/25 | Batch 1051/3680 | Loss: 0.0221\n","Epoch 25/25 | Batch 1081/3680 | Loss: 0.1104\n","Epoch 25/25 | Batch 1111/3680 | Loss: 0.1909\n","Epoch 25/25 | Batch 1141/3680 | Loss: 0.0033\n","Epoch 25/25 | Batch 1171/3680 | Loss: 0.0016\n","Epoch 25/25 | Batch 1201/3680 | Loss: 0.0032\n","Epoch 25/25 | Batch 1231/3680 | Loss: 0.0086\n","Epoch 25/25 | Batch 1261/3680 | Loss: 0.1095\n","Epoch 25/25 | Batch 1291/3680 | Loss: 0.0243\n","Epoch 25/25 | Batch 1321/3680 | Loss: 0.0813\n","Epoch 25/25 | Batch 1351/3680 | Loss: 0.0109\n","Epoch 25/25 | Batch 1381/3680 | Loss: 0.0060\n","Epoch 25/25 | Batch 1411/3680 | Loss: 0.1365\n","Epoch 25/25 | Batch 1441/3680 | Loss: 0.0498\n","Epoch 25/25 | Batch 1471/3680 | Loss: 0.0248\n","Epoch 25/25 | Batch 1501/3680 | Loss: 0.0229\n","Epoch 25/25 | Batch 1531/3680 | Loss: 0.0900\n","Epoch 25/25 | Batch 1561/3680 | Loss: 0.0354\n","Epoch 25/25 | Batch 1591/3680 | Loss: 0.0435\n","Epoch 25/25 | Batch 1621/3680 | Loss: 0.0577\n","Epoch 25/25 | Batch 1651/3680 | Loss: 0.0137\n","Epoch 25/25 | Batch 1681/3680 | Loss: 0.0819\n","Epoch 25/25 | Batch 1711/3680 | Loss: 0.0238\n","Epoch 25/25 | Batch 1741/3680 | Loss: 0.2540\n","Epoch 25/25 | Batch 1771/3680 | Loss: 0.3165\n","Epoch 25/25 | Batch 1801/3680 | Loss: 0.0313\n","Epoch 25/25 | Batch 1831/3680 | Loss: 0.0530\n","Epoch 25/25 | Batch 1861/3680 | Loss: 0.0122\n","Epoch 25/25 | Batch 1891/3680 | Loss: 0.0022\n","Epoch 25/25 | Batch 1921/3680 | Loss: 0.0058\n","Epoch 25/25 | Batch 1951/3680 | Loss: 0.0773\n","Epoch 25/25 | Batch 1981/3680 | Loss: 0.0039\n","Epoch 25/25 | Batch 2011/3680 | Loss: 0.1819\n","Epoch 25/25 | Batch 2041/3680 | Loss: 0.1374\n","Epoch 25/25 | Batch 2071/3680 | Loss: 0.0008\n","Epoch 25/25 | Batch 2101/3680 | Loss: 0.0126\n","Epoch 25/25 | Batch 2131/3680 | Loss: 0.0794\n","Epoch 25/25 | Batch 2161/3680 | Loss: 0.0545\n","Epoch 25/25 | Batch 2191/3680 | Loss: 0.0235\n","Epoch 25/25 | Batch 2221/3680 | Loss: 0.1767\n","Epoch 25/25 | Batch 2251/3680 | Loss: 0.0115\n","Epoch 25/25 | Batch 2281/3680 | Loss: 0.0210\n","Epoch 25/25 | Batch 2311/3680 | Loss: 0.0037\n","Epoch 25/25 | Batch 2341/3680 | Loss: 0.2411\n","Epoch 25/25 | Batch 2371/3680 | Loss: 0.0137\n","Epoch 25/25 | Batch 2401/3680 | Loss: 0.0094\n","Epoch 25/25 | Batch 2431/3680 | Loss: 0.1845\n","Epoch 25/25 | Batch 2461/3680 | Loss: 0.0313\n","Epoch 25/25 | Batch 2491/3680 | Loss: 0.0011\n","Epoch 25/25 | Batch 2521/3680 | Loss: 0.3594\n","Epoch 25/25 | Batch 2551/3680 | Loss: 0.0369\n","Epoch 25/25 | Batch 2581/3680 | Loss: 0.0035\n","Epoch 25/25 | Batch 2611/3680 | Loss: 0.0393\n","Epoch 25/25 | Batch 2641/3680 | Loss: 0.0749\n","Epoch 25/25 | Batch 2671/3680 | Loss: 0.0135\n","Epoch 25/25 | Batch 2701/3680 | Loss: 0.1200\n","Epoch 25/25 | Batch 2731/3680 | Loss: 0.0669\n","Epoch 25/25 | Batch 2761/3680 | Loss: 0.1977\n","Epoch 25/25 | Batch 2791/3680 | Loss: 0.0388\n","Epoch 25/25 | Batch 2821/3680 | Loss: 0.0976\n","Epoch 25/25 | Batch 2851/3680 | Loss: 0.0859\n","Epoch 25/25 | Batch 2881/3680 | Loss: 0.0050\n","Epoch 25/25 | Batch 2911/3680 | Loss: 0.0224\n","Epoch 25/25 | Batch 2941/3680 | Loss: 0.0005\n","Epoch 25/25 | Batch 2971/3680 | Loss: 0.0634\n","Epoch 25/25 | Batch 3001/3680 | Loss: 0.0053\n","Epoch 25/25 | Batch 3031/3680 | Loss: 0.0769\n","Epoch 25/25 | Batch 3061/3680 | Loss: 0.0150\n","Epoch 25/25 | Batch 3091/3680 | Loss: 0.2459\n","Epoch 25/25 | Batch 3121/3680 | Loss: 0.0266\n","Epoch 25/25 | Batch 3151/3680 | Loss: 0.0458\n","Epoch 25/25 | Batch 3181/3680 | Loss: 0.0042\n","Epoch 25/25 | Batch 3211/3680 | Loss: 0.2110\n","Epoch 25/25 | Batch 3241/3680 | Loss: 0.2249\n","Epoch 25/25 | Batch 3271/3680 | Loss: 0.0370\n","Epoch 25/25 | Batch 3301/3680 | Loss: 0.0742\n","Epoch 25/25 | Batch 3331/3680 | Loss: 0.2216\n","Epoch 25/25 | Batch 3361/3680 | Loss: 0.0549\n","Epoch 25/25 | Batch 3391/3680 | Loss: 0.0154\n","Epoch 25/25 | Batch 3421/3680 | Loss: 0.0063\n","Epoch 25/25 | Batch 3451/3680 | Loss: 0.0050\n","Epoch 25/25 | Batch 3481/3680 | Loss: 0.2154\n","Epoch 25/25 | Batch 3511/3680 | Loss: 0.0380\n","Epoch 25/25 | Batch 3541/3680 | Loss: 0.0840\n","Epoch 25/25 | Batch 3571/3680 | Loss: 0.0077\n","Epoch 25/25 | Batch 3601/3680 | Loss: 0.1086\n","Epoch 25/25 | Batch 3631/3680 | Loss: 0.0030\n","Epoch 25/25 | Batch 3661/3680 | Loss: 0.5353\n","Average training loss: 0.08\n","Training complete\n"]}],"source":["# # Initialize scaler for mixed precision training\n","# scaler = GradScaler()\n","\n","\n","# # Train the model\n","# train_model_mixed_precision(model, train_dataloader, optimizer, scheduler, epochs, device=device, scaler=scaler)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:22:33.603842Z","iopub.status.busy":"2024-08-28T04:22:33.603464Z","iopub.status.idle":"2024-08-28T04:22:34.130916Z","shell.execute_reply":"2024-08-28T04:22:34.129878Z","shell.execute_reply.started":"2024-08-28T04:22:33.603805Z"},"id":"yL6KMedbkoK7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["New directory created\n","Model state dictionary saved to ./model_save/text_model/model_state_dict.pt\n"]}],"source":["# # Save the model's state dictionary using torch.save\n","# output_dir = '/kaggle/working/text_model/'\n","\n","# # Remove the previous directory if it exists\n","# if os.path.exists(output_dir):\n","#     shutil.rmtree(output_dir)\n","#     print(\"Previous directory deleted\")\n","\n","# # Create a new directory\n","# os.makedirs(output_dir)\n","# print(\"New directory created\")\n","\n","# # Save the model's state dictionary\n","# model_path = os.path.join(output_dir, 'model_state_dict.pt')\n","# torch.save(model.state_dict(), model_path)\n","\n","# # Save the tokenizer\n","# tokenizer.save_pretrained(output_dir)\n","\n","# print(f\"Model state dictionary saved to {model_path}\")\n","# logging.info(f\"Model state dictionary saved to {model_path}\")\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:22:34.132631Z","iopub.status.busy":"2024-08-28T04:22:34.132264Z","iopub.status.idle":"2024-08-28T04:22:36.301313Z","shell.execute_reply":"2024-08-28T04:22:36.300295Z","shell.execute_reply.started":"2024-08-28T04:22:34.132595Z"},"id":"Nw0TjNWwxkC7","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassificationWithDropout were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Model and tokenizer loaded successfully.\n"]}],"source":["# # Define the directory where the model is saved\n","# output_dir = './model_save/text_model/'\n","\n","# # Load the tokenizer\n","# tokenizer = DistilBertTokenizer.from_pretrained(output_dir)\n","\n","# # Initialize the model with the appropriate configuration\n","# model = DistilBertForSequenceClassificationWithDropout.from_pretrained(\"distilbert-base-uncased\", num_labels=len(set(y_train)))\n","\n","# # Load the model's state dictionary\n","# model_path = os.path.join(output_dir, 'model_state_dict.pt')\n","# model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n","\n","# # Move the model to the same device you used for training (e.g., GPU if available)\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","\n","# print(\"Model and tokenizer loaded successfully.\")\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:22:36.303130Z","iopub.status.busy":"2024-08-28T04:22:36.302743Z","iopub.status.idle":"2024-08-28T04:22:36.311844Z","shell.execute_reply":"2024-08-28T04:22:36.310692Z","shell.execute_reply.started":"2024-08-28T04:22:36.303086Z"},"id":"DSItJxx5koK7","trusted":true},"outputs":[],"source":["\n","# def evaluate(model, dataloader, device):\n","#     model.eval()\n","#     total_eval_loss = 0\n","#     total_eval_accuracy = 0\n","\n","#     for batch in dataloader:\n","#         b_input_ids = batch[0].to(device)\n","#         b_input_mask = batch[1].to(device)\n","#         b_labels = batch[2].to(device)\n","\n","#         with torch.no_grad():\n","#             outputs = model(b_input_ids,\n","#                             attention_mask=b_input_mask,\n","#                             labels=b_labels)\n","            \n","#             # Unpack the outputs tuple\n","#             loss, logits = outputs[:2]\n","\n","#         total_eval_loss += loss.item()\n","\n","#         # Calculate accuracy\n","#         preds = torch.argmax(logits, dim=1).flatten()\n","#         accuracy = (preds == b_labels).cpu().numpy().mean()\n","#         total_eval_accuracy += accuracy\n","\n","#     avg_eval_loss = total_eval_loss / len(dataloader)\n","#     avg_eval_accuracy = total_eval_accuracy / len(dataloader)\n","    \n","#     print(f\"Evaluation Loss: {avg_eval_loss}\")\n","#     print(f\"Evaluation Accuracy: {avg_eval_accuracy}\")\n","\n","#     return avg_eval_loss, avg_eval_accuracy\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:22:36.313369Z","iopub.status.busy":"2024-08-28T04:22:36.313024Z","iopub.status.idle":"2024-08-28T04:25:27.914195Z","shell.execute_reply":"2024-08-28T04:25:27.913225Z","shell.execute_reply.started":"2024-08-28T04:22:36.313317Z"},"id":"QCGen84gkoLL","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'test_dataloader' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m evaluate(model, \u001b[43mtest_dataloader\u001b[49m, device)\n","\u001b[1;31mNameError\u001b[0m: name 'test_dataloader' is not defined"]}],"source":["# # Evaluate the model on the test set\n","# evaluate(model, test_dataloader, device)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:25:27.916496Z","iopub.status.busy":"2024-08-28T04:25:27.915695Z","iopub.status.idle":"2024-08-28T04:25:27.925251Z","shell.execute_reply":"2024-08-28T04:25:27.923900Z","shell.execute_reply.started":"2024-08-28T04:25:27.916443Z"},"id":"D6JZzYltkoLL","trusted":true},"outputs":[],"source":["# def predict(text, model, tokenizer, label_encoder, device, max_len=128):\n","#     model.eval()\n","    \n","#     # Tokenize and encode the text\n","#     inputs = tokenizer.encode_plus(\n","#         text,\n","#         add_special_tokens=True,\n","#         max_length=max_len,\n","#         pad_to_max_length=True,\n","#         return_tensors='pt',\n","#         truncation=True\n","#     )\n","    \n","#     input_ids = inputs['input_ids']\n","#     masks = inputs['attention_mask']\n","\n","#     with torch.no_grad():\n","#         outputs = model(input_ids.to(device), attention_mask=masks.to(device))\n","        \n","#         # Unpack the logits from the model outputs\n","#         logits = outputs[0]\n","#         pred = torch.argmax(logits, dim=1).flatten()\n","    \n","#     label = label_encoder.inverse_transform(pred.cpu().numpy())\n","#     return label[0]\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T04:27:58.248604Z","iopub.status.busy":"2024-08-28T04:27:58.247731Z","iopub.status.idle":"2024-08-28T04:27:58.265584Z","shell.execute_reply":"2024-08-28T04:27:58.264405Z","shell.execute_reply.started":"2024-08-28T04:27:58.248553Z"},"id":"KJLNx9QfkoLM","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Emotion: sad\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# # Example prediction\n","# example_text = \"I am angry mom.\"\n","# predicted_emotion = predict(example_text, model, tokenizer, label_encoder, device)\n","# print(f\"Predicted Emotion: {predicted_emotion}\")"]},{"cell_type":"markdown","metadata":{"id":"jYCp93TJkoLM"},"source":["#### Select or Train an Emotion-Aware TTS Model\n","\n","You will need a TTS model that can generate speech with different emotional tones."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALzaXl7QkoLN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"oRdZUp1JkoLN"},"source":["#### Map Detected Emotion to Speech Emotion\n","\n","Map the detected emotion from the text to the corresponding emotion category in your TTS model or dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcIBzoGikoLN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GRe9qhZMxOoO"},"source":["#### Generate Speech Based on Detected Emotion\n","\n","Once the emotion is detected and mapped, use the TTS model to generate speech that matches the detected emotion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZQ4bGeGxTgB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":861334,"sourceId":1468503,"sourceType":"datasetVersion"},{"datasetId":1116138,"sourceId":1874890,"sourceType":"datasetVersion"},{"datasetId":3583796,"sourceId":6237981,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0360e4d2f421426caa1db975a9a8250a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05ffebef2dc4403daa2ca774197c4c78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06af6c231c9e4afb8a864d0bb5b4b12b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"072d76446d1043f3a076659c9820c1a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c40ed4a79b30449fb2944ba3940a2b88","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_692b154e204c4c62997670e722d5aa87","value":231508}},"0960e6a5415d404088a9318a40605f79":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"097621c1397143aba81508c2b473af25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e9dd76f8a3842ae8e00cbcbf650bd33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f6c021e142e4fca81241cc55ee630fd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11af653a829a40f19ed1ceb10a7c613f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1230c01b3f9c41438f2129f8d6a6f612":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12e2ecb4a806453d851913ba3a8551ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13f0215578334fa5ad169af23ffee9a6","IPY_MODEL_bef2a7cb076b489c99223b9e9d904a47","IPY_MODEL_6b8b80021cf74d44b65981b0b893531a"],"layout":"IPY_MODEL_d7426340cc2e41368bdd6a6de32dfb9d"}},"13f0215578334fa5ad169af23ffee9a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae209bdf47a9437e950631be3ad2faba","placeholder":"","style":"IPY_MODEL_43756902feb6441381a6628749531576","value":"model.safetensors:100%"}},"15fe75d6cf324afcbad76c3fff235e25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1983abd6d9b4492dbead4144f333a40d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21b6a3eec6594af6bd780c39f638a01a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_638f96cbac954d28ac2abb594dfe31f1","IPY_MODEL_b94cfcb00fde491ea4430cc6ab35f6f8","IPY_MODEL_9ee89f2088794e37904d4f312a182b61"],"layout":"IPY_MODEL_097621c1397143aba81508c2b473af25"}},"246a4299eaea4637b0365d6895983db1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9ac0cc6fd244c3ba742f438f17e9a84","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05ffebef2dc4403daa2ca774197c4c78","value":483}},"2a4715b2bb4e458e8a6b38ac6fa1a1db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38b994569beb46c688d5d97141c9fb4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_80654b47b9114dd591eecb060f8ae9c1","IPY_MODEL_072d76446d1043f3a076659c9820c1a2","IPY_MODEL_eab4e13c106142c7982b38fad7cb7a52"],"layout":"IPY_MODEL_f62fc4f416f44362b4aebe5b6d2c3baf"}},"3b4397e7fad448cf821027eece4db216":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43756902feb6441381a6628749531576":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"486cc0c8e8f647d08f73a1cbb7969adb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cd0bc239afb41eb81f2778d47421e2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5423dc8a2efa44c5a31dc410abfdc03b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f4c56909434ababfc5dcab0ca4a5e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5423dc8a2efa44c5a31dc410abfdc03b","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06af6c231c9e4afb8a864d0bb5b4b12b","value":48}},"638f96cbac954d28ac2abb594dfe31f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e9dd76f8a3842ae8e00cbcbf650bd33","placeholder":"","style":"IPY_MODEL_fda0dec552ff48d3a0394188b47977d4","value":"tokenizer.json:100%"}},"63f34dd7c9a446878b8a3f56bb3f4186":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_754d917cd14c43df8436a40767ff356f","IPY_MODEL_246a4299eaea4637b0365d6895983db1","IPY_MODEL_be8a19c8021f4d2187a87d0a9a85569c"],"layout":"IPY_MODEL_c74bfffa98b44498b03114f425eac9bf"}},"6918873d5a9b4f93ba15de0823662954":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"692b154e204c4c62997670e722d5aa87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b8b80021cf74d44b65981b0b893531a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef5600325a2546d3bf60f6e2da3b96d7","placeholder":"","style":"IPY_MODEL_7f24850de9634eafabf53b9df8eeb475","value":"268M/268M[00:01&lt;00:00,148MB/s]"}},"754d917cd14c43df8436a40767ff356f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6918873d5a9b4f93ba15de0823662954","placeholder":"","style":"IPY_MODEL_0360e4d2f421426caa1db975a9a8250a","value":"config.json:100%"}},"7b9ba248d5c947bc8c91b57114f1f545":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f24850de9634eafabf53b9df8eeb475":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80654b47b9114dd591eecb060f8ae9c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f6c021e142e4fca81241cc55ee630fd","placeholder":"","style":"IPY_MODEL_addf7eef65d44323ba566c965431e05a","value":"vocab.txt:100%"}},"85c6ad454bb14cc39498936f4dfc5c1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1230c01b3f9c41438f2129f8d6a6f612","placeholder":"","style":"IPY_MODEL_1983abd6d9b4492dbead4144f333a40d","value":"tokenizer_config.json:100%"}},"8b09c81a61bf43c8bec5ef0f1a2314ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ba3f6f2ecc343a7b3c275cfd00a5b20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ee89f2088794e37904d4f312a182b61":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a31e3e7db2b74eacb59c2980b4a60b41","placeholder":"","style":"IPY_MODEL_3b4397e7fad448cf821027eece4db216","value":"466k/466k[00:00&lt;00:00,22.9MB/s]"}},"a31e3e7db2b74eacb59c2980b4a60b41":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa3341674ca24dd19340826cd6714a86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85c6ad454bb14cc39498936f4dfc5c1a","IPY_MODEL_60f4c56909434ababfc5dcab0ca4a5e6","IPY_MODEL_afd393d05a1c4e75b83d642f7171c6b7"],"layout":"IPY_MODEL_15fe75d6cf324afcbad76c3fff235e25"}},"addf7eef65d44323ba566c965431e05a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae209bdf47a9437e950631be3ad2faba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd393d05a1c4e75b83d642f7171c6b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b09c81a61bf43c8bec5ef0f1a2314ed","placeholder":"","style":"IPY_MODEL_d20edf1dd4c84b23ab79a4956c87b849","value":"48.0/48.0[00:00&lt;00:00,3.62kB/s]"}},"b1ecb339c7f14412acde681d5b8a567b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b94cfcb00fde491ea4430cc6ab35f6f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a4715b2bb4e458e8a6b38ac6fa1a1db","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ba3f6f2ecc343a7b3c275cfd00a5b20","value":466062}},"be8a19c8021f4d2187a87d0a9a85569c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cd0bc239afb41eb81f2778d47421e2a","placeholder":"","style":"IPY_MODEL_b1ecb339c7f14412acde681d5b8a567b","value":"483/483[00:00&lt;00:00,44.1kB/s]"}},"bef2a7cb076b489c99223b9e9d904a47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0960e6a5415d404088a9318a40605f79","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11af653a829a40f19ed1ceb10a7c613f","value":267954768}},"c40ed4a79b30449fb2944ba3940a2b88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c74bfffa98b44498b03114f425eac9bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d20edf1dd4c84b23ab79a4956c87b849":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7426340cc2e41368bdd6a6de32dfb9d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab4e13c106142c7982b38fad7cb7a52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b9ba248d5c947bc8c91b57114f1f545","placeholder":"","style":"IPY_MODEL_486cc0c8e8f647d08f73a1cbb7969adb","value":"232k/232k[00:00&lt;00:00,4.80MB/s]"}},"ef5600325a2546d3bf60f6e2da3b96d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f62fc4f416f44362b4aebe5b6d2c3baf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ac0cc6fd244c3ba742f438f17e9a84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fda0dec552ff48d3a0394188b47977d4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
